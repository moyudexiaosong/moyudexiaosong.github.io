

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.jpg">
  <link rel="icon" href="/img/icon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="概率论基础知识随机变量：取决于随机事件的结果。举个例子，抛硬币就是一个随机事件，X就是随机变量，有两种可能的取值0和1。在随机事件发生前，我们不知道X是什么，但是我们知道概率。  概率统计中，用小写x表示观测值，大写X表示随机变量，x只是一个数而已，并不代表概率，举个例子，扔了四次硬币，结果是：  概率密度函数：意味着随机变量在某个确定的取值点附近的可能性。举个例子，高斯函数是一个随机变量的分布">
<meta property="og:type" content="article">
<meta property="og:title" content="深度强化学习">
<meta property="og:url" content="http://example.com/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="摸鱼之家">
<meta property="og:description" content="概率论基础知识随机变量：取决于随机事件的结果。举个例子，抛硬币就是一个随机事件，X就是随机变量，有两种可能的取值0和1。在随机事件发生前，我们不知道X是什么，但是我们知道概率。  概率统计中，用小写x表示观测值，大写X表示随机变量，x只是一个数而已，并不代表概率，举个例子，扔了四次硬币，结果是：  概率密度函数：意味着随机变量在某个确定的取值点附近的可能性。举个例子，高斯函数是一个随机变量的分布">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/photo37.jpeg">
<meta property="article:published_time" content="2022-09-28T10:26:43.000Z">
<meta property="article:modified_time" content="2022-10-10T02:24:15.270Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="机器学习基础理论">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/photo37.jpeg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>深度强化学习 - 摸鱼之家</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.1","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":false},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz","app_key":"kMsGLN3hzkQJuLrmqQBgquFF","server_url":"https://awhbuajs.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>快乐老家</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                主页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                档案馆
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                目录
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/photo37.jpeg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="深度强化学习"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-09-28 18:26" pubdate>
          September 28, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          9.1k words
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">深度强化学习</h1>
            
              <p class="note note-info">
                
                  
                    Last updated on a month ago
                  
                
              </p>
            
            <div class="markdown-body">
              
              <hr>
<h3 id="概率论基础知识"><a href="#概率论基础知识" class="headerlink" title="概率论基础知识"></a>概率论基础知识</h3><h5 id="随机变量：取决于随机事件的结果。"><a href="#随机变量：取决于随机事件的结果。" class="headerlink" title="随机变量：取决于随机事件的结果。"></a>随机变量：取决于随机事件的结果。</h5><p>举个例子，抛硬币就是一个随机事件，X就是随机变量，有两种可能的取值0和1。在随机事件发生前，我们不知道X是什么，但是我们知道概率。</p>
<p><img src="/../img/image-20220928183006887.png" srcset="/img/loading.gif" lazyload alt="image-20220928183006887"></p>
<p>概率统计中，用小写x表示观测值，大写X表示随机变量，x只是一个数而已，并不代表概率，举个例子，扔了四次硬币，结果是：</p>
<p><img src="/../img/image-20220928183336103.png" srcset="/img/loading.gif" lazyload alt="image-20220928183336103"></p>
<h5 id="概率密度函数：意味着随机变量在某个确定的取值点附近的可能性。"><a href="#概率密度函数：意味着随机变量在某个确定的取值点附近的可能性。" class="headerlink" title="概率密度函数：意味着随机变量在某个确定的取值点附近的可能性。"></a>概率密度函数：意味着随机变量在某个确定的取值点附近的可能性。</h5><p>举个例子，高斯函数是一个随机变量的分布函数。它的概率密度函数在0点最大，远离0点的地方值越来越小，说明它在0点附近取值的概率较大。</p>
<p>把所有可能的取值都算上，它们的概率加和等于1。</p>
<p><img src="/../img/image-20220928183546028.png" srcset="/img/loading.gif" lazyload alt="image-20220928183546028"></p>
<h5 id="期望E-X-如下定义，p-x-是概率密度函数。"><a href="#期望E-X-如下定义，p-x-是概率密度函数。" class="headerlink" title="期望E(X)如下定义，p(x)是概率密度函数。"></a>期望E(X)如下定义，p(x)是概率密度函数。</h5><p><img src="/../img/image-20220928183846442.png" srcset="/img/loading.gif" lazyload alt="image-20220928183846442"></p>
<h5 id="随机抽样："><a href="#随机抽样：" class="headerlink" title="随机抽样："></a>随机抽样：</h5><p><img src="/../img/image-20220928184217776.png" srcset="/img/loading.gif" lazyload alt="image-20220928184217776"></p>
<h3 id="专业术语"><a href="#专业术语" class="headerlink" title="专业术语"></a>专业术语</h3><h5 id="state和action："><a href="#state和action：" class="headerlink" title="state和action："></a>state和action：</h5><p>state可以理解为当前场景，action就是智能体(Agent)的动作。举个例子，假设我们在玩超级马里奥，整个画面就是现在的state，马里奥就是智能体，他能做的上下左右运动就是action。</p>
<p><img src="/../img/image-20220928184404914.png" srcset="/img/loading.gif" lazyload alt="image-20220928184404914"></p>
<h5 id="policy："><a href="#policy：" class="headerlink" title="policy："></a>policy：</h5><p>policy可以理解为决策在这个例子中，就是在我们观测到屏幕上这个画面的时候，我们该让Mario做什么动作。在数学上，policy函数这么定义：$\pi(a|s)=P(A=a|S=s)$。它是一个概率密度函数，意思是给定状态s，做出动作a的概率密度。</p>
<p><img src="/../img/image-20220928193308974.png" srcset="/img/loading.gif" lazyload alt="image-20220928193308974"></p>
<p>如果让policy自己完成这个决策，它就会做一个随机抽样，但是向上的概率最大。</p>
<p>强化学习学的就是这个policy函数，只要有了这个函数，就可以自动操作Mario来自动打游戏了。这个policy函数可以是固定，也可以是随机的，但是在个人博弈的情况下最好还是随机的。举个例子，在玩石头剪刀布时，如果你的出拳顺序是固定的，别人很容易就可以猜到你会出什么，因此最好还是采用概率密度函数作决策函数，随机抽样得到这个动作。</p>
<p>采用随机策略的目的不仅为了增加随机性，也是为了增加探索性。</p>
<h5 id="reward-R："><a href="#reward-R：" class="headerlink" title="reward R："></a>reward R：</h5><p>agent做出一个动作，游戏就会给一个奖励，这个奖励通常需要我们自己来定义，这个奖励定义的好坏非常影响强化学习的效果，举个例子：</p>
<p><img src="/../img/image-20220928202145557.png" srcset="/img/loading.gif" lazyload alt="image-20220928202145557"></p>
<p>吃个金币奖励1，但是为了赢下游戏，赢比赛的奖励要定的断层的高，而强化学习的目的就是为了得到更多的奖励。</p>
<h5 id="state-transition状态转移："><a href="#state-transition状态转移：" class="headerlink" title="state transition状态转移："></a>state transition状态转移：</h5><p>举个例子，当前状态下，Mario跳一下，屏幕就发生改变了，也就是state变了，这个过程就叫状态转移。它可以是确定的，也可以是随机的，通常我们认为是随机的。</p>
<p><img src="/../img/image-20220928220108934.png" srcset="/img/loading.gif" lazyload alt="image-20220928220108934"></p>
<p>状态转移的随机性是从环境里来的，在这个程序里，Mario往上跳一下，确定会吃到金币，但我们不确定的是屎蛋在下个state里会往左还是往右，这就是下个state的随机性。</p>
<p>可以把状态转移用p函数来表示，意思是已经知道当前状态为s，动作为a，下一个状态变为$s’$的概率。在这个例子中，已知Mario在这个状态里往上跳的条件下，下一个state屎蛋往左的概率是0.8，往右的概率是0.2。但是我们作为玩家，并不知道这个状态转移函数，只有环境自己知道。</p>
<h5 id="Agent-Environment-Interaction交互："><a href="#Agent-Environment-Interaction交互：" class="headerlink" title="Agent-Environment Interaction交互："></a>Agent-Environment Interaction交互：</h5><p>在这个例子里，Mario是Agent，环境是游戏程序，state就是当前游戏画面，agent做出action后，反馈给环境，环境改变了state，并且给这个动作一个reward。</p>
<p><img src="/../img/image-20220928221301524.png" srcset="/img/loading.gif" lazyload alt="image-20220928221301524"></p>
<p><img src="/../img/image-20220928222649869.png" srcset="/img/loading.gif" lazyload alt="image-20220928222649869"></p>
<p><img src="/../img/image-20220928222805359.png" srcset="/img/loading.gif" lazyload alt="image-20220928222805359"></p>
<h5 id="强化学习中的随机性来源："><a href="#强化学习中的随机性来源：" class="headerlink" title="强化学习中的随机性来源："></a>强化学习中的随机性来源：</h5><p>一、action产生的随机性：action是根据policy函数策略随机抽样得来的。</p>
<p><img src="/../img/image-20220928222435691.png" srcset="/img/loading.gif" lazyload alt="image-20220928222435691"></p>
<p>二、状态转移产生的随机性：状态概率是环境机制决定的。</p>
<p><img src="/../img/image-20220928222955204.png" srcset="/img/loading.gif" lazyload alt="image-20220928222955204"></p>
<h5 id="怎么样让AI自动打赢游戏？"><a href="#怎么样让AI自动打赢游戏？" class="headerlink" title="怎么样让AI自动打赢游戏？"></a>怎么样让AI自动打赢游戏？</h5><p><img src="/../img/image-20220928223235655.png" srcset="/img/loading.gif" lazyload alt="image-20220928223235655"></p>
<p>根据s，a，r的轨迹，找出总计回报最大的轨迹，以此制定策略。</p>
<h5 id="Return回报："><a href="#Return回报：" class="headerlink" title="Return回报："></a>Return回报：</h5><p>也叫cumulative future reward 未来的累积奖励。</p>
<p><img src="/../img/image-20220928223924496.png" srcset="/img/loading.gif" lazyload alt="image-20220928223924496"></p>
<p>$U_t$代表累积的所有奖励，也就是回报，同时有个问题，对于此刻的你来说，你是更愿意要现在的一百块还是明年的一百块，那肯定是现在。而如果改成今年的八十，和明年的一百，结果可能就是明年，因此得出结论：</p>
<ul>
<li>现在的奖励比未来的奖励更有价值。</li>
<li>未来奖励的权重要比现在的奖励低。</li>
</ul>
<p><img src="/../img/image-20220928224403860.png" srcset="/img/loading.gif" lazyload alt="image-20220928224403860"></p>
<p>由于这种原因，所以强化学习中普遍使用折扣回报，其中折扣率$\gamma$是一个超参数，介于0和1之间，如果未来的奖励不重要，$\gamma$就小，重要的话$\gamma$就大。这个参数需要我们自己来调，对结果有一定影响。</p>
<p>如果在游戏还没有结束的时候看$U_t$，那么已经观测到的值就不再是变量，用小写r来表示，如果还未观测到，那就还是个变量，用大写R来表示。</p>
<p><img src="/../img/image-20220928231508146.png" srcset="/img/loading.gif" lazyload alt="image-20220928231508146"></p>
<p><img src="/../img/image-20220928231524670.png" srcset="/img/loading.gif" lazyload alt="image-20220928231524670"></p>
<h5 id="回报的随机性来源："><a href="#回报的随机性来源：" class="headerlink" title="回报的随机性来源："></a>回报的随机性来源：</h5><p><img src="/../img/image-20220928230716500.png" srcset="/img/loading.gif" lazyload alt="image-20220928230716500"></p>
<p>AI的随机性来源有两个，一是action的随机性，二是状态转移的随机性。</p>
<p>对于从t时刻起的任意一个时刻，该时刻的奖励都取决于该时刻的状态和该时刻的动作，因此，给定一个t时刻的状态，该时刻的回报$U_t$就取决于所有t时刻往后的动作和状态转移。</p>
<p>所以回报$U_t$的随机性来源就是未来所有时刻的动作和状态，当没有观测到具体值的时候，它也是一个随机变量。</p>
<h5 id="价值函数Value-Function"><a href="#价值函数Value-Function" class="headerlink" title="价值函数Value- Function"></a>价值函数Value- Function</h5><p><img src="/../img/image-20220929101346598.png" srcset="/img/loading.gif" lazyload alt="image-20220929101346598"></p>
<p>$U_t$是个随机变量，依赖于t时刻之后所有的动作$A_t,A_{t+1},A_{t+2},…$和未来所有的状态$S_t,S_{t+1},S_{t+2},…$，所以我们并不知道t时刻的$U_t$是什么。</p>
<p>那我们该如何评估当前的形势呢？我们可以对$U_t$求期望，把里面的随机性$A_{t+1},A_{t+2},…,S_{t+1},S_{t+2},…$都用积分积掉，得到的就是一个实数。这样$Q_\pi$就只与当前动作和状态有关，并且积分的时候，还会用到policy函数$\pi$和p。</p>
<p>动作价值函数的意义就是，如果用policy函数$\pi$，那么在当前状态$s_t$下做动作$a_t$是好还是坏。已知policy函数$\pi$的情况下，$Q_\pi$就会给当前状态下所有的动作打分，然后我们就知道哪个动作好哪个动作不好。因此，有不同的policy函数$\pi$，就会有不同的$Q_\pi$。</p>
<p>怎么把$Q_\pi$里的$\pi$去掉呢？可以对$Q_\pi$关于$\pi$求最大化，意思是我们有无数种policy函数$\pi$，而我们要求最好的那种，也就是可以让$Q_\pi$最大化的那种。</p>
<script type="math/tex; mode=display">
Q^\star(s_t,a_t)=\max_\pi Q_\pi(s_t,a_t)</script><p>我们把得到的这个最大化的函数称为最优动作价值函数，它的直观意义就是不管用什么样的决策函数，得到的最大期望也就是这。</p>
<p><img src="/../img/image-20220929104613868.png" srcset="/img/loading.gif" lazyload alt="image-20220929104613868"></p>
<p>状态价值函数$V_\pi$，它是动作价值函数的期望，可以把这里的动作A作为随机变量，然后关于动作A求期望，A的概率密度函数为$\pi$，根据期望定义，可以把它写成积分（假设动作是连续的形势，比如汽车驾驶方向盘的角度）或者连加（假设动作是上下左右这种离散的形式）的形式，把A消掉。它的意义是告诉我们当前的形势好不好，举个例子，如果在下围棋，让$V_\pi$看一眼棋盘，它就可以告诉我们当前的胜算有多大。</p>
<p>总结：$Q_\pi$衡量当前状态下使用$\pi$决策的话a动作的好坏，和$\pi,s,a$都有关，是$U_t$的条件期望。</p>
<p>$V_\pi$用积分把A去掉，这样就只和s有关，衡量的是使用$\pi$决策的话当前形势s的好坏。它也可以衡量$\pi$决策的好坏，如果policy越好，那么$V_\pi$的均值就越大，即$E_S(V_\pi)$就越大。</p>
<h3 id="强化学习实践"><a href="#强化学习实践" class="headerlink" title="强化学习实践"></a>强化学习实践</h3><p><img src="/../img/image-20220929110329614.png" srcset="/img/loading.gif" lazyload alt="image-20220929110329614"></p>
<p>如何操纵AI打游戏？</p>
<p>一、学习一个policy函数$\pi(a|s)$，即策略学习</p>
<p>每观测一个状态$s_t$，就输入策略函数，$\pi$函数会输出每一个动作的概率，将这些动作来做一个随机抽样，得到$a_t$。</p>
<p>二、学习最优动作价值函数，即价值学习</p>
<p>每输入一个状态，选择能得分最大的动作。</p>
<p>这两种方法都可行，所以强化学习的任务就是学习这两个函数之一。</p>
<p>如果设计出了这种算法，就可以把算法用于各种问题上。</p>
<p>强化学习最常用的库：OpenAI Gym</p>
<ol>
<li>经典控制问题</li>
<li>小游戏</li>
<li>连续控制MuJoCo</li>
</ol>
<p><img src="/../img/image-20220929110851316.png" srcset="/img/loading.gif" lazyload alt="image-20220929110851316"></p>
<p>使用gym测试算法优劣：</p>
<ol>
<li>导包</li>
<li>生成环境</li>
</ol>
<p><img src="/../img/image-20220929111106897.png" srcset="/img/loading.gif" lazyload alt="image-20220929111106897"></p>
<p>解释代码：</p>
<ol>
<li>重置环境，返回初始状态state</li>
<li>进入循环</li>
<li>每一轮循环里，先渲染环境，将画面展示给人看</li>
<li>算出一个action，这里是均匀抽样</li>
<li>环境做出action，并更新state，给出一个reward，返回done和info</li>
<li>如果done=1游戏结束，直接结束循环</li>
</ol>
<h5 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><p>机器学习的核心内容就是把数据喂给一个人工设计的模型，然后让模型自动的“学习”，从而优化模型自身的各种参数，最终使得在某一组参数下该模型能够最佳的匹配该学习任务。</p>
<p>梯度下降法就是实现该“学习”过程的一种最常见的方式，尤其是在深度学习(神经网络)模型中，BP反向传播方法的核心就是对每层的权重参数不断使用梯度下降来进行优化。</p>
<p>梯度下降法(gradient descent)是一种常用的一阶(first-order)优化方法，是求解无约束优化问题最简单、最经典的方法之一。我们来考虑一个无约束优化问题 $\min_xf(x) $, 其中 f(x) 为连续可微函数，如果我们能够构造一个序列$ x_0,x_1,x_2,… $，并能够满足：</p>
<p>$f(x^{t+1})&lt;f(x^t),t=0,1,2,…$</p>
<p>那么我们就能够不断执行该过程即可收敛到局部极小点，可参考下图。</p>
<p><img src="/../img/image-20220929150613619.png" srcset="/img/loading.gif" lazyload alt="image-20220929150613619"></p>
<p>现在我们随机找了一个初始的点 x1 ，对于一元函数来说，函数值只会随着 x 的变化而变化，那么我们就设计<strong>下一个 $x^{t+1 }$是从上一个$ x^t $沿着某一方向走一小步$ Δx $得到的。此处的关键问题就是：这一小步的方向是朝向哪里？</strong></p>
<p><img src="/../img/v2-5e43a5099214417029720384e5ac4a17_r.jpg" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>//返回x的平方<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.power(x, <span class="hljs-number">2</span>)<br>//返回2x<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">d_f_1</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2.0</span> * x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">d_f_2</span>(<span class="hljs-params">f, x, delta=<span class="hljs-number">1e-4</span></span>):<br>		//返回x处的斜率<br>    <span class="hljs-keyword">return</span> (f(x+delta) - f(x-delta)) / (<span class="hljs-number">2</span> * delta)<br><br><br><span class="hljs-comment"># plot the function</span><br>//返回一个-<span class="hljs-number">10</span>到<span class="hljs-number">10</span>的数组<br>xs = np.arange(-<span class="hljs-number">10</span>, <span class="hljs-number">11</span>)<br>//画出-<span class="hljs-number">10</span>到<span class="hljs-number">10</span>的x平方的图像<br>plt.plot(xs, f(xs))<br>plt.show()<br><br>learning_rate = <span class="hljs-number">0.1</span><br>max_loop = <span class="hljs-number">30</span><br>//用梯度下降法迭代<span class="hljs-number">30</span>次求f(x)=x^<span class="hljs-number">2</span>的最小值<br>x_init = <span class="hljs-number">10.0</span><br>x = x_init<br>lr = <span class="hljs-number">0.1</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_loop):<br>    <span class="hljs-comment"># d_f_x = d_f_1(x)</span><br>    //把x处的斜率付给d_f_x<br>    d_f_x = d_f_2(f, x)<br>    //更新x，减去步长alpha*d_f_x<br>    x = x - learning_rate * d_f_x<br>    <span class="hljs-built_in">print</span>(x)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;initial x =&#x27;</span>, x_init)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;arg min f(x) of x =&#x27;</span>, x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;f(x) =&#x27;</span>, f(x))<br></code></pre></td></tr></table></figure>
<p>输出内容如下：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs tex">8.000000000004661<br>6.400000000004837<br>5.120000000003557<br>4.0960000000043095<br>3.276800000005622<br>2.6214400000032967<br>2.097152000001259<br>1.6777216000003392<br>1.3421772800004028<br>1.0737418240003205<br>0.8589934592002546<br>0.6871947673602241<br>0.5497558138881831<br>0.4398046511105336<br>0.35184372088844174<br>0.2814749767107627<br>0.22517998136861533<br>0.18014398509489674<br>0.14411518807592116<br>0.11529215046073862<br>0.09223372036859068<br>0.07378697629487216<br>0.05902958103589708<br>0.047223664828717364<br>0.03777893186297355<br>0.03022314549037856<br>0.024178516392302875<br>0.019342813113842325<br>0.015474250491073885<br>0.012379400392859128<br>initial x = 10.0<br>arg min f(x) of x = 0.012379400392859128<br>f(x) = 0.00015324955408672073<br></code></pre></td></tr></table></figure>
<p>经过30次迭代 x 从初始点10.0逐步逼近最小点 f(0) .</p>
<h5 id="无偏估计"><a href="#无偏估计" class="headerlink" title="无偏估计"></a>无偏估计</h5><p>不论总体服从什么分布，样本均值是总体均值的无偏估计量。</p>
<h5 id="蒙特卡洛近似"><a href="#蒙特卡洛近似" class="headerlink" title="蒙特卡洛近似"></a>蒙特卡洛近似</h5><p>来源：蒙特卡洛模拟是在计算机上模拟项目上实施了成千上万次，每次输入都随机选择输入值。由于每个输入很多时候本身就是一个估计区间，因此计算机模型会随机选取每个输入的该区间内的任意值，通过大量成千上万甚至百万次的模拟次数，最终得出一个<strong>累计概率分布图</strong>，这个就是蒙特卡洛模拟。</p>
<p>举例：敌人以两门火炮为单位对我方进行干扰，进并经常变换射击地点。经过长期观察发现，我方指挥所对敌方目标的指示有50%是准确的，而我方火力单位在指示正确时，有1/3的射击效果能毁伤敌人1门火炮，有1/6的射击效果能全部消灭敌人。</p>
<p>解：希望能用某种方法把我方将要对敌人实施的20次打击结果显示出来，确定有效射击的比率及毁伤敌方火炮的平均值。这是一个概率问题，可以通过理论计算得到相应的概率和期望值。但这样只能给出作战行动的最终静态结果，而显示不出作战行动的动态过程。</p>
<p>为了显示我方20次射击的过程，必须用某种方式模拟出以下两件事：一是观察所对目标的指示正确或不正确；二是当指示正确时，我方火力单位的射击结果。对第一件事进行模拟试验时有两种结果，每一种结果出现的概率都是1/2。因此，可用投掷1枚硬币的方式予以确定。当硬币出现正面时为指示正确，反之为不正确。对第二件事进行模拟试验时有3种结果，毁伤1门火炮的可能为1/3，毁伤2门火炮的可能为1/6，没能毁伤敌火炮的可能为1/2。这时，可用投掷骰子的办法来确定，如果出现的是1、2、3三个点则认为没能击中敌人，如果出现的是4、5点则认为毁伤敌1门火炮，如果出现6点则认为毁伤敌2门火炮。</p>
<p>通过上面的方式，就可把我方20次射击的过程动态地显现出来。</p>
<p>使用蒙特卡罗法的基本步骤如下：</p>
<p>(1)根据作战过程的特点构造模拟模型；</p>
<p>(2)确定所需要的各项基础数据；</p>
<p>(3)使用可提高模拟精度和收敛速度的方法；</p>
<p>(4)估计模拟次数；</p>
<p>(5)编制程序并在计算机上运行；</p>
<p>(6)统计处理数据，给出问题的模拟结果及其精度估计。</p>
<p>在蒙特卡罗法中，对同一个问题或现象可采用多种不同的模拟方法，它们有好有差，精度有高有低，计算量有大有小，收敛速度有快有慢，在方法的选择上有一定的技巧。</p>
<h3 id="价值学习"><a href="#价值学习" class="headerlink" title="价值学习"></a>价值学习</h3><p>$U_t$是一个随机变量，为了消除它的随机性，求它的积分并取期望得到动作价值函数$Q_\pi$，来反映当前状态下做动作的好坏。动作的随机性是由$\pi$函数随机抽样得到的，状态的随机性是由p函数环境机制随机抽样得到的。</p>
<p> <img src="/../img/image-20220929144820687.png" srcset="/img/loading.gif" lazyload alt="image-20220929144820687"></p>
<h5 id="Deep-Q-Network-DQN-深度神经网络"><a href="#Deep-Q-Network-DQN-深度神经网络" class="headerlink" title="Deep Q-Network(DQN)深度神经网络"></a>Deep Q-Network(DQN)深度神经网络</h5><p>其实就是神经网络用了近似$Q^\star$函数，因为$Q^\star$其实是不存在的，除非存在一个先知，我们只能用神经网络去近似的学习它。</p>
<p>我们把这个神经网络叫做$Q(s,a;w)$，w是参数，s是输入，输出是对所有可能动作的打分。我们通过奖励来学习神经网络，它的打分会越来越准。</p>
<p><img src="/../img/image-20220929162821122.png" srcset="/img/loading.gif" lazyload alt="image-20220929162821122"></p>
<p>不同问题的DQN结构也可能不一样，举个例子，玩超级玛丽，把显示画面作为输入，用一个Conv卷积层把图片变成特征向量，最后用几个Dense权连接层把特征映射到几个输出向量。</p>
<p>这个输出向量是对动作的打分，向量的每一个元素对应一个动作。因为向上跳得分最高，所以DQN会操纵Mario向上跳。只要训练好DQN，就可以打赢游戏并且得高分了。</p>
<p><img src="/../img/image-20220929163610040.png" srcset="/img/loading.gif" lazyload alt="image-20220929163610040"></p>
<h5 id="怎么训练DQN？"><a href="#怎么训练DQN？" class="headerlink" title="怎么训练DQN？"></a>怎么训练DQN？</h5><p>最常用的是TD算法。</p>
<p><img src="/../img/image-20220929163937684.png" srcset="/img/loading.gif" lazyload alt="image-20220929163937684"></p>
<p>举个例子，如果想开车从纽约去亚特兰大，模型初步预估是1000分钟。但实际上只用了860分钟，因此就产生了损失Loss，对Loss关于模型参数w求导，得出结果。</p>
<p>梯度求出来了，就可以用梯度下降法找Loss的最小值，更新模型参数w，指定一个步长，或者说是学习率。在经过这次迭代后，得到$W_{t+1}$，用$W_{t+1}$做预测的话，会准确很多。</p>
<p>但是这种方法很sb，完成整个旅途也只能对model做一次更新。</p>
<p>下面是一种新的半场开香槟方法：</p>
<p><img src="/../img/image-20220929175259409.png" srcset="/img/loading.gif" lazyload alt="image-20220929175259409"></p>
<p><img src="/../img/image-20220929174723038.png" srcset="/img/loading.gif" lazyload alt="image-20220929174723038"></p>
<p>途径DC的时候看了眼表，发现只用了300min，这时候更新model，判断余下的路程只用600min，这个900也是TD Target，但是比上一种方法来的更准确。得到Loss梯度后更新模型参数，完成一次迭代。</p>
<p>什么情况下可以用TD算法？答案是右边必须有一项是真实观测到的。</p>
<p><img src="/../img/image-20220929175417403.png" srcset="/img/loading.gif" lazyload alt="image-20220929175417403"></p>
<p>在深度强化学习中也有这个公式，左边是t时刻的估计，这是未来奖励总和的期望，相当于纽约到亚特兰大的总时间。</p>
<p>等式右边有一项rt，是真实观测到的奖励。等式右边第二项，是DC到亚特兰大的预计时间。</p>
<p><img src="/../img/image-20220929175827549.png" srcset="/img/loading.gif" lazyload alt="image-20220929175827549"></p>
<p>相邻两个折扣回报的关系如上推导，现在要把TD算法用到DQN上：</p>
<p><img src="/../img/image-20220929180042719.png" srcset="/img/loading.gif" lazyload alt="image-20220929180042719"></p>
<p>有了以上两个参数，就可以更新DQN模型的参数了。</p>
<p><img src="/../img/image-20220929180334200.png" srcset="/img/loading.gif" lazyload alt="image-20220929180334200"></p>
<p>已知预测部分和$r_t$，还观测到了新的状态$s_{t+1}$，有了这个状态，就可以根据DQN算出下一个动作$a_{t+1}$了。因此，在t+1时刻，$y_t,s_{t+1},a_{t+1}$都是已知的，就可以得出TD target $y_t$。</p>
<p>其中，$a_{t+1}$是由$s_{t+1}$下的DQN对各个动作依次打分，得出的那个最高的动作，也就是Q函数关于a求最大化。</p>
<p>我们希望预测尽量接近TD target，因此定义损失函数得到梯度，然后做梯度下降来更新模型参数。</p>
<p>总结：</p>
<p>TD算法的步骤：</p>
<ol>
<li>观察t时刻状态和动作。</li>
<li>输入当前状态，然后对动作进行打分，得到$Q_t(s_t,a_t;w_t)$。</li>
<li>用反向传播对DQN求导，得到梯度$d_t$。</li>
<li>执行动作后，环境会更新状态并给出奖励。</li>
<li>得出TD Target。</li>
<li>更新模型参数。</li>
</ol>
<p>每得到一次r，就可以完成一次迭代，更新一次参数。</p>
<p><img src="/../img/image-20220929181756642.png" srcset="/img/loading.gif" lazyload alt="image-20220929181756642"></p>
<h3 id="策略学习"><a href="#策略学习" class="headerlink" title="策略学习"></a>策略学习</h3><p>我们用一个神经网络来近似这个策略函数，这个神经网络叫做策略网络(Policy-Network)，可以用来控制agent运动，训练这个网络需要用到Policy-gredient算法。</p>
<p>$\pi(a|s)$函数是一个PDF函数，s作为输入，输出每种动作的可能性。我们可以直接学习这个函数吗？如果s是有限的，我们只需要把所有s列举出来，归纳总结就可以，但是在超级玛丽这个游戏中，s明显是无限的，所以我们需要建立一个策略网络来学习这个策略函数。</p>
<p>策略网络$\pi(a|s;\theta)$中，$\theta$是策略网络的参数。</p>
<p><img src="/../img/image-20220930084506357.png" srcset="/img/loading.gif" lazyload alt="image-20220930084506357"></p>
<p>因为$\pi$是一个PDF函数，所以必须保证加和等于1，因此需要用一个Softmax函数，来做归一化。</p>
<h5 id="策略学习主要思想"><a href="#策略学习主要思想" class="headerlink" title="策略学习主要思想"></a>策略学习主要思想</h5><p>用策略网络来近似策略函数，这样状态价值函数就可以写成如下形式，s的形势越好，V就越大，那怎么样让策略网络越变越好？我们可以改进模型参数$\theta$，让$V(s;\theta)$来变大。基于这个想法，我们可以把目标函数定义为V的关于S的期望，把S当作是随机变量，求期望的时候积分掉。这样<script type="math/tex">J(\theta)</script>就是对策略网络的期望，策略网络越好，$J(\theta)$就越大。</p>
<p>怎么样改进$\theta$呢？我们要用到策略梯度算法。让agent玩游戏，每观测到一个s，就相当于从状态中随机抽样出了一个状态，对V求导得到策略梯度，乘上学习率，用梯度上升的方法来迭代$\theta$。</p>
<p><img src="/../img/image-20220930100055379.png" srcset="/img/loading.gif" lazyload alt="image-20220930100055379"> </p>
<p>为什么是梯度上升不是梯度下降呢？因为这又不是损失函数，我们是想让$J(\theta)$越变越大的。</p>
<h5 id="怎么计算策略梯度"><a href="#怎么计算策略梯度" class="headerlink" title="怎么计算策略梯度"></a>怎么计算策略梯度</h5><p><img src="/../img/image-20220930101048159.png" srcset="/img/loading.gif" lazyload alt="image-20220930101048159"></p>
<p>看以上计算过程，虽然Q显然不能直接做常数处理，因为$\theta$神经网络参数也来源于动作价值函数，但是不管了，直接近似了。将原式作下面的变换，得到一个新的展开式。乘了每一个情况下a的PDF函数，就可以变换成对A的期望了。</p>
<p><img src="/../img/image-20220930101456099.png" srcset="/img/loading.gif" lazyload alt="image-20220930101456099"></p>
<p>由此得到了策略梯度。</p>
<h5 id="蒙特卡洛近似策略梯度"><a href="#蒙特卡洛近似策略梯度" class="headerlink" title="蒙特卡洛近似策略梯度"></a>蒙特卡洛近似策略梯度</h5><p>如果是离散变量，我们可以直接把每个动作都枚举出来，但实际上通常不是。</p>
<p><img src="/../img/image-20220930102130793.png" srcset="/img/loading.gif" lazyload alt="image-20220930102130793"></p>
<p>如果是连续变量，计算太复杂了，Q是一个神经网络，很难把它积出来，所以我们只好用蒙特卡洛近似QAQ，把这个期望近似算出来。</p>
<p>步骤：</p>
<ol>
<li><p>随机抽样得到一个动作$\hat{a}$，抽样是根据PDF函数$\pi$来抽的。</p>
</li>
<li><p>这里把期望公式记做$g(\hat{a},\theta)$，因为这里的$\hat{a}$是一个确定的动作，所以直接算出g的具体值就可以了。</p>
</li>
</ol>
<p><img src="/../img/image-20220930150738529.png" srcset="/img/loading.gif" lazyload alt="image-20220930150738529"></p>
<p>根据定义，$g(A,\theta)$关于A求期望，就等于策略梯度。并且由于$\hat{a}$是根据PDF函数$\pi$随机抽出来的，所以$g(\hat{a},\theta)$是关于策略梯度函数的一个无偏估计，因此，可以用它来近似策略梯度，这就是蒙特卡洛近似。</p>
<p>蒙特卡洛近似就是抽一个或者很多个随机样本，用随机样本来近似期望，更新模型参数的时候，用$g(\hat{a},\theta)$来作为近似的梯度就可以了，不用计算出精确的策略梯度。</p>
<p>总结</p>
<ol>
<li>在第t个时间点，观测到状态$s_t$。</li>
<li>把策略网络$\pi(*|s_t;\theta_t)$作为概率密度函数，用它随机抽样得到一个动作$a_t$。</li>
<li>计算价值函数$Q_\pi(s_t,a_t)$的值。</li>
<li>对策略网络求导，算出$\log\pi$关于参数$\theta$的导数。</li>
<li>蒙特卡洛近似算出策略梯度。</li>
<li>用策略梯度来更新策略网络。</li>
</ol>
<p><img src="/../img/image-20220930153220538.png" srcset="/img/loading.gif" lazyload alt="image-20220930153220538"></p>
<p>如何计算$Q_\pi(s_t,a_t)$？</p>
<p>方法一：Reinforce</p>
<p>用策略网络$\pi$来控制agent运动，从一开始一直玩到游戏结束，记录整个游戏的轨迹，就可以计算出整个游戏的$u_t$，而$Q_\pi(s_t,a_t)$又是$U_t$的期望，可以用观测值$u_t$来近似$Q_\pi(s_t,a_t)$。</p>
<p>所以，该方法就是用观测到的$u_t$来代替$Q_\pi(s_t,a_t)$。</p>
<p>方法二：用一个神经网络来做近似</p>
<p>不是已经有了一个神经网络用来近似策略函数了吗，现在再用一个神经网络来近似动作价值函数$Q_\pi$。一个actor演员，一个critic评论员，也叫actor critic算法。</p>
<p><img src="/../img/image-20220930154240239.png" srcset="/img/loading.gif" lazyload alt="image-20220930154240239"></p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/">#机器学习基础理论</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/09/30/Machine-Learning/" title="Machine Learning">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Machine Learning</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/09/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="神经网络">
                        <span class="hidden-mobile">神经网络</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="vcomment" class="comment"></div> 
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
  <script>
    var notify = '' == true ? true : false;
    var verify = '' == true ? true : false;
      window.onload = function() {
          new Valine({
              el: '#vcomment',
              app_id: "AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz",
              app_key: "kMsGLN3hzkQJuLrmqQBgquFF",
              placeholder: "说点什么",
              avatar:"retro",
              visitor: true       

          });
      }
  </script>

 
  <noscript>Please enable JavaScript to view the comments</noscript>



  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  




  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  
    
  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  
      <script>
        MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>

  <script defer src="/js/leancloud.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/yinghua.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiantiao.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiaoxingxing.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
