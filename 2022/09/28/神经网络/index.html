

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.jpg">
  <link rel="icon" href="/img/icon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="参考：损失函数与期望风险 参考：期望、方差和协方差的含义 神经网络数据分割举个例子，判断北京和郑州的房价，定义房价&#x3D;价格&#x2F;面积&#x3D;x&#x2F;y，高于0.69的属于北京，低于0.69的属于郑州。并且写成f(x,y)&#x3D;x-0.69y的形式，当f(x,y)&gt;0时，判定是北京，小于0判定为郑州。 更现实的情况是，这个能正确区分北京和郑州的阈值并不会恰好是0，打个比方是10，我们仍然可以对函数进行校准，需要">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络">
<meta property="og:url" content="http://example.com/2022/09/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="摸鱼之家">
<meta property="og:description" content="参考：损失函数与期望风险 参考：期望、方差和协方差的含义 神经网络数据分割举个例子，判断北京和郑州的房价，定义房价&#x3D;价格&#x2F;面积&#x3D;x&#x2F;y，高于0.69的属于北京，低于0.69的属于郑州。并且写成f(x,y)&#x3D;x-0.69y的形式，当f(x,y)&gt;0时，判定是北京，小于0判定为郑州。 更现实的情况是，这个能正确区分北京和郑州的阈值并不会恰好是0，打个比方是10，我们仍然可以对函数进行校准，需要">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/photo37.jpeg">
<meta property="article:published_time" content="2022-09-28T10:26:43.000Z">
<meta property="article:modified_time" content="2022-11-11T15:32:13.986Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="机器学习基础理论">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/photo37.jpeg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>神经网络 - 摸鱼之家</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.1","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":false},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz","app_key":"kMsGLN3hzkQJuLrmqQBgquFF","server_url":"https://awhbuajs.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>快乐老家</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                主页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                档案馆
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                目录
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/photo37.jpeg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="神经网络"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-09-28 18:26" pubdate>
          September 28, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          40k words
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">神经网络</h1>
            
              <p class="note note-info">
                
                  
                    Last updated on 6 days ago
                  
                
              </p>
            
            <div class="markdown-body">
              
              <hr>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yawei_liu1688/article/details/113528209">参考：损失函数与期望风险</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/MoreAction_/article/details/106230690">参考：期望、方差和协方差的含义</a></p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="数据分割"><a href="#数据分割" class="headerlink" title="数据分割"></a>数据分割</h2><p>举个例子，判断北京和郑州的房价，定义房价=价格/面积=x/y，高于0.69的属于北京，低于0.69的属于郑州。并且写成<script type="math/tex">f(x,y)=x-0.69y</script>的形式，当f(x,y)&gt;0时，判定是北京，小于0判定为郑州。</p>
<p>更现实的情况是，这个能正确区分北京和郑州的阈值并不会恰好是0，打个比方是10，我们仍然可以对函数进行校准，需要写成<script type="math/tex">f(x,y)=x-0.69y-10</script>的形式。</p>
<p>在这个式子里，x的量纲是1，y的量纲是-0.69，每份y贡献-0.69，每份x贡献1，贡献度正好是x,y的系数。依此类推，当出现更多影响因素时，<script type="math/tex">f(价格，面积，距离，楼层，环境，......)</script>，每个维度都可以看作一个变量，最终决定这个房子值不值得购买。</p>
<p>写成变量的形式：$f(x_1,x_2,x_3,x_4,x_5,……)=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5+…+b=y$$，$其中$w_i$决定$x_i$对y的贡献程度，也叫权重系数，b决定的是阈值的偏移程度，也叫偏置系数。</p>
<p>在坐标图上看样本点，一条分界线$0.0018x_1-0.0012x_2-1.78=0$将数据分割成上下两部分，在确定分界线后，数据划分也就确定了。所以我们的目标就是找到这条分界线。</p>
<p><img src="/../img/image-20221002002558453.png" srcset="/img/loading.gif" lazyload alt="image-20221002002558453"></p>
<p>神经网络分为两部分：</p>
<ol>
<li>决策/预测：在分界线确定之后，对于任意一个新数据，我们都可以根据它在分界线上的位置来判断它属于哪一类。但是机器不会看图，所以就会根据将x代入y后大于或者小于0来分类。f(x)&gt;0则在直线上方，反之在下方。</li>
<li>训练/学习：这个过程就是如何找到那条分界线，分界线并不唯一，并且事实上根本不存在一条完美的分界线，我们只能从中选择一条还不错的。所以最重要的是找到判断分界线好坏的标准。</li>
</ol>
<p><u>看一下下面这几个：</u></p>
<p>对比学习：有人类做实验的意思了，随机对照实验</p>
<p>迁移学习：拿了训练好的基础，再基础上继续训练</p>
<p>强化学习：在给定的基础定义（游戏规则）上，枚举（穷举），统计概率</p>
<p>强化学习+系统自己随机生成基础定义（游戏规则）就能实现大胆猜想，小心验证的通用智能了。</p>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>分界线标准：在已有的数据样本点上，选出一条能把所有数据完美分割开的分割线是否可行呢？不行，这样只能做到对已知的数据最好，无法保证对未知的数据也是最好的。所以我们选择了一个判断标准之后，这里面就应该包含对未来数据可能情况的一种预测。这在数学逻辑上无法做到，因此引入公设（一种设定，可以有很多种这样的设定）。</p>
<p>公设一：</p>
<p>算出离直线最近的点到直线的距离，这个距离越大越好，最小间隔函数如下：</p>
<script type="math/tex; mode=display">
M(W)=\min_{i=1,2,...,N}\frac{1}{||W||_2}|f(X^{(i)}|</script><p>我们要找的是使最小间隔最大的那一组系数<script type="math/tex">W=w_1,w_2,...w_N,b</script>，在不考虑算力的情况下，计算机可以自己完成这样的选择。</p>
<p>公设二：</p>
<p><img src="/../img/image-20221002005643117.png" srcset="/img/loading.gif" lazyload alt="image-20221002005643117"></p>
<p>等式左边的意思是在给定系数的条件下，取到该数据点的概率是多少，把数据点输入f(x)，通过sigmoid激活函数后，f(x)值越大，输出越接近于1，左边的概率值也就越接近于1。因为f(x)&gt;0时分类为北京，f(x)<0时分类为郑州，所以当f(x)>0大于的越多，该地属于北京的概率就越大，而-f(x)&gt;0大于的越多，就代表该地属于郑州的概率就越大。</0时分类为郑州，所以当f(x)></p>
<p>因此得到公设：让所有的数据点对应的似然值同时最大的时候最好。因为在这里变动的不是随机变量X，而是条件W，所以不叫概率值，叫似然值。</p>
<blockquote>
<p>概率是随机变量的概率，似然是概率分布函数参数的似然。</p>
<p>有时我们不能在实验前获取随机变量概率分布函数中的具体参数值，这个时候我们通过大量试验收集样本数据，统计样本结果，来推测<strong>参数取值的可能性</strong>，此时这个<strong>可能性大小就是似然值</strong>，这个推测参数取值最大可能性的过程也就是后面我们要讲解的<strong>最大似然估计</strong>。</p>
</blockquote>
<p>因为是所有概率值同时最大，所以是一个相乘的关系，列出如下表达式，并用log把连乘变成连加。</p>
<p><img src="/../img/image-20221002011344066.png" srcset="/img/loading.gif" lazyload alt="image-20221002011344066"></p>
<p>公设三：</p>
<p>假设存在一个可以完美将数据分类的T(x)函数，用图像表示：</p>
<script type="math/tex; mode=display">
T(X)=\begin{cases} 1:X\in\{北京\}\\  0:X\in\{长治\}， \end{cases}</script><p><img src="/../img/image-20221002163443188.png" srcset="/img/loading.gif" lazyload alt="image-20221002163443188"></p>
<p>现实中会有噪声存在，对T叠加一个噪声后，就不再是非0即1的情况了，</p>
<script type="math/tex; mode=display">
T(X)+\epsilon=\begin{cases} 1+\epsilon:X\in\{北京\}\\  0+\epsilon:X\in\{长治\}， \end{cases}</script><p>这时候sigmoid函数又可以发挥作用了,</p>
<script type="math/tex; mode=display">
T(X)+\epsilon=sigmoid(f(X))</script><p><img src="/../img/image-20221002164927433.png" srcset="/img/loading.gif" lazyload alt="image-20221002164927433"></p>
<p> 对f(X)取值sigmoid后与T(X)的偏差，就是定义的误差项$\epsilon$，我们希望这个误差越小越好，也就是说，方差越小越好。</p>
<p><img src="/../img/image-20221002165207081.png" srcset="/img/loading.gif" lazyload alt="image-20221002165207081"></p>
<p>还有更多的公设，在数学上都可以证明。</p>
<h3 id="数学知识"><a href="#数学知识" class="headerlink" title="数学知识"></a>数学知识</h3><p>sigmoid函数：<img src="/../img/image-20221002174627021.png" srcset="/img/loading.gif" lazyload alt="image-20221002174627021"></p>
<p> 做指数变形，把$\hat{y}$从实数域投射到$(0,\infty)$ </p>
<p><img src="/../img/image-20221002174851249.png" srcset="/img/loading.gif" lazyload alt="image-20221002174851249"></p>
<p>softmax函数：<img src="/../img/image-20221002175019842.png" srcset="/img/loading.gif" lazyload alt="image-20221002175019842"></p>
<p>选择哪一种判断标准，也就是选择哪一种公设，在机器学习上叫做策略。<strong>算法（反向传播——梯度下降法）、策略（输出层）加上模型（隐藏层），就是机器学习的三大要素。</strong></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>线性方程在坐标图上的表现就是一条直线，增加多次项就相当于增加曲度，只要增加足够的多次项，无论多么奇怪的形状都能够表示出来，举个例子：</p>
<p><img src="/../img/image-20221002165841885.png" srcset="/img/loading.gif" lazyload alt="image-20221002165841885"></p>
<p>可见，随着次数的增加，曲线会越来越复杂，模型的复杂程度也会提高。 </p>
<p>虽然我们讨论的是分类问题，但是在曲线的拟合方面，和回归在模型上并没有什么本质的区别。当我们拟合出一条回归曲线后，任意给出一个$x_1$的值，我们都可以预测出它对应的$x_2$的数值。这时候完成的就是预测而不是分类。</p>
<p> 既然直线这么不好用，我们就提高模型的复杂程度来分类，神经网络做的就是这样的工作。</p>
<p><img src="/../img/image-20221002170454030.png" srcset="/img/loading.gif" lazyload alt="image-20221002170454030"></p>
<p>神经网络最左边是输入层，对应的是数据的维度，神经网络里任意一个神经元，都会和上一层，下一层所有的节点相连，所以叫全连接神经网络。</p>
<p><img src="/../img/image-20221002170733234.png" srcset="/img/loading.gif" lazyload alt="image-20221002170733234"></p>
<p>看个最简单的，每一条线就代表一个权重，连起来就是一个线性方程。</p>
<p><img src="/../img/image-20221002170823104.png" srcset="/img/loading.gif" lazyload alt="image-20221002170823104"></p>
<p> 然后它的结果z要经过激活函数后才会传给下一层，激活过程用神经元统一表示。</p>
<p>以二维输入向量举例，这个过程就是这样的：</p>
<p><img src="/../img/image-20221002171950200.png" srcset="/img/loading.gif" lazyload alt="image-20221002171950200"></p>
<p><img src="/../img/image-20221002171058240.png" srcset="/img/loading.gif" lazyload alt="image-20221002171058240"></p>
<p>在经过激活函数前：</p>
<p><img src="/../img/image-20221002171434650.png" srcset="/img/loading.gif" lazyload alt="image-20221002171434650"></p>
<p>经过激活函数后：</p>
<p><img src="/../img/image-20221002171111001.png" srcset="/img/loading.gif" lazyload alt="image-20221002171111001"></p>
<p>再加一层神经元：上一层的输出是下一层的输入：</p>
<p><img src="/../img/image-20221002171809601.png" srcset="/img/loading.gif" lazyload alt="image-20221002171809601"></p>
<p><img src="/../img/image-20221002171623928.png" srcset="/img/loading.gif" lazyload alt="image-20221002171623928"></p>
<p>给第一层多加一个神经元：，每层的输出就从单值变成向量了：</p>
<p><img src="/../img/image-20221002171914583.png" srcset="/img/loading.gif" lazyload alt="image-20221002171914583"></p>
<p><img src="/../img/image-20221002172231769.png" srcset="/img/loading.gif" lazyload alt="image-20221002172231769"></p>
<p> 依此类推，神经元数量越多，模型越复杂：</p>
<p><img src="/../img/image-20221002173141748.png" srcset="/img/loading.gif" lazyload alt="image-20221002173141748"></p>
<p>只要增加的够多，什么形状都能编出来，这就叫 万能逼近定理。实现这个定理最关键的是一个非线性的激活函数，否则再怎么增加神经元得到的也只是一个多维的平面。</p>
<blockquote>
<p>总之，神经网络的复杂性来自于激活函数。</p>
</blockquote>
<p>不同的激活函数会拟合出不同的形状，它们可以是这些：</p>
<p><img src="/../img/image-20221002173432075.png" srcset="/img/loading.gif" lazyload alt="image-20221002173432075"></p>
<p>原理：举个例子，加入输入只是一个二维向量，经过矩阵运算后，相当于把它从二维升到了三维，原本不可分的数据，现在即使用更简单的模型也可以把它们分开。</p>
<p><img src="/../img/image-20221002174123794.png" srcset="/img/loading.gif" lazyload alt="image-20221002174123794"></p>
<p>所以我们可以把中间的层看做是对输入的一个升维操作，有多少神经元就会把数据升到多少维。只要维度够高，一定能找到一个超平面，完成对数据的划分。因此，最后一层神经元只需要考虑策略的问题就可以了，该层是否需要激活函数由策略决定。</p>
<p>但如果只从隐藏层的作用是升维这一角度去理解，通常会发现隐藏层的神经元要少于输入个数，比如：<img src="/../img/image-20221002175255234.png" srcset="/img/loading.gif" lazyload alt="image-20221002175255234"></p>
<p>举个例子，在人脸识别中，图片的每个像素都是一个输入，最后对这些输入进行抽象，提取特征，才是我们需要的东西，真实的输入像素反而不是。</p>
<p><img src="/../img/image-20221002175534642.png" srcset="/img/loading.gif" lazyload alt="image-20221002175534642"></p>
<p>再举个例子，在字符识别中，把字符分割成不同大小，分割的最小最具体的一层作为最外层神经元，可复用性最强，个数也最多，越往里分割的部分越抽象，神经元个数也越小，因此隐藏层越深，抽象程度越高。</p>
<p><img src="/../img/image-20221002175642701.png" srcset="/img/loading.gif" lazyload alt="image-20221002175642701"></p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>在预测的时候，用到的是正向传播$f_W(X)=\hat{Y}$，只不过这个f是一个神经网络，运算特别复杂。在训练的时候，这个关系就会反过来，用来训练的是已知的真实数据X和Y，作为变量需要我们求解的是系数。</p>
<p><img src="/../img/image-20221002181308051.png" srcset="/img/loading.gif" lazyload alt="image-20221002181308051"></p>
<p>这个曲面的最高点就是我们在选择策略后想要达到的目标。但是在初始化时，系数可能出现在任何一个点上，理想情况下，它到最优点的距离就是我们希望求解的目标，但是白想，没有用。</p>
<p>但是有一点，那就是在这个曲面上任何一点， 我们都可以求出一个向量，这个向量总是指向上升最快的方向，这个向量在参数平面上的投影就叫做梯度。</p>
<p><img src="/../img/image-20221002182458208.png" srcset="/img/loading.gif" lazyload alt="image-20221002182458208"></p>
<p> 梯度下降法：梯度总是指向上升最快的方向，但是有的策略是求最大，有的是求最小，为了统一他们，即便是求最大的问题。也会在函数前面加一个负号，转换成求最小。</p>
<p><img src="/../img/image-20221002182914297.png" srcset="/img/loading.gif" lazyload alt="image-20221002182914297"></p>
<p>这里曲面虽然反了，但是梯度仍然是指向上升最快的方向，所以会先对梯度求反，让他指向下降最快的方法。</p>
<p><img src="/../img/image-20221002183649863.png" srcset="/img/loading.gif" lazyload alt="image-20221002183649863"></p>
<p>通过这个梯度进行反向传播，更新W的参数：</p>
<p><img src="/../img/image-20221002183846639.png" srcset="/img/loading.gif" lazyload alt="image-20221002183846639"></p>
<p>反向传播表达式：</p>
<p><img src="/../img/image-20221002184116647.png" srcset="/img/loading.gif" lazyload alt="image-20221002184116647"></p>
<p><img src="/../img/image-20221002184128040.png" srcset="/img/loading.gif" lazyload alt="image-20221002184128040"></p>
<h2 id="支持向量机SVM"><a href="#支持向量机SVM" class="headerlink" title="支持向量机SVM"></a>支持向量机SVM</h2><p>软间隔：</p>
<p><img src="/../img/image-20221002191511910.png" srcset="/img/loading.gif" lazyload alt="image-20221002191511910"></p>
<p>延伸到多维平面：</p>
<p><img src="/../img/image-20221002191712109.png" srcset="/img/loading.gif" lazyload alt="image-20221002191712109"></p>
<p>求x到平面的间距，先求和平面垂直的法向量，也就是下降最快的向量——梯度W，把f(X)对X求导，把X约掉，得到梯度W。</p>
<p><img src="/../img/image-20221003011038872.png" srcset="/img/loading.gif" lazyload alt="image-20221003011038872"></p>
<p>求距离公式=法向量x该点到平面上一点的向量/法向量的二范数。原理：两向量相乘，二者内积就等于一个是一方对另一方的贡献 ，几何意义就是A向量在B向量上的投影再乘上B向量。</p>
<p>推导过程：</p>
<p><img src="/../img/image-20221003011103574.png" srcset="/img/loading.gif" lazyload alt="image-20221003011103574"></p>
<p>以上，是间隔最大的证明过程，但还需要保证分类也是正确的，即蓝色的点f(X)都要大于0，黄色的点f(X)都要小于0。</p>
<p><img src="/../img/image-20221003011547492.png" srcset="/img/loading.gif" lazyload alt="image-20221003011547492"></p>
<p>推导过程：</p>
<p><img src="/../img/image-20221003011613894.png" srcset="/img/loading.gif" lazyload alt="image-20221003011613894"></p>
<p>整理以上两个条件可得：</p>
<p><img src="/../img/image-20221003011646310.png" srcset="/img/loading.gif" lazyload alt="image-20221003011646310"></p>
<p>$s.t.$约束条件保证分类是正确的，边界计算还可以更简化。</p>
<p>线性函数中W和b同时增加两倍，直线位置不变，$margin(W,b)$大小也不变。</p>
<p><img src="/../img/image-20221004160750128.png" srcset="/img/loading.gif" lazyload alt="image-20221004160750128"></p>
<p>因此可以做出左边的变形，虽然结果不变，但是括号内的直线部分却发生了变化，为了更确定H的值，另绝对值里的部分强行等于1，于是就又多了一个$s.t.$约束条件，因为左边有个绝对值，所以右边要加上一个标签，可知在分类正确的前提下，加了标签该式就肯定大于0，就相当于加绝对值了。</p>
<p>合并起来后，可以得到：</p>
<p><img src="/../img/image-20221004161255962.png" srcset="/img/loading.gif" lazyload alt="image-20221004161255962" style="zoom:50%;"></p>
<p>还可以转化成求最小问题：$\min||W||_2$。</p>
<p>为了方便用<a target="_blank" rel="noopener" href="https://www.cnblogs.com/maybe2030/p/4946256.html">拉格朗日乘数法</a>，还可以把它展开成：$\min\frac{1}{2}W^TW$。基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题。</p>
<p>这里回看对偶问题那一章，晚会看。</p>
<p>只有在边界上的点，才对边界的大小起作用，才有贡献，因此我们只在乎边界上的点。</p>
<p><img src="/../img/image-20221004165715184.png" srcset="/img/loading.gif" lazyload alt="image-20221004165715184" style="zoom:50%;"></p>
<p>因此，最后决定参数W的只和支持向量有关，SVM就叫做支持向量机。</p>
<p>将拉格朗日函数转化为损失函数：</p>
<p><img src="/../img/image-20221004165937256.png" srcset="/img/loading.gif" lazyload alt="image-20221004165937256" style="zoom:50%;"></p>
<p>上面这些都是硬间隔，将蓝色和黄色绝对分开。而如果遇到不好分开的情况，我们允许它存在，就叫做软间隔，需要引入一个误差项并做一些改动。正则化是用来减少过拟合的，<u>这个暂时还不会，再看看</u>。</p>
<p><img src="/../img/image-20221004170227536.png" srcset="/img/loading.gif" lazyload alt="image-20221004170227536" style="zoom:67%;"></p>
<p>数学表达：</p>
<p><img src="/../img/image-20221004170325892.png" srcset="/img/loading.gif" lazyload alt="image-20221004170325892" style="zoom:50%;"></p>
<p>$\xi$代表被分错的点到边界的距离，如果分对了，那他就是零，因此还需要这个的总和尽量小，约束条件里也从小于0放宽成了小于$\xi$。两个约束条件写到一起就变成：</p>
<p><img src="/../img/image-20221004171736009.png" srcset="/img/loading.gif" lazyload alt="image-20221004171736009" style="zoom:50%;"></p>
<p>于是损失函数就变成：</p>
<p><img src="/../img/image-20221004171833056.png" srcset="/img/loading.gif" lazyload alt="image-20221004171833056" style="zoom:50%;"></p>
<p>看一眼损失函数里的经验损失项怎么来的：</p>
<p><img src="/../img/image-20221004172424868.png" srcset="/img/loading.gif" lazyload alt="image-20221004172424868"></p>
<p>先看紫线，黄点位于其上方，因此代入线性函数大于0，同时它又是黄点，标签应该为-1，二者相乘是个负数，代入不等式大于1，因此经验损失项也大于1。</p>
<p>看绿线，在黄点上方，离黄点距离很近，偏差小于1，代入不等式大于0，经验损失项大于0。</p>
<p>红线也在黄点上方，偏差很大，分类相当正确，代入不等式小于0，没有产生损失，经验损失项为0。</p>
<p>画出经验损失项的图：</p>
<p><img src="/../img/image-20221004172919851.png" srcset="/img/loading.gif" lazyload alt="image-20221004172919851" style="zoom:50%;"></p>
<p>W，b错的越离谱，损失函数越大，W，b正确的话则不会产生损失。</p>
<p>机器学习分类问题整理：</p>
<p><img src="/../img/image-20221004173511340.png" srcset="/img/loading.gif" lazyload alt="image-20221004173511340"></p>
<p>现实中我们能做出来的是分类函数T(X)，它能做到的顶多就是在训练集里能完全分类，但我们的目标其实是在所有情况下均可以正确分类，也就是G(X)。</p>
<p>我们在训练T(X)的过程就是把它拆分成两部分——模型函数和决策函数，用一个模型函数来分割或拟合这些样本点，这样样本点就被转化成了一个具体的数字，再用决策函数对这个数字进行标签化，大于0代表正类，小于0代表反类，因此，只要模型函数确定了，最后标签也就确定了。</p>
<p>可以说，模型函数就是给全空间的样本点都赋予了一个具体的实数值，f(X)=0既是分界线，也是度量的锚点。</p>
<p>我们比较T(X)和G(X)，想让他们尽可能一样，只需要比较模型函数即可，决策函数部分不用比较，因为只要他们对应的模型函数是一致的，决策结果一定是一致的。</p>
<p>因此，这里引入J(X)来比较T(X)和G(X)，这个J(X)就是我们常说的损失函数，学习算法就是用来找到最小损失函数的，这样就把机器学习拆成了两部分：</p>
<p><img src="/../img/image-20221004174609296.png" srcset="/img/loading.gif" lazyload alt="image-20221004174609296"></p>
<p>而损失函数就是为这个度量赋予意义，然后再做适当修正，让修正的结果具有某种现实意义，根据我们对现实意义的理解做出约束，最后在这个现实意义的基础上进行决策。</p>
<p>可以说，最开始得到的这个度量是没有现实意义的，我们无法根据它猜测这个模型函数和真实情况是否一致，只有在我们给它赋予一些在现实中已经证实的有经验的东西——我们对数据、经验、和世界的理解和归纳。（感觉开始玄学了）</p>
<p>举个最小二乘法的例子，它给损失函数赋予的意义是一个服从高斯分布的方差，因为根据经验，我们现实中的噪声多数是高斯噪声。</p>
<p><img src="/../img/image-20221004180202366.png" srcset="/img/loading.gif" lazyload alt="image-20221004180202366"></p>
<p>紫色是理想情况，最小二乘法默认方差是服从高斯分布的，因此我们只要找到这些值进行代入就可以了。但是理想情况的函数值只有因果律知道，不过没关系，我们知道训练集的标签，它是已知的，我们可以把训练集的标签当作理想情况的函数值来输入。但是这个标签很固定，只会是-1和1，怎么和我们的模型参数进行比较啊？用双曲正切函数修正它，把它约束到-1和1之间，那这个时候再去求它和标签值的方差，就很合理了。</p>
<p>再看看别的损失函数的意义：</p>
<p>最大似然估计法：通过sigmoid得到的值在0-1之间，就可以看作一个概率。在训练集里X和Y已知的情况下，对所有未知参数$\theta$求似然值，当这个似然值最大的时候，我们猜测这个函数和最终目标函数相同。（就相当于，如果在取这个$\theta$的时候，得到现实样本点的概率最大，那它是现实函数的参数的可能性就越大）</p>
<p>推导过程的最后一步，因为抽样X的概率各个X的概率是一样大的，所以$P(\overline{X})$可以看作是一个常数，在求max的时候直接忽略掉这个常数就可以了。然后就得到了最大似然值的表达式，要注意的是：$y$是真实标签的值，它的值只能是0或1，否则不成立。</p>
<p><img src="/../img/image-20221004183112819.png" srcset="/img/loading.gif" lazyload alt="image-20221004183112819"></p>
<p>并且，再假设：</p>
<p><img src="/../img/image-20221004183622147.png" srcset="/img/loading.gif" lazyload alt="image-20221004183622147" style="zoom:50%;"></p>
<p>这就是最大后验估计。</p>
<blockquote>
<p>从原因到结果的论证称为“先验的”，而从结果到原因的论证称为“后验的”。</p>
</blockquote>
<p>SVM：在函数模型前乘以一个项，得到它离软间隔的几何距离，有了这个度量后再进行三个约束，默认当约束全部满足时，我们猜的模型和真实模型是一致的。</p>
<p><img src="/../img/image-20221004184019866.png" srcset="/img/loading.gif" lazyload alt="image-20221004184019866" style="zoom:80%;"></p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>如果是两个同类型的model，比如都是高斯分布，那只用分别比较他们的期望和方差就可以了，但如果不是同一种模型，比如一个是高斯分布，一个是泊松分布，那是否存在一个公度，可以把他们放在一起比较呢？</p>
<p>答案是熵。</p>
<p>从信息论的角度可以理解为，从原来的不确定变成确定，它的难度有多大。</p>
<p>信息量其实是无法计算的，我们只能给出定义。</p>
<p><img src="/../img/image-20221004202949393.png" srcset="/img/loading.gif" lazyload alt="image-20221004202949393"></p>
<p>打个比方，阿根廷夺冠的信息量其实是等于进决赛和赢了决赛加在一起的时候的，因为当我们知道阿根廷进决赛并赢下决赛，就等于知道了阿根廷夺冠。但这在概率论上，相加明显就不成立了。为了保持自洽，信息量的计算公式必须有一个log，并且需要函数是递减的，所以要加个负号。而以2为底，最后得出的单位是bit。</p>
<p><img src="/../img/image-20221004211131305.png" srcset="/img/loading.gif" lazyload alt="image-20221004211131305" style="zoom:67%;"></p>
<p>这时候下面的函数值就等于3。</p>
<p><img src="/../img/image-20221004211509482.png" srcset="/img/loading.gif" lazyload alt="image-20221004211509482"></p>
<p>德国和比利时赢下足球的概率是一半一半，非常难确定谁会赢球，这个系统的熵就很大。但是法国和中国几乎稳稳地就是法国赢球，非常容易确定，因此这个系统的熵就很小。然而简单相加左边这个系统的熵就很小，所以要根据贡献度乘以一个比例才行。</p>
<p>因此，右边对系统贡献的信息量大概是0.058，</p>
<p><img src="/../img/image-20221004211951950.png" srcset="/img/loading.gif" lazyload alt="image-20221004211951950" style="zoom:50%;"></p>
<p>左边对系统贡献的信息量是1，</p>
<p><img src="/../img/image-20221004212121257.png" srcset="/img/loading.gif" lazyload alt="image-20221004212121257" style="zoom:50%;"></p>
<p>这么一看不就是求期望吗，因此可以把熵定义为：</p>
<p><img src="/../img/image-20221004212214048.png" srcset="/img/loading.gif" lazyload alt="image-20221004212214048"></p>
<p>最后算出来的是一个不确定（混乱）程度。</p>
<p>相对熵（KL散度）：</p>
<p><img src="/../img/image-20221004212640747.png" srcset="/img/loading.gif" lazyload alt="image-20221004212640747"></p>
<p>P在前，所以是以P系统为基准，熵是恒定的，因此最后得到的式子里，后半部分是恒定的，我们只用看前面部分。当KL散度等于0的时候，P和Q是最接近的。</p>
<p>根据吉布斯不等式，KL散度是绝对大于等于0的。</p>
<p>因此，如果我们想让这两个模型尽可能的接近，那就去找交叉熵的最小值就好了，也就是说，交叉熵本身就可以作为损失函数。</p>
<p>交叉熵的定义：</p>
<p><img src="/../img/image-20221004213244529.png" srcset="/img/loading.gif" lazyload alt="image-20221004213244529" style="zoom:50%;"></p>
<p>这个m是什么呢？如果P，Q的系统里，x也就是发生事件的数量不一样，那么这个x只要选数量大的那个就可以了。</p>
<p><img src="/../img/image-20221004213649587.png" srcset="/img/loading.gif" lazyload alt="image-20221004213649587"></p>
<p>当把图像识别和人脑做交叉熵的时候，p就是人脑，它只肯给出两个可能，要么是猫要么不是，而q就是机器学习要给的它像猫或者不像猫的概率到底是多少，最后展开成上图。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>当我们求梯度的时候，求的就是损失函数的梯度。损失函数就是我们设计出来的函数和我们真正想要的之间的误差，那么损失函数是如何设计出来的呢？</p>
<p>吴恩达老师给出的公式，可以写成两种形式：</p>
<p><img src="/../img/image-20221005085901958.png" srcset="/img/loading.gif" lazyload alt="image-20221005085901958" style="zoom:50%;"></p>
<p><img src="/../img/image-20221005085918649.png" srcset="/img/loading.gif" lazyload alt="image-20221005085918649" style="zoom:50%;"></p>
<h4 id="最小二乘法（最简单）"><a href="#最小二乘法（最简单）" class="headerlink" title="最小二乘法（最简单）"></a>最小二乘法（最简单）</h4><p>以下面这个神经网络为例：</p>
<p><img src="/../img/image-20221005090133768.png" srcset="/img/loading.gif" lazyload alt="image-20221005090133768" style="zoom:80%;"></p>
<p>输入是一个非0即1的标签，代表输入的图片是不是猫，经过神经网络计算后输出一个0-1之间的概率，表示是猫的概率有多少。当x和y相差无几的时候，我们就可以认定机器学习的模型和我们人脑的模型是近似的。</p>
<p>因此，为了求这个差距，我们可以用$\min|x-y|$来表示，但是绝对值通常不会是全程可导的，因此我们需要对$(x-y)$求平方，这样虽然值发生了变化，但是最小还是最小，不影响。</p>
<p><img src="/../img/image-20221005090735612.png" srcset="/img/loading.gif" lazyload alt="image-20221005090735612" style="zoom:50%;"></p>
<p>因此可以证得吴恩达老师的第一个公式：</p>
<script type="math/tex; mode=display">
L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2</script><p>这里的$\hat{y}$就是上面神经网络里的$x_i$，为什么要加一个$\frac{1}{2}$的系数是因为求偏导后会下来一个2，正好可以约掉。</p>
<p>直接把两个模型结果做差求距离虽然很简单，但是在求梯度下降的时候会非常麻烦，因此不建议选用。</p>
<h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>是一种概率的反向运用，打个比方：抛硬币的时候，假设我们不知道其实是正反各0.5的概率，但是我们掷了1w次以后，正反差不多各5k次，我们就可以反推它们的概率分布是正反各0.5。</p>
<p>但如果投了10次硬币，7次正3次反，我们又规定了这样的概率模型，那么这种情况会出现的概率有多大呢？答案很明显是$0.1^7\cdot0.9^3$。</p>
<p><img src="/../img/image-20221005092107603.png" srcset="/img/loading.gif" lazyload alt="image-20221005092107603" style="zoom:50%;"></p>
<p>我们把这个概率值叫做似然值，它虽然也是一种可能性，但是是从真实世界反推回来的。它是在真实的情况已经发生后，我们假设它有很多模型，在某一个概率模型下发生这种真实情况的可能性。</p>
<p>在训练神经网络的时候，我们输入的图片就像是抛出的那些硬币一样。我们人脑已经知道这些图片是不是猫了，并且告诉了神经网络，现在就要找到一种似然值最大的情况，使神经网络输出的分类也和我们人脑一样，这时的概率模型就应该是最接近真实情况的。</p>
<p>举个例子：</p>
<p><img src="/../img/image-20221005093452699.png" srcset="/img/loading.gif" lazyload alt="image-20221005093452699"></p>
<p>这世上有各种各样的猫，但是存在一个边界，超出这个边界就不是猫了，这个边界是我们人脑划定的，比如他可能呈高斯分布，那么神经网络就要调整自己的分布，通过调期望和方差来逼近我们人脑的边界线。</p>
<p>但也可能不是，不过没关系，只要叠加的感知级够多，什么形状都可以逼近出来的，因此我们搭建一个NN神经网络，通过调整W和b来逼近这个人脑模型。</p>
<p>推导极大似然估计的公式：</p>
<p><img src="/../img/image-20221005094256500.png" srcset="/img/loading.gif" lazyload alt="image-20221005094256500" style="zoom:50%;"></p>
<p>$x_i$放到上面那个判断猫猫的例子里，就是判断的结果0或1，将所有概率连乘后得到第二行的式子，但是每一项的结果不是0就是1，用不了，因此要引入一个$y_i$，它的参数是W和b，输出的是猫猫的概率为多少。</p>
<p>因为$x$不是0就是1，所以其实是符合伯努利分布的，p代表是猫猫的概率，y就是猫猫的分布，最后展开成：</p>
<p><img src="/../img/image-20221005095408839.png" srcset="/img/loading.gif" lazyload alt="image-20221005095408839" style="zoom:50%;"></p>
<p>连乘不好算，换成连加，log不改变单调性，该极大值还是极大值：</p>
<p><img src="/../img/image-20221005095629064.png" srcset="/img/loading.gif" lazyload alt="image-20221005095629064" style="zoom:50%;"></p>
<p>然后就得到了吴恩达老师的第二个公式：</p>
<script type="math/tex; mode=display">
L(\hat{y},y)=-[y\log\hat{y}+(1-y)\log(1-\hat{y})]</script><p>这里的$\hat{y}$就是我们的$x_i$，因为我们更常算最小值，所以加个负号把max变成了min。</p>
<p><img src="/../img/image-20221005095753697.png" srcset="/img/loading.gif" lazyload alt="image-20221005095753697" style="zoom:50%;"></p>
<h2 id="最大熵"><a href="#最大熵" class="headerlink" title="最大熵"></a>最大熵</h2><p>训练神经网络用梯度下降法，优势是简单好用，劣势是计算量非常大。看看大佬们怎么优化和改进梯度下降法，其中有一种是在隐藏层，把隐藏层感知机的激活函数换掉，从原来的sigmoid换成ReLU函数。</p>
<p>原因很好理解，因为如果传入的数值恰好在绿框部分的话，这里的梯度是非常小的，如果这个梯度再反向传播，传到前面几层基本上就不剩啥了，这就叫做梯度消失。</p>
<p><img src="/../img/image-20221005101034990.png" srcset="/img/loading.gif" lazyload alt="image-20221005101034990"></p>
<p>如果换成ReLU函数，只要大于0就是一条直线，梯度是一个定值，不会出现梯度消失的问题。但是被ReLU激活过的函数输出是可以无穷大的，怎么转化成概率值需要的0-1之间呢？</p>
<p><img src="/../img/image-20221005101226575.png" srcset="/img/loading.gif" lazyload alt="image-20221005101226575" style="zoom:50%;"></p>
<p>可以在最后一层用sigmoid，把它重新归到0-1之间，这个时候用损失函数就没太大问题了。但是这时候就只能解决一种分类的情况了，因为sigmoid函数只能解决单个感知机归一的问题，比如最后可能会输出两个分类结果，是猫的概率是60%，是狗的概率也是60%，加起来都超过1了。明显不太适合用于这种分类造成的结果互斥情况（是猫就不会是狗），但如果是可以标签重合的情况，就比如这个小动物既可以是可爱的，也可以是吃肉的，等等等等，这些要一起分类的话，softmax反而不合适了，反而要回去用sigmoid。</p>
<p>输出层只有单个感知机的情况：</p>
<p><img src="/../img/image-20221005103127031.png" srcset="/img/loading.gif" lazyload alt="image-20221005103127031"></p>
<p>输出层有多个感知机的情况，z，y，a就变成了一个向量：</p>
<p><img src="/../img/image-20221005103222720.png" srcset="/img/loading.gif" lazyload alt="image-20221005103222720"></p>
<p>a作为最后输出的结果：</p>
<p><img src="/../img/image-20221005103326143.png" srcset="/img/loading.gif" lazyload alt="image-20221005103326143" style="zoom:50%;"></p>
<p>首先是一个向量，其次每一项都是一个概率，所以大于等于0肯定，然后任何一个图片或数据输入给神经网络，都应该有一个分类，把每一个分类的概率都加起来，那它就应该是百分百会发生的事，所以元素之和等于1，用e为底，将z作为e的指数，就可以做到值域绝对大于0，再除以所有分子的总和，做归一化处理，得到的就是softmax函数。</p>
<p><img src="/../img/image-20221005150321481.png" srcset="/img/loading.gif" lazyload alt="image-20221005150321481" style="zoom:50%;"></p>
<p>通过比较softmax和sigmoid函数发现，sigmoid函数其实就是只有一种分类情况下的softmax，$t_2$是不会变化的。</p>
<p>之前章节里，我们学到的都是无论这个神经网络有多么复杂，只要似然值最大并且交叉熵最小，就可以认为这个模型已经逼近了人脑，但其实还存在一个隐藏的前提假设，只要激活函数选择了sigmoid或者softmax，这个前提条件就默认被勾选了，它就是最大熵原理。</p>
<p>举个例子，比如说你对一个概率问题完全没有什么额外的信息，我们去猜概率的话只能挨个猜，相当于每个事件的概率都是相等的，当然也可以猜第一个是50%，第二个是25%，第三个是5%这样。这样计算出来第二种的熵明显比第一种要小，而第一种熵是所有猜测里最大的。</p>
<p>在已知一部分概率的条件下，未知的信息都要符合最大熵原理。这样得到的模型既能满足已知的信息，又能让未知的信息熵最大。因此，问题可以被拆成两部分，第一个部分是解决相同的问题，第二个部分是解决最大熵的问题。</p>
<p>前面提到过的交叉熵，KL散度，可以用来比较两个概率模型之间的差别，但前提是两个概率模型是已知的，而现在两个里我们只知道半个，首先目标概率模型是不到的，已知的只是一堆样本数据，还要从中归纳出背后的概率模型。</p>
<p>那咋办呢？数学家提出了一个概念，是概率论里的矩。</p>
<p>先看一眼正态分布：</p>
<p><img src="/../img/image-20221006092037858.png" srcset="/img/loading.gif" lazyload alt="image-20221006092037858"></p>
<p>不论什么样的正态分布，只要确定了期望和方差，都可以没有任何信息损失的把它的概率分布描绘出来，它的期望和方差表达式如上，期望就是平均分布，方差就是一个二次减一次的平方。但就凭这个还是无法归纳出背后的规律，现在对正态分布做一定量的偏移，得到：</p>
<p><img src="/../img/image-20221006092517835.png" srcset="/img/loading.gif" lazyload alt="image-20221006092517835"></p>
<p>多了一个三次项，而$\mu$和$\sigma$在前面都已经知道了，那是不是可以猜想，概率分布和这些多次项有关呢？然后数学家证明了，真的有，而且还起了名字。</p>
<p><img src="/../img/image-20221006092915969.png" srcset="/img/loading.gif" lazyload alt="image-20221006092915969"></p>
<p>得到的归纳结果是，如果是一个正态分布，只用一阶矩和二阶矩就能描述出来，更复杂一点，需要三阶矩，再复杂的话就通过四阶矩、五阶矩、六阶矩等等，也能给描述出来。</p>
<p>也就是说，任何一个概率分布，它的特征都可以用这样一个向量来表现：</p>
<p><img src="/../img/image-20221006093434038.png" srcset="/img/loading.gif" lazyload alt="image-20221006093434038" style="zoom:50%;"></p>
<p>数学家们定义了一个特征函数，$e^{itx}$是一个复数，求这个复数的期望，任何一个概率分布都可以用特征函数来表示出来，而且一个概率分布就对应一个特征函数，把这个特征函数在$x=0$处<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/404446570#:~:text=%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%98%AF%E5%A4%9A%E9%A1%B9%E5%BC%8F%E9%80%BC%E8%BF%91%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E4%BD%8E%E6%AC%A1%E5%88%B0%E9%AB%98%E6%AC%A1%E7%9A%84%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%B4%AF%E5%8A%A0%E6%9D%A5%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0%20f%20%28x%29%20%E5%9C%A8%E6%9F%90%E4%B8%AA%E7%82%B9%E9%82%BB%E5%9F%9F%E7%9A%84%E5%87%BD%E6%95%B0%E5%80%BC%E3%80%82%20%E6%AF%94%E5%A6%82%E5%9C%A8%E7%89%A9%E7%90%86%E4%B8%8A%EF%BC%8C%20text%20%7Bsin%7D%20%28x%29,x%20%E6%9D%A5%E9%80%BC%E8%BF%91%20text%20%7Bsin%7D%20%28x%29%20%E5%9C%A8%20x%3D0%20%E9%99%84%E8%BF%91%E7%9A%84%E5%87%BD%E6%95%B0%E5%80%BC%E3%80%82">泰勒展开</a>，得到上面的式子。</p>
<blockquote>
<p><strong>泰勒展开的实质是找一个各阶导数都和原函数在展开点处的导数相等的多项式，来达到拟合的效果。</strong></p>
</blockquote>
<p>这个式子其实就是上面那个向量的线性关系。</p>
<p>也就是说，如果有两组数据，想知道它们的概率分布是否一样，只需要去比较它们的一阶矩，二阶矩，三阶矩……是否相同，如果相同，不需要去计算他们的概率分布具体表达式，就可以确定是一样的。</p>
<p>因此，我们在做神经网络拟合现实模型的时候，只需要保证：</p>
<p><img src="/../img/image-20221006100355037.png" srcset="/img/loading.gif" lazyload alt="image-20221006100355037" style="zoom:50%;"></p>
<p>现在我们知道拿到两个概率模型之后，就算我们不知道它们具体是什么，也知道应该比什么了。具体到机器学习和训练集，这个情况是什么样的呢？</p>
<blockquote>
<p>$\tilde{P}$代表经验概率，即从数据中直接归纳出的概率。</p>
</blockquote>
<p><img src="/../img/image-20221006100815684.png" srcset="/img/loading.gif" lazyload alt="image-20221006100815684"></p>
<p>在图片识别应用中：</p>
<p><img src="/../img/image-20221006102106310.png" srcset="/img/loading.gif" lazyload alt="image-20221006102106310"></p>
<p>训练集内有N个图片，每个照片都可以用$(x,y)$来表达，其中x表示照片，y表示标签，它是一个向量，每个分量代表一个分类，因为神经网络的输出层有四个，所以是一个四维的向量，代表它属于哪一个分类。</p>
<p>但是这里给出的照片不可能有两个完全一样的照片，所以若按像素输入的话概率值只可能等于1/N，这样就没有意义了，无法区分开不同的照片。 </p>
<p>但不看输入层，看隐藏层，尤其是输出层前一层的隐藏层，感知机就没有那么多了，可以把这一层的感知机看作一个一个的特征，将特征作为x输入，这时候概率值就不是1/N了。</p>
<p><img src="/../img/image-20221006104908439.png" srcset="/img/loading.gif" lazyload alt="image-20221006104908439"></p>
<p>$\tilde{P}(x,y)$和$\tilde{P}(x)$都是已知的，我们的目标是求在输入x图片的情况下，y——即分类概率是多少，$P(y|x)$中包含了一部分已知的，又包含了一部分未知的，已知的部分要完全相等，未知的部分要用最大熵，这就是我们接下来求解问题的方针。</p>
<p>用贝叶斯公式把目标概率展开得：</p>
<p><img src="/../img/image-20221006105744859.png" srcset="/img/loading.gif" lazyload alt="image-20221006105744859" style="zoom:50%;"></p>
<p>$P(x,y)$按理说是不知道的，但是前面有经验概率，我们若想利用这个经验概率，需要保证$P(x)\cdot P(y|x)$和这个经验概率相等，$P(x)$仍然可以用经验概率，未知部分要用最大熵来确定，因此，要保证：</p>
<p><img src="/../img/image-20221006110648640.png" srcset="/img/loading.gif" lazyload alt="image-20221006110648640" style="zoom:50%;"></p>
<p>第三个式子虽然也可以用，但是求偏导不太方便，所以不建议，我们不用。</p>
<p>在前面通过数数的方式得到了$\tilde{P}$，但是只知道概率是无法求期望的，期望的公式是概率乘以随机变量的累加或积分，在这里只有概率，没有随机变量，所以我们还需要在设计一个x和y的随机变量。</p>
<p>举个例子：</p>
<p>当确定了事件A以后，只要不符合其中的一个特征，比如“尖脸，是鸭”，就不属于这个事件那么X就等于0。</p>
<p><img src="/../img/image-20221006164639901.png" srcset="/img/loading.gif" lazyload alt="image-20221006164639901" style="zoom:50%;"></p>
<p>这样满足A和不满足A这两个事件就构成了二项伯努利分布非0即1的条件，最后得到X的期望结果就是$\tilde{P}$，就是满足A的概率本身。</p>
<p>以上是只有一个条件，当一共有m个条件，把样本空间里面所有可能的事件都覆盖掉，那么对$X_1,X_2,…,X_3$求期望，得到的就是对应事件的概率。这部分的函数在很多论文里叫特征函数，其实英文直译应该是指示函数。</p>
<p><img src="/../img/image-20221006171520590.png" srcset="/img/loading.gif" lazyload alt="image-20221006171520590" style="zoom:50%;"></p>
<p>这时候再去完成让两个模型分布相等的目标，只需要把f设计成所有事件的一阶矩，因为只要一阶矩相等，这两个概率分布就一定是相等的。因为对于伯努利分布来说，决定它的形状的只有一阶矩。</p>
<p>因此，之前那个需要考虑很多阶矩的复杂问题就只用考虑一阶矩了，只要将样本空间里所有的可能事件都列举出来就可以了。</p>
<p>这时候，下面这个问题其实就已经解决了：</p>
<p><img src="/../img/image-20221006172021361.png" srcset="/img/loading.gif" lazyload alt="image-20221006172021361" style="zoom:50%;"></p>
<p>在设计了这些$X_m$事件后，就相当于在原来的样本空间$(x^(k),y^(k))$上，重新设计了一个样本空间，原样本空间的维度非常高，现在变成了只有一维的情况，这个一维就是事件。</p>
<p><img src="/../img/image-20221006172434773.png" srcset="/img/loading.gif" lazyload alt="image-20221006172434773"></p>
<p>也就是把原来很多维的输入，都投射到一维的事件上，在一维上进行比较。</p>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>我们之前的目标有两个，第一个已经达成了，还有第二个求条件概率的最大熵。要求$P(y|x)$的熵，需要把每一个事件下y的熵都求出来，并对它们求期望，所以前面再乘以一个概率，然后求和。</p>
<p><img src="/../img/image-20221006174155241.png" srcset="/img/loading.gif" lazyload alt="image-20221006174155241"></p>
<p>得到右边条件熵的公式$H(Y|X)$，代表X发生之后Y的概率，求和符号下的$x,y$代表x和y的任意一种组合情况，我们可以把它看作X取一个具体值时的熵，然后再对这个熵求期望。</p>
<p>我们要求它的最大值，还需要满足别的条件。</p>
<blockquote>
<p>sigmoid和softmax函数的本质其实就是最大熵。</p>
</blockquote>
<p>因为概率经验$\tilde{P}$是可以直接从训练样本里数出来的，所以这里把它看作常数$\Delta$。</p>
<p><img src="/../img/image-20221006181845123.png" srcset="/img/loading.gif" lazyload alt="image-20221006181845123"></p>
<p>在同时满足上面两个条件的同时，用拉格朗日乘数法来求极小值，推导过程如下：</p>
<p><img src="/../img/image-20221006182040984.png" srcset="/img/loading.gif" lazyload alt="image-20221006182040984"></p>
<p>拉格朗日乘数法求极值有一个基础，原本的问题是求当$P,\lambda$等于多少的时候，L的值为最小值，具体过程是，先把$\lambda$当作需要调整的参数，P当作一个参数，找到L的最大值，然后再在最大值的情况下，再去调整P，找到L的最小值。对偶问题则是先把P当作参数，再把$\lambda$当作参数。二者结果是一样的，具体求解如下：</p>
<p><img src="/../img/image-20221006183112036.png" srcset="/img/loading.gif" lazyload alt="image-20221006183112036"></p>
<p>因为$\lambda$项没有$\tilde{P}(x)$系数，为了把$\tilde{P}(x)$提出来，乘上一个它的加和，易知是等于1。这时候就需要偏导等于0来求极值，因为后半部分始终同号，所以求和所有项都是同号，要求和为0，所以要每项都为0。因为L对所有的$P(y_j|x_i)$的偏导都要为0才能保证是可能的极值点。这块的实质是变分，<u>不知道是啥，再看看。</u></p>
<p><img src="/../img/image-20221006183804353.png" srcset="/img/loading.gif" lazyload alt="image-20221006183804353"></p>
<p>把结果化简成感知机进行线性运算的形式，因为$\lambda_0$是归一化条件的系数，所以条件概率最后是归一的，并且样本空间所有样本概率求和结果为1，因此分子分母应该相等。</p>
<p><img src="https://gitee.com/labulavish/blog/raw/master/img/202210061841725.png" srcset="/img/loading.gif" lazyload alt="image-20221006184118190" style="zoom:50%;"></p>
<p>回到softmax函数，可以闭环了。</p>
<p><img src="/../img/image-20221006184557423.png" srcset="/img/loading.gif" lazyload alt="image-20221006184557423"></p>
<p>这里softmax以e为底，不只是为了保证结果大于0，而且是因为在求最大熵偏导的过程中引入了$\log$，反函数用到了e，因此，只要在输出层用到了sigmoid或者softmax函数，就保留了最大熵原理的应用。</p>
<p>前面只确定了最大熵P的形式，还没有确定参数。</p>
<p><img src="/../img/image-20221006185111870.png" srcset="/img/loading.gif" lazyload alt="image-20221006185111870" style="zoom:50%;"></p>
<p>我们在做机器学习的时候，我们已知的是训练集，但我们希望可以把模型应用到训练集的延长线上，我们扩大的是什么？</p>
<p><img src="/../img/image-20221006185301603.png" srcset="/img/loading.gif" lazyload alt="image-20221006185301603"></p>
<p>如果x和y都在训练集上，那么条件概率就等于经验概率，如果x和y有一个不在训练集上，那么条件概率就是未知的，未知的话就可以用到最大熵原理，因此，这就是机器学习的前提：</p>
<p><img src="/../img/image-20221006185450171.png" srcset="/img/loading.gif" lazyload alt="image-20221006185450171" style="zoom:50%;"></p>
<p>通过$\min L(P,\lambda)$确定出$P(y|x)$的形式，他逃不出指数族的形式。</p>
<p><img src="/../img/image-20221006191000121.png" srcset="/img/loading.gif" lazyload alt="image-20221006191000121"></p>
<p>确定形式后代回原式来求$\lambda$的具体值，这个$\lambda$是一个向量，维数非常多，仅凭计算是求不出来的，常用方法是构建一个损失函数，用梯度下降法训练得到。</p>
<p><img src="/../img/image-20221006191420978.png" srcset="/img/loading.gif" lazyload alt="image-20221006191420978"></p>
<p>整理思路：1、设定样本的概率模型与目标概率模型一致（两个模型特性函数一致）建立服从伯努利分布的变量（此处需要理解样本空间及变量的关系）对两个模型进行降维（降维后两个概率模型依旧一致），由于降维后的两个概率模型只有一阶矩不为零，可以产生一个条件用于后面的计算：降维后样本模型与目标模型的一阶矩相等；2、在满足第一个条件后（降维后样本模型与目标模型的一阶矩相等），可大胆的地将目标概率模型的条件熵转换公式用样本概率模型的经验概率替换，然后计算目标概率模型的条件熵最大值（即满足最大熵原理），计算过程使用了拉格朗日乘数法以及概率论的基本知识。</p>
<p>只能说为了水论文看这个不合适，想要自己设计算法，知道谁是谁的爹谁和谁有血缘关系太重要了。</p>
<p>我觉得他们最终归一其实就是为了解决机器学习，但针对不同工程问题而设计出来的算法，极大似然和交叉熵这种计算简单，可以用在直观数据的机器学习，而softmax这种适合更加复杂的问题，比如图片。</p>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>正向传播：把数据输入神经网络，通过参数W，b和无数感知机，得到一个在误差范围内的最终结果。</p>
<p>反向传播：把偏差的信息传递到各个参数上，根据这些参数对偏差贡献的大小，来相应的承担修改的责任。</p>
<p>凭直觉来设计一种分配偏差的方式：</p>
<p><img src="/../img/image-20221007085853363.png" srcset="/img/loading.gif" lazyload alt="image-20221007085853363"></p>
<p>绿色的球代表偏差，能对它产生影响的可以分为三层——偏置系数b、权重系数W和上一层神经网络的输出$a^{[2]}$。根据该层每一个感知机的贡献度反向传递对应的偏差，调整参数W和把，上一层输出的调整则要反向传递到上上一层来进行处理。通过这样一层层的向上传播、分配，最终神经网络里每一个W和b都会被分配到对应的份额，并且根据对偏差的贡献相应地调整自己。</p>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度=$\nabla f(x,y)$，以下是一个展示梯度的示意图：</p>
<p><img src="/../img/image-20221007090756143.png" srcset="/img/loading.gif" lazyload alt="image-20221007090756143"></p>
<p>红色的曲面代表$f(x,y)$的曲面，选定一个点做一个切面，并做一个向量，它在xy平面上的投影如左图所示，分解得到：</p>
<p><img src="/../img/image-20221007091404287.png" srcset="/img/loading.gif" lazyload alt="image-20221007091404287"></p>
<p>梯度是增大最快的那个方向，如果把它取反，就是减小最快的方向向量。我们要求的就是$(W_i,b_i)$空间中的梯度。在这样一个神经网络中进行反向传播：</p>
<p><img src="/../img/image-20221007091849342.png" srcset="/img/loading.gif" lazyload alt="image-20221007091849342"></p>
<p>梯度的数学表达：</p>
<p><img src="/../img/image-20221007094155934.png" srcset="/img/loading.gif" lazyload alt="image-20221007094155934" style="zoom: 25%;"></p>
<p>方便的理解：梯度就是对不同的方向各自求偏导。</p>
<p>看下图，P点处的切线是红线，绿线是过曲线的一条直线，当$\Delta x \rightarrow 0$时，绿线会和红线重合。</p>
<p><img src="/../img/image-20221007094315358.png" srcset="/img/loading.gif" lazyload alt="image-20221007094315358"></p>
<p>拓展到高维：</p>
<p><img src="/../img/image-20221007101241894.png" srcset="/img/loading.gif" lazyload alt="image-20221007101241894"></p>
<p>高维平面无法确定，因此引入偏导：</p>
<p><img src="/../img/image-20221007101407873.png" srcset="/img/loading.gif" lazyload alt="image-20221007101407873"></p>
<p>把y当作常数，对应的就是绿色的平面，这个平面和$f(x,y)$的曲面相交，得到曲线$f_y(x)$，对它求偏导就相当于是求在这个曲线上的切线，把x当作常数同理，得到两条相交的直线，可以唯一确定一个平面，这个平面恰好就是这个曲面的切平面。</p>
<p>继续简化，从z轴向下俯视：</p>
<p><img src="/../img/image-20221007102302062.png" srcset="/img/loading.gif" lazyload alt="image-20221007102302062"></p>
<p>这个点x轴的切向量和y轴的切向量加起来恰好是梯度，因此，梯度就是一个向量，向量的分量就是那些偏导。因此可以引申到全微分：</p>
<p><img src="/../img/image-20221007102627206.png" srcset="/img/loading.gif" lazyload alt="image-20221007102627206"></p>
<p>本来是当作面积来理解的，但是这里两个偏微分组成的一个梯度分别乘上x和y方向上的单位投影，加起来就相当于在梯度方向上移动了一个单位。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>这里要区分$\alpha,\beta,\gamma$，$\alpha$和$\gamma$直接可以修改，但是$\beta$需要反向传递，直至传递到第一层，最终体现在W和b上</p>
<p><img src="/../img/image-20221007092028475.png" srcset="/img/loading.gif" lazyload alt="image-20221007092028475"></p>
<p>想让J沿着变化最快的方向进行变化，之前看到的是分别把$x,y,z$减去对应系数的倍数，把这个过程照搬过来，乘一个$\eta$倍数，而$a^{[2]}$是依赖于前面的输入的，所以还需要继续向前传递，因此变形成：</p>
<p><img src="/../img/image-20221007093228796.png" srcset="/img/loading.gif" lazyload alt="image-20221007093228796" style="zoom:50%;"></p>
<p>第一个$a^{[2]}$是原来的，第二个$a^{[2]}$是我们梯度下降后的目标，相减得到一个“单位向量”，再一层层的传递，传递到最上面那一层，遍布每一个W和b之后乘$\eta$，结果是一样的。这样就和损失函数“原输出-目标输出”的形式一致了，于是我们做一个额外的假设：</p>
<p><img src="/../img/image-20221007093825763.png" srcset="/img/loading.gif" lazyload alt="image-20221007093825763"></p>
<p>这样就可以把第二层的损失函数继续展开成第二行的样子，就可以继续迭代，进入下一轮，直到把所有参数都分配一遍。</p>
<p>引入前面梯度就是求偏导的概念，得到：</p>
<p><img src="/../img/image-20221007103010507.png" srcset="/img/loading.gif" lazyload alt="image-20221007103010507" style="zoom:50%;"></p>
<p>下一轮更新：</p>
<p><img src="/../img/image-20221007103141209.png" srcset="/img/loading.gif" lazyload alt="image-20221007103141209" style="zoom:50%;"></p>
<p>由于a的偏差数值不止有一个感知机确定，是由上一层所有感知机确定，所以把他们加起来，然后求一个平均（为了求导之后可以抵消）。</p>
<p>到了$a^{[0]}$输入层，它就是一个输入的常量了，前面没有层了，因此就只有W和b需要修改了。</p>
<p>拿出一个感知机，对它放大后得到：</p>
<p><img src="/../img/image-20221007104106527.png" srcset="/img/loading.gif" lazyload alt="image-20221007104106527"></p>
<p>$l$代表的是第$l$层，$i$代表的是第$i$个，算出的$z$值进行激活函数运算后，可以解决非线性问题，得到一个值$a^{[l]}_i$，也就是这个感知机向外输出的值。再强调一下，每一个感知机的输入都是上一层所有感知机的输出。</p>
<p>抽象解决了表述的复杂性，每个数学家都有潜在的编程天赋，展开成矩阵形式，紫色表示$l-1$层第一个感知机对$l$层所有感知机的作用：</p>
<p><img src="/../img/image-20221007152657608.png" srcset="/img/loading.gif" lazyload alt="image-20221007152657608" style="zoom:50%;"></p>
<p>上面是隐藏层，接下来看输出层。最后得出一个$a^{[l]}$要通过损失函数的计算，计算出一个偏差值，这个损失函数除了依赖$a^{[l]}$，还要依赖y，y就是标签。</p>
<p>上标代表的是有m组数据，下标代表的是在一组数据有j个特征。</p>
<p><img src="/../img/image-20221007153826926.png" srcset="/img/loading.gif" lazyload alt="image-20221007153826926"></p>
<p>反向传播：</p>
<p><img src="/../img/image-20221007153944578.png" srcset="/img/loading.gif" lazyload alt="image-20221007153944578"></p>
<p>可以把输出层看作第$l+1$层：</p>
<p><img src="/../img/image-20221007154031476.png" srcset="/img/loading.gif" lazyload alt="image-20221007154031476"></p>
<p>得到：</p>
<p><img src="/../img/image-20221007154345236.png" srcset="/img/loading.gif" lazyload alt="image-20221007154345236" style="zoom:50%;"></p>
<p>已知：</p>
<p><img src="/../img/image-20221007154408181.png" srcset="/img/loading.gif" lazyload alt="image-20221007154408181" style="zoom:50%;"></p>
<p>用链式求导法则：</p>
<p><img src="/../img/image-20221007154420979.png" srcset="/img/loading.gif" lazyload alt="image-20221007154420979"></p>
<p>把第一项展开，就是之前求的梯度：</p>
<p><img src="/../img/image-20221007154640396.png" srcset="/img/loading.gif" lazyload alt="image-20221007154640396"></p>
<p>反向传播之后参数的修改值：</p>
<p><img src="/../img/image-20221007154918337.png" srcset="/img/loading.gif" lazyload alt="image-20221007154918337"></p>
<p><img src="/../img/image-20221007155154457.png" srcset="/img/loading.gif" lazyload alt="image-20221007155154457"></p>
<p>正常训练的话不需要理解这么多，就知道梯度是根据损失函数调参就可以，参数那么多，网络层那么多，这个微分只有计算机能做，所以不用纠结自己会不会计算。</p>
<h3 id="mini-batch"><a href="#mini-batch" class="headerlink" title="mini-batch"></a>mini-batch</h3><p>什么是mini-batch？</p>
<p>我们已知在梯度下降中，先训练一个模型，然后对所有样本进行分类，通过最小化损失函数来对参数进行一次调整。</p>
<p>也就是说，需要把所有样本处理一遍后才能往前走一步，此时样本规模特别大的话效率就会比较低。例如有500万，甚至5000万个样本(在我们的业务场景中，一般有几千万行，有些大数据有10亿行)的话走一轮迭代就非常的耗时，别说整个训练过程了。这个时候的梯度下降叫做full batch。</p>
<p> 所以为了提高效率，我们可以把样本分成等量的子集，例如我们把100万样本分成1000份， 每份1000个样本， 这些子集就称为mini-batch。我们对每一个子集做一次梯度下降。 然后更新参数W和b的值。接着到下一个子集中继续进行梯度下降。 这样在遍历完所有的mini-batch之后我们相当于在梯度下降中做了1000次迭代。 我们将遍历一次所有样本的行为叫做一个 epoch，也就是一个世代。 在mini-batch下的梯度下降中做的事情其实跟full batch一样，只不过我们训练的数据不再是所有的样本，而是一个个的子集。 这样在mini batch我们在一个epoch中就能进行1000次的梯度下降，而在full batch中只有一次。 这样就大大的提高了我们算法的运行速度。</p>
<p><strong>mini batch的效果</strong></p>
<p>左边是full batch的梯度下降效果。 可以看到每一次迭代成本函数都呈现下降趋势，这是好的现象，说明我们w和b的设定一直再减少误差。 这样一直迭代下去我们就可以找到最优解。 右边是mini batch的梯度下降效果，可以看到它是上下波动的，成本函数的值有时高有时低，但总体还是呈现下降的趋势。 这个也是正常的，因为我们每一次梯度下降都是在mini-batch上跑的而不是在整个数据集上。 数据的差异可能会导致这样的效果(可能某段数据效果特别好，某段数据效果不好)。但没关系，因为他整体的是呈下降趋势的。</p>
<p><img src="/../img/format,png.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>由于mini batch一个epoch就走了5000步，而full batch一个epoch只有一步。所以虽然mini batch走了弯路但还是会快很多。</p>
<p><strong>经验公式</strong></p>
<p>既然有了mini batch那就会有一个batch size的超参数，也就是块大小。代表着每一个mini batch中有多少个样本。 我们一般设置为2的n次方。 例如64，128，512，1024.。一般不会超过这个范围。不能太大，因为太大了会无限接近full batch的行为，速度会慢。 也不能太小，太小了以后可能算法永远不会收敛。 当然如果我们的数据比较小， 但也用不着mini batch了。 full batch的效果是最好的。</p>
<h2 id="优化梯度下降算法"><a href="#优化梯度下降算法" class="headerlink" title="优化梯度下降算法"></a>优化梯度下降算法</h2><p>梯度下降法计算太过复杂，计算机也难以做到，因此需要将梯度下降法进行优化，有两个思路，分别是减少每次计算的计算量和优化下降路径，用更少的步数，更快的到达极值点。</p>
<h3 id="优化每次计算量"><a href="#优化每次计算量" class="headerlink" title="优化每次计算量"></a>优化每次计算量</h3><h4 id="随机梯度法"><a href="#随机梯度法" class="headerlink" title="随机梯度法"></a>随机梯度法</h4><p>该方法旨在减少每次计算的计算量。</p>
<p>期望的意义是，可以摆脱开样本的具体数据，又能代表整个样本的均值。如果从期望或者说平均值这个角度来考虑的话，那计算的时候用了多少样本数据，是否覆盖了整个样本空间，就不再是一个关键性要素了。</p>
<p>简单说就是，样本反映整体，机器学习本质上都是统计学。</p>
<p>按照这个思路，随机梯度法被提出来了。</p>
<p>原来是把每一个数据都计算一遍然后求和，现在是随机挑一个数据，用这个数据计算，最后修改参数值，下一次再随机挑一个数据，再训练一次，最后也能达到训练的效果。</p>
<p>题外话：随机梯度下降法是一次一个样本，只是现在多将小批量随机梯度下降法简称为随机梯度下降法了，很多人也将他叫做mini-batch方法。</p>
<p>下面需要证明的是，样本越大均值的方差也越小，我们这样选样本最后真的可以收敛到我们想要的极值点吗？</p>
<p>证明结果是在凸问题中，最后结果是：</p>
<p><img src="/../img/image-20221007161011189.png" srcset="/img/loading.gif" lazyload alt="image-20221007161011189" style="zoom:50%;"></p>
<p>$k$代表迭代次数，$f^\star$代表极值点，$o$代表时间复杂度的渐进符。</p>
<p>意思是，经过k次训练后，随机梯度下降法能达到的误差，是$\frac{1}{\sqrt{k}}$这个量级的。如果是强凸问题的话，它的收敛还会更快：</p>
<p><img src="/../img/image-20221007161352579.png" srcset="/img/loading.gif" lazyload alt="image-20221007161352579" style="zoom:50%;"></p>
<p>也就是说，除非迭代很多次，否则误差还是这么大，收益不是很理想，再看看别的方法。</p>
<h3 id="优化步长"><a href="#优化步长" class="headerlink" title="优化步长"></a>优化步长</h3><p>即使梯度已经指向了下降最快的方向，但还是有一定优化空间的，因为求的是每一个点的梯度，如果我们将步长设置为无限小的话，就等于找到了一条完美的最优路径，但是这种计算量是目前计算机无法达到的。因此，每次迭代要确定的步长就为优化带来了可能性。</p>
<p>打个比方，如图，在A点求梯度后，延长到了B点，B点的梯度已经不和A点的重合了，但是由于设置步长过长的缘故，还是把B点的梯度设置为和A点一样的，这就和最优路径产生了偏差。</p>
<p><img src="/../img/image-20221007162141113.png" srcset="/img/loading.gif" lazyload alt="image-20221007162141113" style="zoom:50%;"></p>
<p>但是减小步长就意味着增加了步数，也就是计算量。</p>
<h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>不用直线当路径了，用曲线。如图，绿色抛物线在一定范围内，效果是优于橙色直线的，但是超过一定范围，效果比直线还差。也就是说，如果以抛物线的方式进行学习的话，步长在$\Delta x$范围内，它的学习效果最好，远远优于直线。可以得到：</p>
<ol>
<li>固定学习率，即需要在参数空间的附近的等距面上寻找最小值。</li>
<li>在当前位置做泰勒展开，取阶数越多，寻找的最小值位置越精确。</li>
<li>学习率无限小时，一阶即完全精确，学习率有限时，二阶比一阶精确。</li>
<li>下一步走到哪取决于学习率多大。</li>
</ol>
<p><img src="/../img/image-20221007165007749.png" srcset="/img/loading.gif" lazyload alt="image-20221007165007749"></p>
<p>拓展到高维平面，如图是损失函数的等高线：</p>
<p><img src="/../img/image-20221007165336607.png" srcset="/img/loading.gif" lazyload alt="image-20221007165336607"></p>
<p>橙线是梯度下降法的优化路径，绿线是牛顿法的优化路径，明显更接近最优路径，数学表达就是左边对参数W进行迭代，计算量依旧很大。</p>
<p>牛顿法本质是把下降路径的所有维度放到一起，统一考虑，看看能不能找到一个更好的路径，现在我们把下降路径的维度拆分开，看看有没有更好的思路。</p>
<p>这是一种常见的求梯度下降的时候会遇到的情况，梯度方向反复震荡折叠趋近于极值点，我们看到造成它来回震荡的原因是纵轴上的分量，横轴始终指向正确的方向。</p>
<p><img src="/../img/image-20221007170043059.png" srcset="/img/loading.gif" lazyload alt="image-20221007170043059"></p>
<p>我们也许能找到一种方法，使他在横轴上加速，让它可以在横轴的分量上更快的到达极值点。</p>
<p><img src="/../img/image-20221007170342354.png" srcset="/img/loading.gif" lazyload alt="image-20221007170342354"></p>
<p>比如说在某一个点，它计算的梯度在纵轴的分量是一个向上的纵向向量，再去看它的历史向量，与它方向相反，是向下的，那它就会减少这个维度上的量。再看横轴，现在是向前的，之前也是向前的，和自己一样，那就增加这个维度上的量。</p>
<p>通过这个方法减少纵轴上的震动，增加横轴上的变化速度，这就是利用历史数据来修正分量。具体计算如下：</p>
<p><img src="/../img/image-20221007174235832.png" srcset="/img/loading.gif" lazyload alt="image-20221007174235832"></p>
<p>新一步的参数=上一步的参数-学习率x梯度，对应到每一个分量上，就是第二列的样子。</p>
<p>为了表达方便再引入一个变量，$\Delta W$代表了W系数在$i$维度下应该调整多少，在标准的梯度下降法下，它调整的幅度应该是梯度的分量，写出来就是它下面那个式子。</p>
<p><img src="/../img/image-20221007181926093.png" srcset="/img/loading.gif" lazyload alt="image-20221007181926093"></p>
<p>再把历史数据也考虑进来：本来有了$\Delta W$之后，直接用在参数上进行学习，现在增加一个中间过程，一个迭代出的量$V(t)$，它其实代表了历史上前面所有步的$\Delta W$的和，最后把它作为参数修改的量放进来，然后进行学习，<u>这里可以参考卡尔曼滤波。</u></p>
<p>只是简单相加的话，就相当于把前面所有历史数据都一视同仁的全部考虑，不是很合适，所以会用一下<u>指数加权移动平均</u>。</p>
<p><img src="/../img/image-20221007182950802.png" srcset="/img/loading.gif" lazyload alt="image-20221007182950802"></p>
<p>假设$\beta$取0.9的话，把这个式子展开：</p>
<p><img src="/../img/image-20221007183159392.png" srcset="/img/loading.gif" lazyload alt="image-20221007183159392"></p>
<p>可以看到越往前的量越不重要，也说明了以前的改变是有惯性的，要想发生改变必须先讲它们抵消，然后才能发生变化。</p>
<p>这样的方法就叫动量法，或者叫冲量法。</p>
<p>橙色是梯度方向，绿色是历史冲量方向，通过前面的铺垫知道，实际优化的下降路径介于这二者之间，然后我们发现它绕路了。于是提出假设：如果它有预知未来的能力，知道第二步会落在那个点上，它第一步修正的幅度就不是这么小了，怎么做到超前呢？引入Nesterov算法。</p>
<p><img src="/../img/image-20221007184507900.png" srcset="/img/loading.gif" lazyload alt="image-20221007184507900" style="zoom:50%;"></p>
<h4 id="Nesterov算法-动量法"><a href="#Nesterov算法-动量法" class="headerlink" title="Nesterov算法+动量法"></a>Nesterov算法+动量法</h4><p>在上一节点把当前的梯度纳入历史，所以每一个节点都比之前多考虑一步。移动到新的一点的梯度方向，和下一级的v合成新的方向。</p>
<p><img src="/../img/image-20221007184831378.png" srcset="/img/loading.gif" lazyload alt="image-20221007184831378" style="zoom:50%;"></p>
<p>将第t-1步的$\Delta W$做一个修正，本质上是让点进行移动，相当于让点移到第t-1步的$V(t-1)$的位置，也就是绿色向量的终点——红点的位置。也就是说，求的已经不是原点的偏导了，而是红点处的偏导$\overrightarrow{g}$，求出红点的偏导方向后再拿进来放到原点，去计算冲量。</p>
<p><img src="/../img/image-20221007202254214.png" srcset="/img/loading.gif" lazyload alt="image-20221007202254214"></p>
<p>得到的效果就相当于把红点处的偏导$\overrightarrow{g}$平移到原点处，也就是图上的紫色向量，它就是t时刻的$\Delta W$，而$V(t-1)$是绿色向量，它们之和就是新的冲量方向。</p>
<p>相当于在训练和学习的时候，对参数的变化量增加了一个0次项的修正，我们是否也可以对一次项，也就是步长也进行一定修正呢？可以理解为0次项修正是在修正路径方向，1次项修正是在修正步长。</p>
<p><img src="/../img/image-20221007202423959.png" srcset="/img/loading.gif" lazyload alt="image-20221007202423959" style="zoom:50%;"></p>
<p>学习率不应该是一个确定的值，否则在极值点附近很可能就无法收敛，比如说下面的情况，模型直接跨过了极值点。</p>
<p><img src="/../img/image-20221007202619106.png" srcset="/img/loading.gif" lazyload alt="image-20221007202619106" style="zoom:50%;"></p>
<p>接下来的过程中，由于步长一定，A点无法恰好到达极值点，就只会在极值点附近来回震荡无法停止。</p>
<p><img src="/../img/image-20221007202742713.png" srcset="/img/loading.gif" lazyload alt="image-20221007202742713" style="zoom:50%;"></p>
<p>最简单的方法是，每一步减少一个固定的值，当减少到0的时候，迭代停止。那么能否让学习率自动调整呢？那得看Adagrad方法了。</p>
<h4 id="Adagrad方法"><a href="#Adagrad方法" class="headerlink" title="Adagrad方法"></a>Adagrad方法</h4><p>靠前面的历史数据来让学习率实现自适应。</p>
<p><img src="/../img/image-20221007203006949.png" srcset="/img/loading.gif" lazyload alt="image-20221007203006949" style="zoom:50%;"></p>
<p>相当于是在学习率$\eta$下面除以一个数值，是历史上每一个梯度的内积开方，学习到的梯度是真实梯度除以梯度内积的开方。adagrad本质是解决各方向导数数值量级的不一致而将梯度数值归一化。再增加一个$\epsilon$极小量避免分母为0。</p>
<p>最后得到的结果是：在相对平坦梯度较小时，始终保持较大学习率；一旦产生较大梯度，学习率将维持在较低水平。</p>
<p>总之，动量法修正的是0次项，adagrad修正的是一次项，适合稀疏数据，二者不可替代，也为二者结合提供了可能性。</p>
<p>虽然我们输入神经网络一开始的是一个照片，但经过一层层的隐藏层之后，到了最后一层隐藏层上的感知机，其实已经变成了一个个的特征。我们说的向量上的维度就是这些特征，如果某一个特征的学习率较大，就意味着接下来的学习中，对这个特征的调整比较大，如果某一个特征的学习率较小，就意味着接下来的学习中，对这个特征的调整比较小。</p>
<p>稀疏数据是指，训练集里的两个数据，它们之间的不同，更多的是体现在了特征的不同上，而不是某个具体特征上的程度不一样。举例来说：给两张照片，一个人一个猴子，要想去分开只要看有毛还是无毛，有尾巴还是没尾巴，不用看毛的多少或者尾巴的长短。</p>
<p><img src="/../img/image-20221007204448895.png" srcset="/img/loading.gif" lazyload alt="image-20221007204448895"></p>
<p>再给两只猫，一只英短，一只波斯，区分起来就要看毛的长短，就不是稀疏数据。</p>
<p><img src="/../img/image-20221007204636556.png" srcset="/img/loading.gif" lazyload alt="image-20221007204636556"></p>
<p>在稀疏数据的情况下，意味着某些数据的缺失，梯度下降法会产生振荡，而使用adagrad算法优势就比较明显了，因为它就是为了减少震荡的。</p>
<p>维度灾难：随着维度的增加，数据会越来越稀疏的，选择adagrad算法会更有效果。</p>
<p>优化方法1：加一个指数加权，变成RMSprop方法。</p>
<p><img src="/../img/image-20221007205300980.png" srcset="/img/loading.gif" lazyload alt="image-20221007205300980"></p>
<p>优化方法 2 : 将动量法和RMSprop方法结合起来，变成Adam算法。</p>
<p><img src="/../img/image-20221007205401792.png" srcset="/img/loading.gif" lazyload alt="image-20221007205401792"></p>
<p>优化方法 3:  和Nesterov方法结合，变成Nadam算法。</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p><img src="/../img/image-20221030223602834.png" srcset="/img/loading.gif" lazyload alt="image-20221030223602834"></p>
<p>回看全联接神经网络，输入层是我们的数据，隐藏层是对输入的数据提取特征，或是对已经提取出来的特征进行线性组合，输出层是根据特征对输入进行线性划分。</p>
<p><img src="/../img/image-20221030223838275.png" srcset="/img/loading.gif" lazyload alt="image-20221030223838275"></p>
<p>图片作为输入，一个像素点代表一个维度，每个像素点都是同质的，它的意义就是该像素点的灰度值或RGB值。</p>
<p>而上面的神经网络中，播放量、点赞数这些数据的维度是异质的，完全可以作为最后可以作为判断的特征，神经网络只要对他们做线性组合就可以进行判断了。即使他们还可以组合成更抽象的特征，比如将点赞数和投币数抽象为视频质量等，但是不能否认，最开始的这些数据已经可以看成抽象程度比较低的特征了。</p>
<p>而在看图片输入，我们很难说一个像素或几个像素就是一个特征，打个比方，把一个像素平移8个单位，像素点完全变了，但内容完全没变，所以像素不是特征。毕竟对图片识别起到作用的不是某个像素点具体的RGB值，而是一个像素点和周围其他像素点的相对关系。</p>
<p>因此，我们再将图片数据输入时，可以对其作预处理，不是每个点具体的数值，而是一个它和周围像素点存在着什么关系的特征值，这样神经网络就可以直接对特征进行各种组合，然后进行判断了，而不是还要通过全联接层，把像素点和像素点之间关系的特征计算一遍。</p>
<p>因为在我们直接把图片信息输入神经网络时，它提供的每个像素的信息和位置通常是绑定的，很难被复用。就比如这个数字8，我们可以看出它上下两个圆圈具有的特征是相同的，但是神经网络却不能对他们进行复用，只能把它们分别看作是不同的特征。但如果我们把输入变为每个像素点和周围像素点的位置关系，而不是它本身的RGB数值，就可以避免这一点。</p>
<p>打个比方，在识别图例8时，有下面几个模式，我们可以看到图例8最符合的是模式1，有0.9的相似度，别的模式相似度仅有0.01。我们把这些预设的模式作为特征，将输入转化为这些特征值，特征值即为原始数据里面具有这些特征的强弱。</p>
<p><img src="/../img/image-20221030234240073.png" srcset="/img/loading.gif" lazyload alt="image-20221030234240073"></p>
<p>这个模式，或者说特征，用数据的方式呈现出来，就是卷积核，这些特征值就是卷积核和图像计算后的结果。</p>
<p>这些模式之间是无法公度的，那么我们要如何把它们呢集合成一个数据，保存到一个像素点中呢？还是用加权求和的方法。</p>
<p><img src="/../img/image-20221031010817052.png" srcset="/img/loading.gif" lazyload alt="image-20221031010817052"></p>
<p>基于这样加工后的数据再输入到神经网络里，神经网络用来判断的依据就不再是像素点的绝对位置，而是相对位置了，目标达成。</p>
<p>而且加工后的数据与RGB数值形式上无本质区别，因此就可以对已经处理过的数据反复进行同样的操作，这样就像神经网络增加了更多隐藏层一样，让相对位置的模式抽象程度可以更高，让一个像素点的数据可以体现更大范围的特征。</p>
<p>现在要做的就是把这个模式和特征值通过计算机能实现的方式来表达和计算出来。</p>
<h3 id="傅立叶变换"><a href="#傅立叶变换" class="headerlink" title="傅立叶变换"></a>傅立叶变换</h3><p>信号里常见的概念——时域和频域，假如左边是一个电脉冲信号，纵轴是电压的高低，横轴是时间，用频域表达成右边这样，横轴就是频率，纵轴是对应频率信号的强弱，这个转换关系就是把F(t)信号变成如下形式：</p>
<script type="math/tex; mode=display">
F(w)=\int^{+\infty }_{-\infty}f(t)\cdot e^{-iwt}dt</script><p><img src="/../img/image-20221031011558817.png" srcset="/img/loading.gif" lazyload alt="image-20221031011558817"></p>
<p>我们可以把图片看作一个静态的信号，但可以把像素点的位置看成是这个空间里的坐标轴，而且是一个2维的，像素点间的距离是x，y轴组成的平面，像素点的RGB数值是第三根坐标轴，这样这个信号的周期就不是用时间来表示了，而是像素点之间的距离，此时时域就是空间域，频域就是变换域。</p>
<p><img src="/../img/image-20221031013032998.png" srcset="/img/loading.gif" lazyload alt="image-20221031013032998" style="zoom:50%;"></p>
<p>我们希望的是把一个像素点和周围其他像素点的关系浓缩到一点上，而傅立叶变换中，变换域里的一个点对应一个F(w)的取值，映射到空间域就是把空间域从正无穷到负无穷全部都考虑了进来，也就是一个变换域的点把空间域的全域信息进行了浓缩。</p>
<p>具体来讲，就是在频域中不同角速度w的取值在整个信号阶段的强弱，把它对应到模式中，就是w取不同模式时像素点的特征值，只不过这里的F(w)是一个复变函数，它的具体取值是一个复数，把F(w)换成复数表达形式如下，而复数会提供两个信息，一个是复数的模长——不同模式下正弦余弦曲线的振幅，另一个则是幅角——对应不同模式下正弦余弦曲线的相位。当我们变换空间域信号的时候，信号的振幅已经和位置无关了，只会对幅角产生影响，达到了我们想要提取特征值的效果。</p>
<script type="math/tex; mode=display">
F(w)=|F(w)|\cdot e^{i\varphi}</script><script type="math/tex; mode=display">
\varphi=arctan \frac{Im[F(w)]}{Re[F(w)]}</script><p><img src="/../img/image-20221031014110911.png" srcset="/img/loading.gif" lazyload alt="image-20221031014110911" style="zoom:50%;"></p>
<p>用信号的角度看模式和特征值，二者相乘就代表这个模式在原信号中的强弱，我们只需要把所有特征值和对应模式相乘再相加，应该就能把原信号给还原出来了，这就是傅立叶逆变换。</p>
<script type="math/tex; mode=display">
f(t)=\frac{1}{2 \pi} \int^{+\infty }_{-\infty}F(w)\cdot e^{iwt}dw</script><p>我们目前已经做到把空间域中一个模式的全局情况浓缩到一个点上，但我们希望的是局部情况，这就是加窗（短时）傅立叶变换。</p>
<p>对空间域做一个升维，升到无穷维。在空间是二维的时候，我们可以把空间中任何一个点在x和y数轴上的投影看作是坐标，用两个数字就可以把这个点描述出来，同样，在无穷维的时候，就相当于是有无穷个坐标轴，每个点在无穷个维上都可以做投影。我们把这些维度一一对应到一个实数数轴上，纵轴就代表在这些维度上的取值，于是在无穷维空间中的一个点，就被投射成了一个二维空间中的一条曲线。</p>
<p><img src="/../img/image-20221031163942538.png" srcset="/img/loading.gif" lazyload alt="image-20221031163942538"></p>
<p>现在这个绿色曲线是乱画的，但如果它是下图这样，不就是空间域里信号所代表的曲线嘛。也就是说，二维时域空间里用一条曲线描述的问题，到无穷空间只需要一个点就可以。</p>
<p><img src="/../img/image-20221031164032094.png" srcset="/img/loading.gif" lazyload alt="image-20221031164032094"></p>
<p>通俗来讲，一个实数函数就是无穷维空间的一个点！！无穷维空间的点也可以和二维空间中的曲线一一对应。</p>
<p>这样我们把空间升到无穷维的好处就体现出来了，我们可以让二维空间的曲线降维成一个点，并且因为变换后的空间是无穷维的，就算是二维平面，三维立体，只要图像的维度是有限的，都可以被降维到一点。</p>
<p>降维成一个点的好处真正体现的还是一个空间的点可以用向量来等价表示，当我们把原来的f(t)函数看作是一个无穷维空间中的向量之后，就可以进行别的操作了，就比如下图所示的坐标系变换，它的坐标向量推导公式如下，最终凑出了一个f向量和坐标轴向量的内积乘上坐标轴向量的单位向量。</p>
<p><img src="/../img/image-20221031164959659.png" srcset="/img/loading.gif" lazyload alt="image-20221031164959659"></p>
<p>一个是无穷维，又有内积的空间，就被称作是希尔伯特空间，它其实是欧几里得空间的扩展。在欧几里得空间中，向量内积如第一个公式所示，希尔伯特空间和它类似，只不过向量的坐标值可以用函数来表示，于是最后内积就写成了下面的形式。</p>
<p><img src="/../img/image-20221031165338747.png" srcset="/img/loading.gif" lazyload alt="image-20221031165338747"></p>
<p>于是坐标变换公式就等于内积的定积分乘以对应坐标向量的单位向量$\hat d_n$，这个坐标轴的单位向量$\hat d_n$就可以看成我们前面的模式，不同坐标轴代表着不同的维度，也就是不同的模式，前面的定积分也就是在这个模式下的特征值。</p>
<p><img src="/../img/image-20221031165847437.png" srcset="/img/loading.gif" lazyload alt="image-20221031165847437" style="zoom:50%;"></p>
<p>综上，从时域到频域其实就是在希尔伯特空间做了一次坐标变化，频域里的函数就是坐标变换后特征值的表达式，这个特征值F(n)代表的就是向量f在新坐标系下各个坐标轴上分量的大小，因此它和坐标轴的单位向量相乘再求和，就是f向量本身，因此又得到了这个变化的逆过程。</p>
<p><img src="/../img/image-20221031171010244.png" srcset="/img/loading.gif" lazyload alt="image-20221031171010244" style="zoom:50%;"></p>
<p>注意：我们能做坐标变化的前提是要保证这些坐标轴$\hat d_n$要是两两正交的，如果模式不正交的话就需要先把模式进行简化为正交系，在数学上面就是求矩阵的满秩矩阵，或者看ds洗数据的本事，数值分析函数拟合有讲，如果不正交需要求解希尔伯特线性方程组。</p>
<p><img src="/../img/image-20221031173415628.png" srcset="/img/loading.gif" lazyload alt="image-20221031173415628" style="zoom:50%;"></p>
<p> 而傅立叶变换就是把希尔伯特空间坐标变换里的坐标基变为$e^{-iwt}$，这组基一定是正交的。这组基本身也是向量，它在时域中的样子是一个全局的正弦余弦曲线。我们需要的加窗傅立叶变换需要对这组基做一些改变，使它不再考虑全局的情况，就需要对这个正弦余弦曲线做一个约束，让它只在一个小窗口里进行波动，其他情况全等于0。于是这个窗口的特征就成了衡量信号的新锚点，坐标轴需要考虑的情况就增加了一个维度。</p>
<p>窗口开在不同的位置就是不同的坐标轴，窗口里面信号的频率不同也是不同的坐标轴，这个有窗口范围的基就可以这样表示，其中变量s就是窗口的位置，$(s-1,s+1)$就是窗口的大小。</p>
<p><img src="/../img/image-20221031180105954.png" srcset="/img/loading.gif" lazyload alt="image-20221031180105954"></p>
<p>但在实际计算时，这种分段函数不是处处可微的，于是我们可以用高斯分布来做g函数，在这里a就是方差，决定了窗口的大小，s是期望，决定了窗口的位置，窗口画出来就长这样：</p>
<p><img src="/../img/image-20221031182426495.png" srcset="/img/loading.gif" lazyload alt="image-20221031182426495" style="zoom:50%;"></p>
<p><img src="/../img/image-20221031182446980.png" srcset="/img/loading.gif" lazyload alt="image-20221031182446980" style="zoom:50%;"></p>
<p>这个用高斯分布做g分布其实就是Gabor变换，经过变换后的特征函数有两个变量，一个是n控制w，也就是不同的模式，另一个就是s，控制窗口的位置，窗口大小只是一个参数而非变量。</p>
<p><img src="/../img/image-20221031182822562.png" srcset="/img/loading.gif" lazyload alt="image-20221031182822562" style="zoom:50%;"></p>
<p>这个g函数为傅立叶变换增加了很多可能性，如果g是指数函数，其实就是拉普拉斯变换，它会让正弦余弦曲线不断衰减。</p>
<p><img src="/../img/image-20221031183049530.png" srcset="/img/loading.gif" lazyload alt="image-20221031183049530" style="zoom:50%;"></p>
<p>如果这个窗口大小不再是固定的，而是也可以作为变量，可以根据w动态的变化，那么这就是一个小波变化。</p>
<p><img src="/../img/image-20221031183309106.png" srcset="/img/loading.gif" lazyload alt="image-20221031183309106" style="zoom:50%;"></p>
<p>并且正弦余弦曲线也不是固定的，也是可以被其他的波形代替的，但本质上都是在希尔伯特空间选择一组不同的基进行变换。</p>
<p>现在我们已经通过傅立叶变换实现了让特征和位置无关，不论这个特征出现在什么位置，反映到变换域中都应该是相同的特征值。现在我们要通过Garbo变换来实现当图像的不同位置出现了相同的两个特征，那么反映到变换域里，它们的特征值也应该是相同的。</p>
<p>左边是时域空间，因为gabor变换对应的特征值f函数有了两个变量，所以他就需要用三维画面来展示了。如果是这样的一个波形，在其他地方增加一个同样的波形，那么我们在变换域上就会相应的增加了一个形状相同的鼓包，这里的变量n代表不同的w取值，或者说是不同的模式，而变量S则是位置信息，它的取值对应到时域里面就是不同的窗口位置，只有把窗口开在相应的波形的位置上，在变换域上才会被明显的体现出来。</p>
<p><img src="/../img/image-20221031184840290.png" srcset="/img/loading.gif" lazyload alt="image-20221031184840290" style="zoom:50%;"></p>
<p>那么要如何把Gabor变换对应到卷积上呢，我们来一一对应一下。</p>
<p><img src="/../img/image-20221031185832483.png" srcset="/img/loading.gif" lazyload alt="image-20221031185832483" style="zoom:50%;"></p>
<p>在把图片输入到全联接神经网络之前，先对图片$f(t)$进行$F(n,S)$变换，左边绿色像素点周围的特征就会被储存到右边相应位置的点阵中，</p>
<p>把这个点阵中的绿色像素点展开，就是各种不同卷积核计算的结果，不同的卷积核代表的是不同的模式n，卷积核下面具体的特征值就是$F(n,S)$的取值，卷积核的中心对准的就是变换结果里的位置S，所以卷积核的大小就是窗口的大小3x3，也就是参数a。而模式或者说卷积核中的具体数值，也就是窗口里面具体的波形是什么，最终是由黄框里的函数来决定的，也是由卷积神经网络自己学习出来的。因为用卷积核进行计算本质上也是一种线性计算，所以一个卷积核里面它的参数具体是多少，通过梯度下降法就能学习到。</p>
<p><img src="/../img/image-20221031190853629.png" srcset="/img/loading.gif" lazyload alt="image-20221031190853629" style="zoom:50%;"></p>
<p>严格地说，这里已经不是Gabor变换了，Gabor变换一般特指窗口函数是高斯分布，对应的波形也得是正弦余弦曲线，但在卷积神经网络里，具体学到的卷积核是什么样子就不一定了，只是作为一个理解的切入点。</p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>CNN的卷积其实是互相关，实际上加入进行卷积，还需要对过滤器进行两次镜面反转。</p>
<p>卷积公式：</p>
<p><img src="/../img/image-20221009220304188.png" srcset="/img/loading.gif" lazyload alt="image-20221009220304188" style="zoom:50%;"></p>
<p>如何理解卷积，打个比方，一个人特别喜欢吃东西，不止一日三餐吃，零食也不断，总之就是24h都在吃，如果画下来就是这样。</p>
<p><img src="/../img/image-20221031192207176.png" srcset="/img/loading.gif" lazyload alt="image-20221031192207176" style="zoom:50%;"></p>
<p>g就表示某一刻进食后该食物随着时间变化剩余量的比例。</p>
<p><img src="/../img/image-20221031192404328.png" srcset="/img/loading.gif" lazyload alt="image-20221031192404328" style="zoom:50%;"></p>
<p>把进食多少用f函数表示，消化多少用g函数表示，带进卷积公式后就能计算出这个人在某个时间胃里还剩多少食物，以下午两点进行举例，只看它进食不考虑消化的话只要求一个积分就行，但是他还要消化。如果在12点吃了一碗米饭，在14点时就要讲2h消化的比率代入g函数，看米饭在14点还剩多少。</p>
<p><img src="/../img/image-20221031193039444.png" srcset="/img/loading.gif" lazyload alt="image-20221031193039444" style="zoom:50%;"></p>
<p>更一般的可以写成如下形式，</p>
<p><img src="/../img/image-20221031193144915.png" srcset="/img/loading.gif" lazyload alt="image-20221031193144915"></p>
<p>将这对x和t-x用f和g联系起来就是：</p>
<p><img src="/../img/image-20221031193338930.png" srcset="/img/loading.gif" lazyload alt="image-20221031193338930" style="zoom:50%;"></p>
<p>总结一下就是存在一个系统，他的输入是不稳定的，输出是稳定的，那么我们就可以用卷积来求系统的存量。上面这张图看着很别扭，但是我们只要把它翻转一下，就很明朗了。</p>
<p><img src="/../img/image-20221031193601047.png" srcset="/img/loading.gif" lazyload alt="image-20221031193601047" style="zoom:50%;"></p>
<p>CNN主要应用在图片输入，它在把图片输入神经网络之前，会对图像进行一个卷积操作。</p>
<p>什么是卷积操作？其实就是用一个3x3的点阵，和图像进行一个操作，这个3x3的点阵就叫做卷积核。操作的过程就是把卷积核扣在图像的点阵上，然后对应的两个各自相乘，3x3点阵也就是9个数，把这九个结果相加保存到一起，就相当于得到了一个新的像素点。</p>
<p><img src="/../img/image-20221031194441478.png" srcset="/img/loading.gif" lazyload alt="image-20221031194441478" style="zoom:50%;"></p>
<p>然后用这个卷积核把整个图像都扫一遍，得到一个新的点阵，这样处理得到的图片会少了最外面一圈，我们只需要在处理之前给图像外面加一圈都是0的像素就可以了，</p>
<p><img src="/../img/image-20221031194648579.png" srcset="/img/loading.gif" lazyload alt="image-20221031194648579" style="zoom:50%;"></p>
<p><img src="/../img/image-20221031194820757.png" srcset="/img/loading.gif" lazyload alt="image-20221031194820757" style="zoom:50%;"></p>
<p>在这里，图像就是函数f，卷积核就是函数g，因为图像里的像素点总在不停的变化，对应了系统中那个不稳定的输入，卷积核不变，对应的是稳定的输出。把刚刚吃饭的那个一维的例子扩展到二维，画出来就是一个平面波，这个F(X,Y)就是图像，g函数是一个随着距离t的变化对x的影响。可以说，就是把之前的事件的影响按距离的权重方程累加起来</p>
<p><img src="/../img/image-20221102154417085.png" srcset="/img/loading.gif" lazyload alt="image-20221102154417085" style="zoom:50%;"></p>
<p>比如这样一个卷积核，本质上就是找了一个像素点，把它周围的像素点全加起来，然后求平均，这样处理完图像就会变得更平滑，更朦胧，所以叫做平滑卷积操作。</p>
<p><img src="/../img/image-20221102163132560.png" srcset="/img/loading.gif" lazyload alt="image-20221102163132560" style="zoom:50%;"></p>
<p><img src="/../img/image-20221102163437083.png" srcset="/img/loading.gif" lazyload alt="image-20221102163437083" style="zoom:50%;"></p>
<p>也就是说，卷积核就是规定了周围的3x3像素点是如何对当前的像素点产生影响的，当然也存在5x5或者7x7这种两圈甚至三圈像素点对当前像素点产生的影响，甚至是无穷圈，但是考虑计算机计算时的性价比，如果一圈能搞定的话就不会再往多圈考虑。</p>
<p>f和g之间的星号代表卷积操作，像素点不是连续的，因此展开不是积分，而是连加。x和m-x相加之后消掉x只剩m，同理消掉y只剩n。</p>
<p><img src="/../img/image-20221102190652020.png" srcset="/img/loading.gif" lazyload alt="image-20221102190652020" style="zoom:50%;"></p>
<p>如果我们考虑左下角像素对中间像素的影响，就相当于把中间像素当作t时刻，左下角像素当作x时刻，则影响因子g(t-x)代入计算就是g(1,1)。如果把所有周围的和他自己对该像素产生的影响，就是以下形式：</p>
<p><img src="/../img/image-20221102191041854.png" srcset="/img/loading.gif" lazyload alt="image-20221102191041854" style="zoom:50%;"></p>
<p>这里表现的g函数并不是卷积核，它要旋转180度之后才是卷积核。如果用了下面这两种卷积核的话，上面这个只把垂直方向的边界给挑出来了，忽略了横向的边界，下面这个只把左右方向的边界挑出来。这个时候虽然还是进行的卷积操作，但其实是把图片里的一些特征给挑了出来，所以说卷积核还有个功能，就是如果选择的合适，还可以把图片的某些特征给挑出来，而其他的特征就被过滤掉了，这样的卷积核也叫过滤器，比如下图这两个就叫做垂直边界过滤器和水平边界过滤器。</p>
<p><img src="/../img/image-20221102192426626.png" srcset="/img/loading.gif" lazyload alt="image-20221102192426626" style="zoom:50%;"></p>
<p>而且，这个卷积先相乘再相加的操作可以看作是对周围像素点的试探，而卷积核就是试探的模版，当你不想考虑某个位置的时候，就把这个位置的卷积核设为0，想考虑的时候就把它设置为一个较高的数值。给人的感觉这个像素在主动的试探和选择周围的像素点，通过卷积核把周围有用的特征给保留了下来。</p>
<p>假设一层有K个神经元，下一层有m个神经元，全联接层需要的连接数量就是Km个，而卷积层和下一层之间的连接数则大大减少，变为mL个连接，L为过滤器大小，打个比方，下一层可能有1024个神经元，全联接层的计算量超大，但是卷积过滤器通常只连接3个神经元。若同时存在多个过滤器的话，一个神经元就对应一个多维的向量。</p>
<p><img src="/../img/image-20221109205728857.png" srcset="/img/loading.gif" lazyload alt="image-20221109205728857"></p>
<p>举个例子，假如我们想识别“X”这个字母，利用以下三个特征就可以把所有的X都识别出来。 </p>
<p><img src="/../img/image-20221102205255736.png" srcset="/img/loading.gif" lazyload alt="image-20221102205255736" style="zoom:50%;"></p>
<p>比如说第一种情况，卷积核计算出来等于1，完美匹配。</p>
<p><img src="/../img/image-20221102205409463.png" srcset="/img/loading.gif" lazyload alt="image-20221102205409463" style="zoom:50%;"></p>
<p>而用以下这个卷积核虽然也能匹配上，但不是完美匹配，最后计算出的结果就是0.55。</p>
<p><img src="/../img/image-20221102205619995.png" srcset="/img/loading.gif" lazyload alt="image-20221102205619995" style="zoom:50%;"></p>
<p>将整个图像和卷积核匹配后，就得到了与这个特征匹配的全部信息。</p>
<p><img src="/../img/image-20221102210038004.png" srcset="/img/loading.gif" lazyload alt="image-20221102210038004" style="zoom:50%;"></p>
<p>然后分别用三个卷积核进行操作，用不同的卷积核去对同一个图像做卷积，相当于是三个通道了所以输出也是三个不同的特征图，然后就要输入神经网络开始干活了，后面还有池化反向传播等等等等。</p>
<p><img src="/../img/image-20221102210105602.png" srcset="/img/loading.gif" lazyload alt="image-20221102210105602" style="zoom:50%;"></p>
<p>由卷积核得到的的输出称为feature map，卷积核可以理解为是一个权重的矩阵，一个5x5的输入数据经过3x3卷积核处理过后就输出了一个3x3的feature map。因此，我们只要设置了卷积核的大小，根据输入数据就可以计算出输出数据的形状。而在全联接层中，我们必须明确指定出隐藏层和输出层的神经元个数。</p>
<p><img src="/../img/image-20221109210542142.png" srcset="/img/loading.gif" lazyload alt="image-20221109210542142"></p>
<p>上面讲过，卷积层可由多个卷积核共同作用实现提取数据不同角度的抽象特征，打个比方，在识别一只猫时候，我们要对眼睛、鼻子、嘴巴、胡须、尾巴等多个特征进行识别，每个特征就是一个卷积核，每个卷积核输出的矩阵或者一维向量就是它在全局不同区域出现的权重。而识别这些具体特征之前，肯定要从更基本的线条弧度，粗细等特征进行提取，这样才能拼凑出像眼睛、鼻子这样的更抽象的特征。</p>
<p><img src="/../img/image-20221109211509614.png" srcset="/img/loading.gif" lazyload alt="image-20221109211509614"></p>
<p><strong>卷积核的深度</strong></p>
<p>上面这个图片是一张黑白图片，每个像素点只有一个灰度值信息，但我们如果要识别一张彩色图片的话，那每一个像素点就会有RGB三个值，输入就变成了下面的这种形式，一个6x6x3的张量，这里的6x6就是长和宽，3就是深度。每一个像素点是一个三维的向量，那么对应的卷积核，或者说是权重矩阵，它的每一个权重也应该是一个三维向量。</p>
<p><img src="/../img/image-20221109212531091.png" srcset="/img/loading.gif" lazyload alt="image-20221109212531091"></p>
<p><strong>卷积核的维度</strong></p>
<p>假如是一个语义判断这样的任务，输入就是一个时间序列这样的一维数据，那么卷积核的移动也应该遵从时间序列的一维方向，输出也将是时间这个维度。</p>
<p><img src="/../img/image-20221110172143879.png" srcset="/img/loading.gif" lazyload alt="image-20221110172143879"></p>
<p>假如在每个时间点上对应的是一个三维的向量，卷积核仍然遵从时间序列的移动方向，只不过卷积核会多两个深度，也变成三维向量。</p>
<p>假如把一个词变成一个5维的向量，那么6个词就是一个6x5的矩阵，这个5就是数据点的深度，而这一整串数据作为一个序列，方向还是只有一个，就是词位置的方向，所以处理这些5维向量，用的还是一维的卷积。等于说，你既可以把这一整串数据看作是长x宽=5x6的矩阵，也可以看作6个深度为5的一维向量，这要根据具体问题来分析。</p>
<p><strong>通道的概念</strong></p>
<p>和深度可以理解为是一个意思，打个比方，RGB图就是三个通道，对应三个颜色通道的特征映射，得到三个特征图，线性加权或直接加权后得到一个单通道的特征图。</p>
<p><img src="/../img/image-20221110191814267.png" srcset="/img/loading.gif" lazyload alt="image-20221110191814267"></p>
<p>因此，通过一个相同通道数的卷积核，原始数据会变得更加简单。因此，卷积核与它所处理的数据一般拥有相同的通道数。</p>
<p><img src="/../img/image-20221110192111943.png" srcset="/img/loading.gif" lazyload alt="image-20221110192111943"></p>
<p>输出数据的通道数则是由卷积核的个数所决定的，例如对一个32x32x3的输入数据用6个大小为5x5x3的卷积核进行处理后，32-5+1=28，我们就得到了6个通道的28x28的输出数据。</p>
<p><img src="/../img/image-20221110192523231.png" srcset="/img/loading.gif" lazyload alt="image-20221110192523231"></p>
<p>卷积核的参数量就等于6x3x5x5，再加上每个卷积核的偏置系数，总共就有6x3x5x5+6个参数需要训练。</p>
<p><strong>1x1的卷积核</strong></p>
<p>用于对输入数据的通道进行约减，相当于给数据降维，举个例子，图中输入数据的规模是64x64x192，按照前面所讲的，应该用一个与它相同深度的卷积核来进行处理，但这里用了1X1卷积核后，就相当于把64x64个192维的向量的每一个向量都用权值为1的全联接层处理了一遍，压缩成了64x64x1的数据规模。</p>
<p><img src="/../img/image-20221110193713766.png" srcset="/img/loading.gif" lazyload alt="image-20221110193713766" style="zoom:50%;"></p>
<p>因此，这个1x1的卷积核并没有改变特征图的结构，只是把这192个特征图压缩成了一个特征图。</p>
<p>这样做的目的就是对参数量进行一个约减，减少计算量，打个比方，现在想用一个5x5x32的卷积核去处理一个28x28x192的数据，28-5+1=24，它会变成一个24x24x32的输出数据。这个过程对应的计算量为：[24x24]x[5x5]x192x32=120.422million。</p>
<p>如果用16个1x1的卷积核先进行处理，再用5x5x32的卷积核，输入数据会先变成一个28x28x16的数据，再变成一个24x24x32的数据，需要的计算量为：(28x28x192x1x1x16)+(24x24x16x5x5x32)=12.4million，计算量少了几个亿。</p>
<p><img src="/../img/image-20221110200125297.png" srcset="/img/loading.gif" lazyload alt="image-20221110200125297" style="zoom:50%;"></p>
<p><strong>步长——卷积核滑动的间隔</strong></p>
<p>上面的案例都默认步长为1，若设定步长为2，(5-3)/2+1=2，最后将会得到一个2x2的特征图。</p>
<p><img src="/../img/image-20221110200323151.png" srcset="/img/loading.gif" lazyload alt="image-20221110200323151" style="zoom:50%;"></p>
<p>如果不想让输出规模小于输入，可以将周围进行一个填充，常用的有0填充或者是均值填充。</p>
<p><img src="/../img/image-20221110201000665.png" srcset="/img/loading.gif" lazyload alt="image-20221110201000665" style="zoom:50%;"></p>
<p><strong>汇聚层</strong></p>
<p>输入数据经过卷积层处理后，我们希望它已经可以执行一个分类的任务，比如输出一个十维的向量，每一维代表一个类别的后验概率。但是它输出的可能是一个5x5x16的张量，我们可以把它刻化为一个400维的向量，全联接层是一个很适合做向量维度转化的模型，我们可以把这个400维的向量输入进一个有10个输出神经元的全联接层实现分类。平铺层就是一个把5x5x16的张量铺开成向量的非功能层。</p>
<p>虽然在卷积层中，卷积核缩减了大部分的参数量，但是它输出的数据量可能并不小，当这个输出又作为全联接层的输入的时候，全联接层的参数数量将会非常的大。所以我们希望这个输出在进入全联接层之前，规模可以进行一些缩减。汇聚层就是一个可以对特征图形态进行约减的功能层，实际上它就是做了一个特征选择，从而减少参数量。</p>
<p><img src="/../img/image-20221110202709381.png" srcset="/img/loading.gif" lazyload alt="image-20221110202709381"> </p>
<p>其实就是池化层，也可以叫它过滤器，就和降低图片分辨率一个道理。它是在特征图上作约减，而卷积核是在通道上作约减。</p>
<p><img src="/../img/image-20221110202852743.png" srcset="/img/loading.gif" lazyload alt="image-20221110202852743" style="zoom:50%;"></p>
<p><strong>ResNet残差网络</strong></p>
<p>为了防止梯度爆炸和过拟合，它在经典的结构上加了一个前馈层。这样卷积层就变成了一个对H(x)的拟合与x的一阶表示的残差。</p>
<p><img src="/../img/image-20221110212840340.png" srcset="/img/loading.gif" lazyload alt="image-20221110212840340" style="zoom:50%;"></p>
<p>简化学习任务F(x)学习的是高阶表示相较于低阶表示之间的残差，并且当网络已经在某一层达到了最优的时候，将会在后续层上达到$F_l (x)=0$，来使得网络一直维持在最优状态上，如此缓解了由深度带来的过拟合。</p>
<p>在卷积神经网络中，我们可以活用1x1的卷积核来做维度的约减与还原。举一个比较简单的例子，假设输入数据的深度是256，先用一个64层的1x1卷积核做一个数据降维，然后再用3x3的卷积核做一个特征提取，再用一个256层的1x1卷积核将数据还原，来和输入数据的形状对齐，这样就能进行比较了，这种跨越2层或者3层来进行比较的方法也叫做shortcut。</p>
<p><img src="/../img/image-20221110215437720.png" srcset="/img/loading.gif" lazyload alt="image-20221110215437720" style="zoom:50%;"></p>
<h3 id="池化层的作用"><a href="#池化层的作用" class="headerlink" title="池化层的作用"></a>池化层的作用</h3><p>在卷积神经网络中通常会在相邻的卷积层之间加入一个池化层，池化层可以有效的缩小参数矩阵的尺寸，从而减少最后连接层的中的参数数量。所以加入池化层可以加快计算速度和防止过拟合的作用。</p>
<p>池化的原理或者是过程：pooling是在不同的通道上分开执行的（就是池化操作不改变通道数），且不需要参数控制。然后根据窗口大小进行相应的操作。 一般有max pooling、average pooling等。</p>
<p><strong>主要的作用</strong></p>
<p>（1）首要作用，下采样（downsamping）</p>
<p>（2）降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减小计算量、减小内存消耗等等。各种说辞吧，总的理解就是减少参数量。</p>
<p>（3）实现非线性（这个可以想一下，relu函数，是不是有点类似的感觉？）。</p>
<p>（4）可以扩大感知野。</p>
<p>（5）可以实现不变性，其中不变形性包括，平移不变性、旋转不变性和尺度不变性。</p>
<p><strong>池化主要有哪几种</strong></p>
<p>（1）一般池化（General Pooling）：其中最常见的池化操作有平均池化、最大池化：</p>
<ul>
<li><p>​    平均池化（average pooling）：计算图像区域的平均值作为该区域池化后的值。</p>
</li>
<li><p>​    最大池化（max pooling）：选图像区域的最大值作为该区域池化后的值。</p>
</li>
</ul>
<p>（2）重叠池化（OverlappingPooling）：重叠池化就是，相邻池化窗口之间有重叠区域，此时一般sizeX &gt; stride。</p>
<p>（3）空金字塔池化（Spatial Pyramid Pooling）：空间金字塔池化可以把任意尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。</p>
<p><strong>堆叠式卷积核</strong></p>
<p>用图说明，从下往上看。</p>
<p><img src="/../img/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3F5aGFpbGw=,size_16,color_FFFFFF,t_70.png" srcset="/img/loading.gif" lazyload alt="img" style="zoom:50%;"></p>
<ol>
<li>保证具有相同感受野的条件下，提升了网络的深度，在一定程度上提升了网络效果（从这点来说，也说明了ResNet的巨大作用）</li>
<li>保证具有相同感受野的条件下，减少了计算量和参数量</li>
</ol>
<p>两个3x3的可以代替一个5x5的，四个3x3的可以代替一个7x7的，以此类推。</p>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>我们想知道机器是怎么模仿人类学习的，先要知道人类是怎么学习的。首先，学习的意义就是预判，举一反三，见过同类型的题以后，变一变还能做出来。那么这个预判能力怎么来的呢？大概有三个方式：演绎、归纳和演化。而实现人工智能的三大流派分别是：符号主义、连接主义和行为主义，正好可以和上面一一对应。</p>
<p>演绎意味着绝对理性，单纯从数据中找到真理。归纳则是理性和非理性结合，从数据中萃取出可能需要的特征，再去寻找特征之间的关系，归纳出一定的规律。通过演绎推导出的结论一般是不会错的，而归纳可能因为数据量不够或者提取的特征不恰当等等原因推导出一个不恰当的规律。</p>
<p>而归纳可能会出错的特点，也意味着它更能容忍模糊和不精确，打个比方，在语义翻译领域，绝对精确演绎出的结果根本没法用，因为语言并不是绝对理性的，它还掺杂着情感。归纳则将理性降格，因为它承认这个世界理性与混乱共存。SVM的本质就是将数据两两比较，寻找差异最小的一种划分方法，也是一种归纳的方法。</p>
<p>演化则是利用一代又一代的经历和反馈，上一代全部是已知，下一代则还未知，比如遗传算法。和归纳的区别是，归纳虽然承认混乱和理性并存，但是还是会利用理性从已有的数据中去总结规律，它相信万事万物背后<u>都会有一个具有一致性的规律</u>，只是会被各种各样的因素污染，反映到数学上就是，<u>所有事物都可以被用一个函数表达出来</u>，而不确定的污染元素可以把它们看作是噪声。但是演化对规律并不执着，它是靠死记硬背把以前经历的事物中好的因素保留，不好的因素筛去，最后编织成一个巨大的DNA序列，它可以用来作神经网络的参数选择。</p>
<p>我们现在的机器学习主要还是归纳，也就是说，机器学习其实就是机器归纳。归纳的过程分为两步，从已知的数据中总结规律，再把规律应用到未知的领域中。难点是，随便应用到未知的领域，结果能靠谱吗？打个比方，有下面几个蓝点，要想用线把它们串起来，并且未来的点还会出现在这条线上，人脑会怎么穿呢？</p>
<p><img src="/../img/image-20221011102205004.png" srcset="/img/loading.gif" lazyload alt="image-20221011102205004" style="zoom:50%;"></p>
<p>这就用到了大数定律，比如一个罐子中黄豆和绿豆各占多少，不需要把它们全都倒出来，只需要随机抽样，利用抽样结果的比例来确定即可，抽样数量越多，这个比例和真实的比例就越接近，这个比例其实就是期望。</p>
<p><img src="/../img/image-20221011102412119.png" srcset="/img/loading.gif" lazyload alt="image-20221011102412119" style="zoom:50%;"></p>
<p>除了大数定律，数学家还没找出别的能用于归纳的好办法。大数定律需要的变量只有期望和方差，而我们归纳出的其实是一个函数$f(x)$，把$f(x)$变成期望的过程会出很多差错。</p>
<h3 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h3><p>线性模型的VC维度等于数据空间的维度+1，</p>
<p>归纳方法认为，所有事物都会存在一个具有一致性的规律，用函数表达出来就是目标函数G，它是一个我们默认的理想规律，因此分类结果总是正确的。</p>
<p><img src="/../img/image-20221011104454733.png" srcset="/img/loading.gif" lazyload alt="image-20221011104454733" style="zoom:50%;"></p>
<p>虽然G很好，但是我们不知道，只能假设一个$h(x)$去猜：</p>
<p><img src="/../img/image-20221011104604368.png" srcset="/img/loading.gif" lazyload alt="image-20221011104604368" style="zoom:50%;"></p>
<p>把二者做损失函数，把其中损失函数返回值最小的那个$h(x)$挑出来，就是我们学习到的规律，但是现实中我们无法对这两个函数比较，只能用大数定律。</p>
<p><img src="/../img/image-20221011105147709.png" srcset="/img/loading.gif" lazyload alt="image-20221011105147709" style="zoom:50%;"></p>
<p>辛钦大数定律从理论上指出：用算术平均值来近似实际真值是合理的，只要样本容量$N$够大。$\infty$现实中无法到达，而霍尔夫不等式又说明，当阈值$\epsilon$确定之后，我们无法接受的情况发生的概率是有一个上限的，并且不会超过不等式右边那个数字。</p>
<p>也就是说，我们可以接受的情况发生的概率不会小于$1-\delta$，$\delta$越小情况越好，这就给了判断$h(x)$好坏的一个方法，上界$1-\delta$越大$h(x)$越好，这样就把大数定律里极限和无穷的问题规避掉了。</p>
<p>下面的问题就是怎么求大数定律里面的参数$v,\mu$平均数和均值，最简单的方法是把假设函数h在样本D中的函数值的平均值带进去，但实际上这样做毫无意义，因为红色的是全样本的期望，绿色是训练样本中的均值，只求了h的均值和期望，完全没有体现h和G的差距。</p>
<p><img src="/../img/image-20221011115929445.png" srcset="/img/loading.gif" lazyload alt="image-20221011115929445" style="zoom:50%;"></p>
<p>正确的方式是将h和G之间的差别作为考虑因素，把这个差别作为平均值和期望。这样输入就不只有x了，而是x和y组成一组，x是输入数据(图片、语音等数据)，y是分类标签。</p>
<p><img src="/../img/image-20221011121256787.png" srcset="/img/loading.gif" lazyload alt="image-20221011121256787" style="zoom:50%;"></p>
<p>$E_{in}$算出在样本数据里h和G不相等的时候，这个平均值是多少，$E_{out}$算出在全样本中h和G不相等的那个期望值。通过霍尔夫不等式可得，二者差距超过阈值$\epsilon$的概率不会超过$\delta$。因此，只要尽力拉低$E_{in}$，$E_{out}$自然会被降低。</p>
<p>这就得到了PAC（概率近似正确）机器学习理论框架，双重不确定得出的是一个肯定的结论。</p>
<p>横轴是h取值不同时候的值，纵轴是误差量，两条曲线不重合，中间阴影代表它们的差值，而$E_{out}$曲线的最低点就是G，因为只有当假设函数和目标函数相等的时候，$E_{out}$才能完全等于0。数据集也是影响$E_{in}$曲线的一个因素，但是不会对$E_{out}$造成影响，因为$E_{out}$和数据集没有关系，他是整体的全样本的一个数值。</p>
<p><img src="/../img/image-20221011145211273.png" srcset="/img/loading.gif" lazyload alt="image-20221011145211273"></p>
<p>有了这个关系后，就能得到两条曲线的差值了，就是竖条的大小。</p>
<p><img src="/../img/image-20221011145302431.png" srcset="/img/loading.gif" lazyload alt="image-20221011145302431"></p>
<p>再回到上边的不等式，其中的概率代表的是什么的概率呢？</p>
<p><img src="/../img/image-20221011145603637.png" srcset="/img/loading.gif" lazyload alt="image-20221011145603637" style="zoom:50%;"></p>
<p>影响随机变量的有两个因素，一个是h一个是数据集，那么所有情况是指所有h，也就是所有竖线，还是指所有数据集，就又回到随机变量的概念问题上。</p>
<p>随机变量是样本的函数，对于这个问题，它的样本是什么，它的样本就是数据集。大数定律所描述的问题就是抽样很多次数据以后，它们之间的期望差值是趋于一致的。也就是说，这里的随机变量来源就是数据集，而h只是他的一个隐性的参数，是先要把h确定，然后通过D这个数据集的不同，去看不等式中的概率是什么样子的。</p>
<p>总结，这里的所有情况就是在同一个h的情况下，所有不同的数据集代表的情况。大数定律的核心是对抽样分布的理解，本质是总体期望与样本期望的差异。</p>
<p><img src="/../img/image-20221011150551600.png" srcset="/img/loading.gif" lazyload alt="image-20221011150551600" style="zoom:50%;"></p>
<p>以上是确定h，将不同数据集作为变量的情况下，$E_{in}$和$E_{out}$满足的不等式关系，现在要考虑不同的假设函数会带来什么样的影响。</p>
<p><img src="/../img/image-20221011151335713.png" srcset="/img/loading.gif" lazyload alt="image-20221011151335713" style="zoom:50%;"></p>
<p>打个比方，$f(h)$长这样，只有当h等于G的时候，才能保证随意抽样一个数据集，$E_{in}$和$E_{out}$都是等于0的。当$f(h)$越小，大于$\epsilon$的坏情况发生的可能性就越小，取值越大，坏情况发生的可能性就越大。</p>
<p><img src="/../img/image-20221011152438122.png" srcset="/img/loading.gif" lazyload alt="image-20221011152438122"></p>
<p>理想情况是我们知道了h在所有数据集中的f(h)大小，越小效果越好，现实是我们只知道这个上限，并且这个上限还只和样本容量大小有关。</p>
<p>举个例子，在给定数据集的情况下，两条$E_{in}$的最优$h^*$明显$D_N^2$的更好一点，因为离G更近，但是这是在开了上帝视角的情况下看到的，现实中我们只知道它们的上界都一样，哪个好是不知道的。</p>
<p><img src="/../img/image-20221011153329494.png" srcset="/img/loading.gif" lazyload alt="image-20221011153329494"></p>
<p>所以怎么做h之间的横向对比，只能多氪数据，氪得够多就能靠近$E_{out}$了。</p>
<p>其实也不是没有别的办法。</p>
<p>上图中我们的做法是在所有的h里去挑一个最好的$h^*$，现在直接挑一堆出来，如下图，$H_1,H_2,H_3$分别代表三个假设函数的集合，我们来比较这三个集合的好坏，把比较好的$H_2$挑出来，再从$H_2$里挑选一个确定的假设函数。</p>
<blockquote>
<p>令A和B是任意两个集合，若序偶的第一个成员是A的元素，第二个成员是B的元素，所有这样的序偶集合，称为集合A和B的笛卡尔乘积或直积，记做A X B</p>
</blockquote>
<p>这个概率的随机变量只和数据集D有关，和h无关，为了让h也成为影响随机变量的一个因素，把随机变量看作$\Delta$，原来的函数是将h作为一个参数，将数据集D带进去，来得到随机变量，样本空间就是$D$。现在把h也作为变量，拿到括号里面，这样原来的样本就变了，从单纯的$D_N$变成了$<D,h>$，样本空间也就变成了D和H的笛卡尔乘积。</D,h></p>
<p><img src="/../img/image-20221011155835598.png" srcset="/img/loading.gif" lazyload alt="image-20221011155835598" style="zoom:50%;"></p>
<p>再继续进行化简，增加一行，当一个数据集中，一旦出现了一个坏情况，就把这一列记做❌，因此最后一行的坏情况个数肯定是所有行中最多的，由此得到一个上界。</p>
<p><img src="/../img/image-20221011160402535.png" srcset="/img/loading.gif" lazyload alt="image-20221011160402535" style="zoom:50%;"></p>
<p>笛卡尔乘积中H是可以约掉的，把会出现坏情况的数据集集中起来，然后计算每个数据集下每个h的的表现，最后就等于每个h下会出现坏情况的概率的加和。</p>
<p><img src="/../img/image-20221011162209527.png" srcset="/img/loading.gif" lazyload alt="image-20221011162209527" style="zoom: 67%;"></p>
<p>坏的H组，会让很多的数据集都坏，好的H组只会让少量的数据集表现得坏。因此单个h虽然无法比较，但是比较群体H就不一样了。</p>
<p>现在终于让$P_H$和H建立联系了，虽然只是上界。但只是和群体的数量有关，H的总体数量也不一定是有限的。</p>
<p><u>太难证了，不想看了，用到再看吧。</u></p>
<h3 id="拉格朗日乘数法"><a href="#拉格朗日乘数法" class="headerlink" title="拉格朗日乘数法"></a>拉格朗日乘数法</h3><p>凸优化的核心就是拉格朗日，避免过拟合的L1和L2正则化本质也是拉格朗日函数。</p>
<p>以前学的拉格朗日方法总结一下就是帮助我们求最值的，让函数的导数或梯度等于0，得到一堆极值点，然后在里面去挑极大值或极小值。</p>
<p>拉格朗日乘数法加了一个约束空间，函数的变量只能在我们规定的范围内取值，在这个范围内找到极值。打个比方，以前是从A地到B地怎么快怎么走，现在加了一个路费100块，就只能挑选在这个价格范围内合适的路线了。</p>
<p>这样的话，有可能会发生在规定范围内根本没有我们想要的梯度等于0的点，这个时候就可以用拉格朗日乘数法，把原来的函数和约束条件写到一个式子里，对这个式子又可以继续用求导求梯度的方法来求最值了。</p>
<p>这时候原来问题的函数就叫做目标函数，目标函数在约束条件下自变量的取值范围叫做可行域，写成拉格朗日乘数法形式就是下图，其中$\lambda$是一个待定的系数，我们把它叫做拉格朗日乘子。</p>
<p><img src="/../img/image-20221012211851188.png" srcset="/img/loading.gif" lazyload alt="image-20221012211851188" style="zoom:50%;"></p>
<p>注：拉格朗日函数和拉格朗日方程是两个东西。</p>
<h4 id="互补松弛条件"><a href="#互补松弛条件" class="headerlink" title="互补松弛条件"></a>互补松弛条件</h4><p>拉格朗日函数求出的最值和目标函数求出的最值是一样的，两个问题是等价的，都和梯度有关，举个例子：图中圆圈是$f(x,y)$的等高线，中心位置的值最小，越往外值越大，黄线是约束条件的等高线，等高线的法线就是梯度方向，只有在黄线和等高线相切的位置，目标函数的梯度方向和约束条件的梯度方向一致。</p>
<blockquote>
<p>等高线就是等值线，就是函数值取相同值时的那些输入$(x,y)$</p>
</blockquote>
<p>只有方向一致，它们相加才有可能为0，然后再通过拉格朗日乘子去调整两个梯度的长短，让它们两个可以大小相等，方向相反，这样子的话梯度就等于0，极值点就找到了。除了相切的这一点，其他任何地方都没有办法实现这一点。</p>
<p><img src="/../img/image-20221012215652662.png" srcset="/img/loading.gif" lazyload alt="image-20221012215652662" style="zoom:50%;"></p>
<p>以上是只有一个约束条件的情况，如果有多个约束条件，比如下图有5个约束条件，则要找到所有约束条件的梯度的一个线性组合，这个线性组合为原目标函数的梯度的等长反向向量。</p>
<p><img src="/../img/image-20221013111625956.png" srcset="/img/loading.gif" lazyload alt="image-20221013111625956"></p>
<p>反映在图形上就是围成的橙色区域，真正起到约束作用的只有$g_\alpha(x)$和$g_\beta(x)$两条直线，只考虑它们两个的梯度，在线性组合代入拉格朗日函数后应该能正好和$f(x)$抵消，因此$\lambda$是不能等于0的。</p>
<p>再看剩下三个没起到约束条件的直线，区域内的任何点反映在它们那里都是小于0的，因为区域内的每一个点都是五个边界条件同时&lt;0的范围，某条边界上的点对这个约束条件=0，对其他约束条件&lt;0。（线的两侧图形的内外侧分别是大于0和小于0）为了让它们的梯度对整体梯度不要造成影响，所以让它们的$\lambda$都等于0。</p>
<blockquote>
<p>alpha和beta的线性组合要能够和目标函数的梯度共面，而其他的三个方向的向量，引入任何一个，都会破坏共面的条件（三个非零向量共面行列式要为0）。</p>
</blockquote>
<p><img src="/../img/image-20221013122657881.png" srcset="/img/loading.gif" lazyload alt="image-20221013122657881"></p>
<blockquote>
<p>考研数学的话，区域内的点是去掉约束条件，直接求是否有极值点（就是原目标函数的极值点），然后与边界比较。</p>
</blockquote>
<p>上面这个就叫做互补松弛条件，当极值点就在橙色区域内的话，这个时候就相当于是所有的约束条件都是松弛的，直接就是目标函数的极值点。</p>
<p><img src="/../img/image-20221013123045477.png" srcset="/img/loading.gif" lazyload alt="image-20221013123045477" style="zoom:50%;"></p>
<p>虽然但是，这样求出的极值点或者鞍点可能并不是最值点，但如果是凸问题的话，就可以认为它就是最值点了。</p>
<blockquote>
<p>凸问题要求目标函数和可行域都是凸的，第一个图里那个曲里拐弯的线明显不是凸的。</p>
</blockquote>
<p>因此，在一个凸函数上求最小值或者在一个凹函数上求最大值都是简单的，所以遇到凹函数一般都会在前面加个负号变成凸函数，然后再求最小值。举个例子，熵函数就是个凹函数，因此我们经常去掉它的负号，把它变成一个凸函数。</p>
<p><img src="/../img/image-20221013124003283.png" srcset="/img/loading.gif" lazyload alt="image-20221013124003283" style="zoom: 25%;"></p>
<blockquote>
<p>为什么我们这么喜欢用墒函数就是因为很多熵本身就是一个凸问题，求最值非常容易，比如交叉熵和最大熵。</p>
<p>但是放到神经网络里可能就不一定了。</p>
</blockquote>
<p>那么非凸问题的最值怎么求解？看拉格朗日的对偶问题。</p>
<p>先把x当作常数，$\lambda$和$\mu$这两个拉格朗日乘子当作变量，去求拉格朗日函数的最大值。然后再把x当作变量，去求拉格朗日函数的最小值。</p>
<p>因为约束条件都是小于等于零的，而且乘子是非负的，要求最小，那就要减掉一个最大的数。</p>
<p>说的简单点是这样，但其实不是先后求，是循环的过程。</p>
<h4 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h4><p><img src="/../img/image-20221013175308351.png" srcset="/img/loading.gif" lazyload alt="image-20221013175308351"></p>
<p>当x不在可行域内的时候，不满足约束条件，所以后两项是大于0的，要求拉格朗日函数的最大值，后两项可以达到无穷。</p>
<p>当x在可行域内，满足约束条件，后两项是小于0的，为了使拉格朗日函数最大，就要让乘子等于0，于是得到的函数的最大值形式如右边所示，其实是和最上面的式子是等价的，令$f_i(x)$小于0得到一个凸问题，约束条件等于0保证函数取得最大值。</p>
<p>拉格朗日对偶函数是拉格朗日函数在x处取得的下界，而拉格朗日对偶问题是拉格朗日对偶函数如何取最大值的问题。</p>
<p><img src="/../img/image-20221014000940530.png" srcset="/img/loading.gif" lazyload alt="image-20221014000940530"></p>
<p>对偶函数相当于把原函数中先求x再求乘子的操作颠倒了一下，先把乘子看作常数，求变量x，再在对偶函数上求参数。</p>
<p>看求函数最小值的部分，相当于是把这部分的x当作变量求梯度，梯度等于0的时候就是函数的最小值。</p>
<p><img src="/../img/image-20221014002430811.png" srcset="/img/loading.gif" lazyload alt="image-20221014002430811" style="zoom:50%;"></p>
<p>不论原问题是什么，只要变成对偶问题，就都是凸问题。</p>
<h4 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h4><p>凸集简单地说，可以想象成一种形状，其中任何连接2点的线都不会超出凸集。</p>
<p><img src="/../img/image-20221012124524306.png" srcset="/img/loading.gif" lazyload alt="image-20221012124524306" style="zoom:50%;"></p>
<p>用数学表达一个凸集 C ：</p>
<p><img src="/../img/image-20221014002740515.png" srcset="/img/loading.gif" lazyload alt="image-20221014002740515" style="zoom:50%;"></p>
<p>去掉$\theta$的约束条件后，线段就变成了一条直线，如果是多维的话，那线段就成了一个超平面，这时候的C就是仿射集，仿射集一定是一个凸集。</p>
<p><img src="/../img/image-20221014003039652.png" srcset="/img/loading.gif" lazyload alt="image-20221014003039652" style="zoom:50%;"></p>
<p>这里把上图的线性关系写成了这样的形式，我们看着就更熟悉了。</p>
<blockquote>
<p>如果$C_1,C_2,C_3$都是仿射集，那么三者相交仍然是凸集。</p>
</blockquote>
<p>如果规定一个直线，把某个空间切割了一半，那一半的空间也是凸集，半空间相交的部分也是凸集。</p>
<p><img src="/../img/image-20221014003521184.png" srcset="/img/loading.gif" lazyload alt="image-20221014003521184" style="zoom:50%;"></p>
<p>epigraph 是在函数中或其上的所有点，$f(x)$的epigraph如下：</p>
<p><img src="/../img/image-20221012130232456.png" srcset="/img/loading.gif" lazyload alt="image-20221012130232456" style="zoom:50%;"></p>
<p>如果一个函数 f 的 epigraph 是凸集（如左下方绿色图所示)），则称该函数为凸函数。这意味着在这个 <em>epigraph</em> 上画的每个两点间线段<strong>总是</strong>等于或高于函数图。</p>
<p><img src="/../img/image-20221012135304781.png" srcset="/img/loading.gif" lazyload alt="image-20221012135304781" style="zoom:50%;"></p>
<p>凸性检验：在神经网络中，大多数的成本函数是非凸的。因此，如果想优化学习器的话，必须先测试一下函数的凸性。</p>
<blockquote>
<p>只有一个样本时叫损失函数，对于m个样本，叫成本函数。</p>
</blockquote>
<p>如果函数 f 的二阶导数$f’’(x)$大于或等于0，则称该函数为凸函数。如果$ -f(x) $是一个凸函数，那么函数 f 称为凹函数。</p>
<p>凸函数在神经网络里的作用：</p>
<p>一个凸函数意味着没有局部极小值，只有全局极小值。因此，梯度下降法将会收敛到全局极小值。</p>
<p>如果不是凸函数，可以看到梯度下降法将停止在局部极小值，而不是收敛到全局极小值。因为这一点的梯度已经到零了，而且还是附近区域的极小值，解决这个问题的一个方法是使用动量。</p>
<p><img src="/../img/image-20221012145230982.png" srcset="/img/loading.gif" lazyload alt="image-20221012145230982" style="zoom:50%;"></p>
<p>原问题本身和约束条件都是凸函数组成的问题才是凸优化问题。</p>
<p>因此，在对偶问题中，将x看作一个常数的话，$g(\lambda,v)$就相当于是一条直线，直线既是凸问题，又是凹问题，而约束条件$\lambda&gt;0$相当于是一个半空间，半空间是仿射集中的一个凸集，因此，对偶问题本身和其约束条件都是凸的，所以它一定是一个凸优化问题。</p>
<p><img src="/../img/image-20221019163620244.png" srcset="/img/loading.gif" lazyload alt="image-20221019163620244" style="zoom:50%;"></p>
<p>因此，不论是什么问题，只要化成对偶问题，就都是凸问题。但是这一切的前提条件是原问题和对偶问题必须是等价的。</p>
<p><img src="/../img/image-20221019164321590.png" srcset="/img/loading.gif" lazyload alt="image-20221019164321590" style="zoom:50%;"></p>
<p>将原问题和对偶问题做第一步变换后，一个可以看成是x的函数，另一个可以看成是$\lambda,v$的函数，并且左边是恒大于等于右边的，由此可以得出的结论是原问题的解一定是大于对偶问题的。我们的期望是二者相等，因此就引入了KKT条件，只要一个问题是强对偶的，那么就一定符合KKT条件，反过来不一定。这有点像用求导的方式求最值，导数等于0的点不一定就是最值点，KKT条件也一样，找到了最优值的点，这个点一定是满足KKT条件的，满足KKT条件的点，不一定是最优值的点，</p>
<p>但是当问题为凸规划时，在约束规范下，KKT条件可以直接作为全局最优解的充要条件。</p>
<p><img src="/../img/image-20221019170717481.png" srcset="/img/loading.gif" lazyload alt="image-20221019170717481"></p>
<p>前两个条件为原问题的约束条件，三四为对偶问题的约束条件，互补松弛条件参考右下两幅图，上面是$\lambda&gt;0$，最优点位于边界上，条件是紧致的，下面的图代表$\lambda=0$，最优点位于范围内，条件是松弛的。</p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。</p>
<p>集成学习在各个规模的数据集上都有很好的策略。</p>
<ul>
<li>数据集大：划分成多个小数据集，学习多个模型进行组合。</li>
<li>数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合。</li>
</ul>
<p>这篇博客介绍一下集成学习的几类：Bagging，Boosting以及Stacking。</p>
<p><strong>Bootstrap</strong></p>
<p>Bootstrap是一类非参数Monte Carlo方法，实质是对观测信息进行再抽样，进而对总体的分布特性进行统计推断。</p>
<p>步骤：</p>
<ol>
<li><p>使用这个方法的前提是每个抽样的样本集大小和原数据集大小一样，并且数据集大小要非常大，而不是像本文这样设置成固定的100个。</p>
</li>
<li><p>在已有的样本通过重抽样抽取一定数量（比如100）的新样本，重抽样（Re-sample）的意思就是有放回的抽取，即一个数据有可以被重复抽取超过一次。</p>
</li>
<li><p>根据我们抽取的样本，计算我们需要估计的统计量。</p>
</li>
<li><p>重复上述步骤n次（一般是n&gt;1000次）。通过n次（假设n=1000），我们就可以得到1000个$α_i$计算出统计量的置信区间。</p>
</li>
</ol>
<h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>在Bagging方法中，利用bootstrap方法从整体数据集中重抽样出得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。</p>
<p>例如<strong>随机森林（Random Forest）</strong>就属于Bagging。</p>
<p>随机森林简单地来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。</p>
<p>在我们学习每一棵决策树的时候就需要用到Bootstrap方法。在随机森林中，有两个随机采样的过程：对输入数据的数量与数据的特征都进行采样。对于输入数据采用重采样，这样在训练的时候每一棵树都不是全部的样本，相对而言不容易出现overfitting；接着进行特征采样，从M个特征中选择出m个，进行决策树的学习。</p>
<p>预测的时候，随机森林中的每一棵树都对输入进行预测，最后进行投票，哪个类别多，输入样本就属于哪个类别。这就相当于前面说的，每一个分类器（每一棵树）都比较弱，但组合到一起（投票）就比较强了。</p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p>提升方法（Boosting）是一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合为一个强分类器。Boosting中有代表性的是<strong>AdaBoost（Adaptive boosting）算法</strong>：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。</p>
<p><strong>GBDT（Gradient Boost Decision Tree)</strong>就是一种Boosting的方法，与AdaBoost不同，GBDT每一次的计算是为了减少上一次的残差，GBDT在残差减少（负梯度）的方向上建立一个新的模型。</p>
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>Stacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。</p>
<p>如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，称之为Tier 1分类器（可以采用交叉验证的方式学习），然后将输出用于训练Tier 2 分类器。</p>
<p><img src="/../img/v2-bc3b2612dd0ff778c53db4165bc35449_r.jpg" srcset="/img/loading.gif" lazyload alt="img" style="zoom:75%;"></p>
<h3 id="Bagging与Boosting"><a href="#Bagging与Boosting" class="headerlink" title="Bagging与Boosting"></a>Bagging与Boosting</h3><p>Bagging和Boosting采用的都是<strong>采样-学习-组合</strong>的方式，但在细节上有一些不同，如</p>
<ul>
<li><strong>Bagging中每个训练集互不相关，也就是每个基分类器互不相关，而Boosting中训练集要在上一轮的结果上进行调整，也使得其不能并行计算</strong></li>
<li><strong>Bagging中预测函数是均匀平等的，但在Boosting中预测函数是加权的</strong></li>
</ul>
<p>在算法学习的时候，通常在bias和variance之间要有一个权衡。bias与variance的关系如下图，因而模型要想达到最优的效果，必须要兼顾bias和variance，也就是要采取策略使得两者比较平衡。</p>
<p><img src="/../img/v2-9000c0e50e1a97d0d12e85dc93affa5f_1440w.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>从算法来看，Bagging关注的是多个基模型的投票组合，保证了模型的稳定，因而每一个基模型就要相对复杂一些以降低偏差（比如每一棵决策树都很深）；而Boosting采用的策略是在每一次学习中都减少上一轮的偏差，因而在保证了偏差的基础上就要将每一个基分类器简化使得方差更小。</p>
<h2 id="生成对抗网络GAN"><a href="#生成对抗网络GAN" class="headerlink" title="生成对抗网络GAN"></a>生成对抗网络GAN</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>GAN包含有两个模型，生成模型和判别模型。生成模型的任务是生成看起来自然真实的、和原始数据相似的实例。判别模型的任务是判断给定的实例看起来是自然真实的还是人为伪造的（真实实例来源于数据集，伪造实例来源于生成模型）。</p>
<p>生成模型像“一个造假团伙，试图生产和使用假币”，而判别模型像“检测假币的警察”。生成器试图欺骗判别器，判别器则努力不被生成器欺骗。模型经过交替优化训练，两种模型都能得到提升，但最终我们要得到的是效果提升到很高很好的生成模型（造假团伙），这个生成模型（造假团伙）所生成的产品能达到真假难分的地步。</p>
<p><img src="/../img/v2-d36c35e3bb9ba1aac119304b225c0cda_720w.jpg" srcset="/img/loading.gif" lazyload alt="生成对抗网络（GAN）"></p>
<p>假设我们有两个网络，G（Generator）和D（Discriminator）。G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。D是一个判别网络，判别一张图片是不是“真实的”。它的输入x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。</p>
<h3 id="GAN模型优化训练"><a href="#GAN模型优化训练" class="headerlink" title="GAN模型优化训练"></a>GAN模型优化训练</h3><p>在训练过程中，生成网络的目标就是尽量生成真实的图片去欺骗判别网络D。而网络D的目标就是尽量把网络G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。这个博弈过程具体是怎么样的呢？</p>
<p>先了解下<strong>纳什均衡</strong>，纳什均衡是指博弈中这样的局面，对于每个参与者来说，只要其他人不改变策略，他就无法改善自己的状况。最经典的例子就是囚徒困境，两人都抗拒各判1年，都坦白各判3年，一人坦白一人抗拒，坦白的人直接放，抗拒的人判5年。</p>
<p>最终两人都会选择坦白，首先，纳什均衡的基础点是消息不互通，纳什均衡也不意味着博弈双方达到了一个整体的最优状态（都抗拒各判1年），但严格劣势策略不可能成为最佳对策（一方坦白一方抗拒）。</p>
<p>对应的，对于GAN，情况就是生成模型 G 恢复了训练数据的分布（造出了和真实数据一模一样的样本），判别模型再也判别不出来结果，准确率为 50%，约等于乱猜。这时双方网路都得到利益最大化，不再改变自己的策略，也就是不再更新自己的权重。</p>
<p>GAN模型的目标函数如下：</p>
<p><img src="/../img/v2-64263acb7eeb012f7fa7e80446d4dac3_1440w.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>它的含义就是：最大化D的能力的前提下，最小化D对G的判断能力。</p>
<p>在这里，要训练网络D能最大概率地分对训练样本的标签（即最大化$log D(x)$和$ log(1−D(G(z)))$ ），训练网络G最小化$log(1 – D(G(z)))$，即最大化D的损失。训练过程中固定一方，更新另一个网络的参数，交替迭代，使得对方的错误最大化，最终，G 能估测出样本数据的分布，也就是生成的样本更加的真实。</p>
<blockquote>
<p>或者我们可以直接理解G网络的想要最小化的loss是$log(1−D(G(z))$，而D想要最小化的loss是$−(log(D(x))+log(1−D(G(z)))$</p>
</blockquote>
<p>然后从式子中解释对抗，我们知道G网络的训练是希望D(G(z))趋近于1，也就是正类，这样G的loss就会最小。而D网络的训练就是一个二分类，目标是分清楚真实数据和生成数据，也就是希望真实数据的D输出趋近于1，而生成数据的输出即D(G(z))趋近于0，或是负类。这里就是体现了对抗的思想。</p>
<p>然后，这样对抗训练之后，效果可能有几个过程，原论文画出的图如下：</p>
<p><img src="/../img/v2-aab535a56ee0fabaa3d52998d1baf616_1440w.webp" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>黑色的线表示数据x的实际分布，绿色的线表示数据的生成分布，蓝色的虚线表示生成的数据对应在判别器中的分布效果。</p>
<p>图a，D刚开始训练，本身分类的能力还很有限，有波动，但是初步区分实际数据和生成数据还是可以的。图b，D训练得比较好了，可以很明显的区分出生成数据。然后对于图c，绿色的线与黑色的线的偏移，蓝色的线下降了，也就是生成数据的概率下降了。那么，由于绿色的线的目标是提升概率，因此就会往蓝色线高的方向移动。那么随着训练的持续，由于G网络的提升，G也反过来影响D的分布。假设固定G网络不动，训练D，那么训练到最优，Dg∗(x)=pdata(x)/(pdata(x)+pg(x))。因此，随着pg(x)趋近于pdata(x),Dg∗(x)会趋近于0.5，也就是到图d。而我们的目标就是希望绿色的线能够趋近于黑色的线，也就是让生成的数据分布与实际分布相同。图d符合我们最终想要的训练结果。到这里，G网络和D网络就处于纳什均衡状态，无法再进一步更新了。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>机器学习的两个核心问题——优化和正则化，正则化就是减少机器学习过拟合的过程，最常见的做法就是对模型的权重进行L1和L2正则化，Dropout算法就可以理解为一种L2正则化。</p>
<p>凡是可以减少泛化误差，而不是减少训练误差的方法都可以称作是正则化，也就是说，凡是可以减少过拟合的方法，都可以称作是正则化。在神经网络里，就是针对权重W的正则化，因为W的取值情况直接决定了模型曲线是什么样子的，而b的取值不会改变模型曲线的样子，只会改变模型的平移情况。因此，要想减少过拟合，重点还是W。</p>
<p>L1和L2正则化就是L1和L2范数，范数就是把空间中两点的距离这个概念进行了扩充。比如说权重W可以理解为是一个高维的向量，也可以理解为是在高维空间中的一个点，这个点到原点的距离如果是欧氏距离的话，就是L2范数，如果把L2范数相同的点连起来的话，组成的就是一个圆。</p>
<p><img src="/../img/image-20221107153713226.png" srcset="/img/loading.gif" lazyload alt="image-20221107153713226" style="zoom:50%;"></p>
<p>如果直接对距离取绝对值就是L1范数，把L1范数相同的点连起来的话，就是一个正方形。</p>
<p><img src="/../img/image-20221107153939852.png" srcset="/img/loading.gif" lazyload alt="image-20221107153939852" style="zoom:50%;"></p>
<p>这个L几其实是可以任意取值的，其中只有在LP范数中的这个P取值是大于等于1的时候，构成的集合才是凸集。</p>
<p><img src="/../img/image-20221107154111266.png" srcset="/img/loading.gif" lazyload alt="image-20221107154111266" style="zoom:50%;"></p>
<p>因此，L1和L2利用的就是它们的凸集特性。</p>
<p>在神经网络中，出发点不同，也就是初始值不同的情况下，即使最后都得到了使损失函数最小的一组W和b，它们也有可能各不相同。假如初始值很大，就会得到一组绝对值很大的参数，假如初始值很小。就会得到一组很小的参数。</p>
<p>在大参数神经网络中，噪声和误差也是同时被放大的，假如一只猫输入，噪声放大后可能得到的就是一只狗，因此我们要想办法，让参数取值不要那么大。</p>
<p>我们可以给参数一个可行域范围，参数超出这个可行域之后，即使损失函数取得再小，我们也不去考虑。</p>
<p>在一个限定条件下求最值，这就是拉格朗日乘数法最擅长的事。</p>
<h3 id="拉格朗日乘数法-1"><a href="#拉格朗日乘数法-1" class="headerlink" title="拉格朗日乘数法"></a>拉格朗日乘数法</h3><p>J是损失函数，x是训练集的数据，一个常数，因此可以不用考虑。$||W_1||-C$限制的就是W对应的高维空间的点到原点的L1距离，L2范数同理。红色部分是J对应的等高线，绿色的框代表着可行域范围，对应的交点就是我们想要的参数。</p>
<p><img src="/../img/image-20221107160045455.png" srcset="/img/loading.gif" lazyload alt="image-20221107160045455" style="zoom:50%;"></p>
<p>单独对L2范数来举例，最后得到红框里L2正则化的一般表达式。而红色和绿色的函数用拉格朗日求最值的话，因为对W的梯度其实是相等的，所以二者其实是等价的，</p>
<p>红色函数里没有把圆的半径C写出来，也就代表了，在这个函数里，它包含了任意半径的范围。但没有了半径，如何确定唯一的极值点，就要通过乘子$\lambda$。这个乘子的作用其实就是调节两个梯度的大小，红色是损失函数J的梯度，绿色是约束条件的梯度，二者应该是方向相反的，因为只有方向相反、大小相等，才能做到二者相加等于0，$\lambda$起到的就是调节二者梯度大小的作用。</p>
<p>确定了$\lambda$，就相当于确定了点，就相当于确定了半径，反过来也一样。因此绿色函数的意义就是，只要C确定了，$\lambda$就可以通过它计算出来，所以说噢，$\lambda$不是一个超参数，不需要我们人为确定，只有C需要人为确定。</p>
<p>在红色函数中，就相当于不再管C了，而把$\lambda$作为一个超参数。</p>
<p><img src="/../img/image-20221107162221997.png" srcset="/img/loading.gif" lazyload alt="image-20221107162221997" style="zoom:50%;"></p>
<p>再去看一眼L2和L1正则化对比，可以看到L2确定的极值点基本都不在坐标轴上，在坐标轴上的W点意味着特征选择。打个比方，输入一张照片，想要判断是猫还是狗，有两个特征需要判断，分别是胡子和耳朵的长短，在L2正则化得到的极值点，既需要看胡子的特征，也需要看耳朵的特征。而在L1正则化中，可以通过调参数$\lambda$，找到一个坐标轴上的极值点，来把两个特征给剥离开，只要去看耳朵的特征，就可以判断是狗还是猫，这就是L1正则的稀疏性和选择特征性。稀疏性就是在某个坐标轴上有值，其他项都是0，来让他的某些特征起作用，而不是所有特征都起作用。也就是把特征之间的关系去耦合了，这个去耦合的过程恰好就是减少过拟合的过程。</p>
<p>而L2正则化就是把W这个权重的绝对值缩小了，L1正则化带来了稀疏性，因此有的论文只用一个，有的两个一起用。</p>
<p>但正则化可能也带来了问题，假如说损失函数的值距离原点特别远，强行约束权重W来让W变得很小，就会离真正的最值非常远，但其实我们找到的最小点在图像上对应的不是一个点，而是一个路径，可以理解为是W的一个线性函数，给他加一个系数让他任意缩放（对应前面讲的相同的损失函数值可能对应多组W），所以是一条直线，这个路径上的任意一点就代表了这个最值的点。正则化带来的偏差并不是到原点的距离，而是到这条路径的距离， 所以这个偏差并没有我们想的那么大，也就不存在当这个绿色框框越小，得到的点就越偏离损失函数最小值的情况。</p>
<p><img src="/../img/image-20221107171906420.png" srcset="/img/loading.gif" lazyload alt="image-20221107171906420"></p>
<p>这也就是能用正则化就用正则化的原因，好处非常大，坏处可以忽略不计。</p>
<h3 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h3>
              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/">#机器学习基础理论</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="深度强化学习">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">深度强化学习</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/09/21/vue3-x-%E5%8A%A8%E7%94%BB%E7%AF%87/" title="vue3.x 动画篇">
                        <span class="hidden-mobile">vue3.x 动画篇</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="vcomment" class="comment"></div> 
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
  <script>
    var notify = '' == true ? true : false;
    var verify = '' == true ? true : false;
      window.onload = function() {
          new Valine({
              el: '#vcomment',
              app_id: "AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz",
              app_key: "kMsGLN3hzkQJuLrmqQBgquFF",
              placeholder: "说点什么",
              avatar:"retro",
              visitor: true       

          });
      }
  </script>

 
  <noscript>Please enable JavaScript to view the comments</noscript>



  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  




  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  
    
  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  
      <script>
        MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>

  <script defer src="/js/leancloud.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/yinghua.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiantiao.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiaoxingxing.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
