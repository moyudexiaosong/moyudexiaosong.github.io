

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.jpg">
  <link rel="icon" href="/img/icon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="神经网络数据分割举个例子，判断北京和郑州的房价，定义房价&#x3D;价格&#x2F;面积&#x3D;x&#x2F;y，高于0.69的属于北京，低于0.69的属于郑州。并且写成f(x,y)&#x3D;x-0.69y的形式，当f(x,y)&gt;0时，判定是北京，小于0判定为郑州。 更现实的情况是，这个能正确区分北京和郑州的阈值并不会恰好是0，打个比方是10，我们仍然可以对函数进行校准，需要写成f(x,y)&#x3D;x-0.69y-10的形式。 在这个式子">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络">
<meta property="og:url" content="http://example.com/2022/09/28/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="摸鱼之家">
<meta property="og:description" content="神经网络数据分割举个例子，判断北京和郑州的房价，定义房价&#x3D;价格&#x2F;面积&#x3D;x&#x2F;y，高于0.69的属于北京，低于0.69的属于郑州。并且写成f(x,y)&#x3D;x-0.69y的形式，当f(x,y)&gt;0时，判定是北京，小于0判定为郑州。 更现实的情况是，这个能正确区分北京和郑州的阈值并不会恰好是0，打个比方是10，我们仍然可以对函数进行校准，需要写成f(x,y)&#x3D;x-0.69y-10的形式。 在这个式子">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/photo14.jpeg">
<meta property="article:published_time" content="2022-09-28T10:26:43.000Z">
<meta property="article:modified_time" content="2022-10-06T11:17:38.600Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="book">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/photo14.jpeg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>神经网络 - 摸鱼之家</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.1","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":false},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz","app_key":"kMsGLN3hzkQJuLrmqQBgquFF","server_url":"https://awhbuajs.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>快乐老家</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                主页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                档案馆
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                目录
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/photo14.jpeg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="神经网络"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-09-28 18:26" pubdate>
          September 28, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          12k words
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">神经网络</h1>
            
              <p class="note note-info">
                
                  
                    Last updated on a few seconds ago
                  
                
              </p>
            
            <div class="markdown-body">
              
              <hr>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="数据分割"><a href="#数据分割" class="headerlink" title="数据分割"></a>数据分割</h2><p>举个例子，判断北京和郑州的房价，定义房价=价格/面积=x/y，高于0.69的属于北京，低于0.69的属于郑州。并且写成<script type="math/tex">f(x,y)=x-0.69y</script>的形式，当f(x,y)&gt;0时，判定是北京，小于0判定为郑州。</p>
<p>更现实的情况是，这个能正确区分北京和郑州的阈值并不会恰好是0，打个比方是10，我们仍然可以对函数进行校准，需要写成<script type="math/tex">f(x,y)=x-0.69y-10</script>的形式。</p>
<p>在这个式子里，x的量纲是1，y的量纲是-0.69，每份y贡献-0.69，每份x贡献1，贡献度正好是x,y的系数。依此类推，当出现更多影响因素时，<script type="math/tex">f(价格，面积，距离，楼层，环境，......)</script>，每个维度都可以看作一个变量，最终决定这个房子值不值得购买。</p>
<p>写成变量的形式：$f(x_1,x_2,x_3,x_4,x_5,……)=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5+…+b=y$$，其中$w_i$决定$x_i$对y的贡献程度，也叫权重系数，b决定的事阈值的偏移程度，也叫偏置系数。</p>
<p>在坐标图上看样本点，一条分界线$0.0018x_1-0.0012x_2-1.78=0$将数据分割成上下两部分，在确定分界线后，数据划分也就确定了。所以我们的目标就是找到这条分界线。</p>
<p><img src="/../img/image-20221002002558453.png" srcset="/img/loading.gif" lazyload alt="image-20221002002558453"></p>
<p>神经网络分为两部分：</p>
<ol>
<li>决策/预测：在分界线确定之后，对于任意一个新数据，我们都可以根据它在分界线上的位置来判断它属于哪一类。但是机器不会看图，所以就会根据将x代入y后大于或者小于0来分类。f(x)&gt;0则在直线上方，反之在下方。</li>
<li>训练/学习：这个过程就是如何找到那条分界线，分界线并不唯一，并且事实上根本不存在一条完美的分界线，我们只能从中选择一条还不错的。所以最重要的是找到判断分界线好坏的标准。</li>
</ol>
<h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>分界线标准：在已有的数据样本点上，选出一条能把所有数据完美分割开的分割线是否可行呢？不行，这样只能做到对已知的数据最好，无法保证对未知的数据也是最好的。所以我们选择了一个判断标准之后，这里面就应该包含对未来数据可能情况的一种预测。这在数学逻辑上无法做到，因此引入公设（一种设定，可以有很多种这样的设定）。</p>
<p>公设一：</p>
<p>算出离直线最近的点到直线的距离，这个距离越大越好，最小间隔函数如下：</p>
<script type="math/tex; mode=display">
M(W)=\min_{i=1,2,...,N}\frac{1}{||W||_2}|f(X^{(i)}|</script><p>我们要找的是使最小间隔最大的那一组系数<script type="math/tex">W=w_1,w_2,...w_N,b</script>，在不考虑算力的情况下，计算机可以自己完成这样的选择。</p>
<p>公设二：</p>
<p><img src="/../img/image-20221002005643117.png" srcset="/img/loading.gif" lazyload alt="image-20221002005643117"></p>
<p>等式左边的意思是在给定系数的条件下，取到该数据点的概率是多少，把数据点输入f(x)，通过sigmoid激活函数后，f(x)值越大，输出越接近于1，左边的概率值也就越接近于1。因为f(x)&gt;0时分类为北京，f(x)<0时分类为郑州，所以当f(x)>0大于的越多，该地属于北京的概率就越大，而-f(x)&gt;0大于的越多，就代表该地属于郑州的概率就越大。</0时分类为郑州，所以当f(x)></p>
<p>因此得到公设：让所有的数据点对应的似然值同时最大的时候最好。因为在这里变动的不是随机变量X，而是条件W，所以不叫概率值，叫似然值。</p>
<blockquote>
<p>概率是随机变量的概率，似然是概率分布函数参数的似然。</p>
<p>有时我们不能在实验前获取随机变量概率分布函数中的具体参数值，这个时候我们通过大量试验收集样本数据，统计样本结果，来推测<strong>参数取值的可能性</strong>，此时这个<strong>可能性大小就是似然值</strong>，这个推测参数取值最大可能性的过程也就是后面我们要讲解的<strong>最大似然估计</strong>。</p>
</blockquote>
<p>因为是所有概率值同时最大，所以是一个相乘的关系，列出如下表达式，并用log把连乘变成连加。</p>
<p><img src="/../img/image-20221002011344066.png" srcset="/img/loading.gif" lazyload alt="image-20221002011344066"></p>
<p>公设三：</p>
<p>假设存在一个可以完美将数据分类的T(x)函数，用图像表示：</p>
<script type="math/tex; mode=display">
T(X)=\begin{cases} 1:X\in\{北京\}\\  0:X\in\{长治\}， \end{cases}</script><p><img src="/../img/image-20221002163443188.png" srcset="/img/loading.gif" lazyload alt="image-20221002163443188"></p>
<p>现实中会有噪声存在，对T叠加一个噪声后，就不再是非0即1的情况了，</p>
<script type="math/tex; mode=display">
T(X)+\epsilon=\begin{cases} 1+\epsilon:X\in\{北京\}\\  0+\epsilon:X\in\{长治\}， \end{cases}</script><p>这时候sigmoid函数又可以发挥作用了,</p>
<script type="math/tex; mode=display">
T(X)+\epsilon=sigmoid(f(X))</script><p><img src="/../img/image-20221002164927433.png" srcset="/img/loading.gif" lazyload alt="image-20221002164927433"></p>
<p> 对f(X)取值sigmoid后与T(X)的偏差，就是定义的误差项$\epsilon$，我们希望这个误差越小越好，也就是说，方差越小越好。</p>
<p><img src="/../img/image-20221002165207081.png" srcset="/img/loading.gif" lazyload alt="image-20221002165207081"></p>
<p>还有更多的公设，在数学上都可以证明。</p>
<h3 id="数学知识"><a href="#数学知识" class="headerlink" title="数学知识"></a>数学知识</h3><p>sigmoid函数：<img src="/../img/image-20221002174627021.png" srcset="/img/loading.gif" lazyload alt="image-20221002174627021"></p>
<p> 做指数变形，把$\hat{y}$从实数域投射到$(0,\infty)$ </p>
<p><img src="/../img/image-20221002174851249.png" srcset="/img/loading.gif" lazyload alt="image-20221002174851249"></p>
<p>softmax函数：<img src="/../img/image-20221002175019842.png" srcset="/img/loading.gif" lazyload alt="image-20221002175019842"></p>
<p>选择哪一种判断标准，也就是选择哪一种公设，在机器学习上叫做策略。<strong>算法（反向传播——梯度下降法）、策略（输出层）加上模型（隐藏层），就是机器学习的三大要素。</strong></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>线性方程在坐标图上的表现就是一条直线，增加多次项就相当于增加曲度，只要增加足够的多次项，无论多么奇怪的形状都能够表示出来，举个例子：</p>
<p><img src="/../img/image-20221002165841885.png" srcset="/img/loading.gif" lazyload alt="image-20221002165841885"></p>
<p>可见，随着次数的增加，曲线会越来越复杂，模型的复杂程度也会提高。 </p>
<p>虽然我们讨论的是分类问题，但是在曲线的拟合方面，和回归在模型上并没有什么本质的区别。当我们拟合出一条回归曲线后，任意给出一个$x_1$的值，我们都可以预测出它对应的$x_2$的数值。这时候完成的就是预测而不是分类。</p>
<p> 既然直线这么不好用，我们就提高模型的复杂程度来分类，神经网络做的就是这样的工作。</p>
<p><img src="/../img/image-20221002170454030.png" srcset="/img/loading.gif" lazyload alt="image-20221002170454030"></p>
<p>神经网络最左边是输入层，对应的是数据的维度，神经网络里任意一个神经元，都会和上一层，下一层所有的节点相连，所以叫全连接神经网络。</p>
<p><img src="/../img/image-20221002170733234.png" srcset="/img/loading.gif" lazyload alt="image-20221002170733234"></p>
<p>看个最简单的，每一条线就代表一个权重，连起来就是一个线性方程。</p>
<p><img src="/../img/image-20221002170823104.png" srcset="/img/loading.gif" lazyload alt="image-20221002170823104"></p>
<p> 然后它的结果z要经过激活函数后才会传给下一层，激活过程用神经元统一表示。</p>
<p>以二维输入向量举例，这个过程就是这样的：</p>
<p><img src="/../img/image-20221002171950200.png" srcset="/img/loading.gif" lazyload alt="image-20221002171950200"></p>
<p><img src="/../img/image-20221002171058240.png" srcset="/img/loading.gif" lazyload alt="image-20221002171058240"></p>
<p>在经过激活函数前：</p>
<p><img src="/../img/image-20221002171434650.png" srcset="/img/loading.gif" lazyload alt="image-20221002171434650"></p>
<p>经过激活函数后：</p>
<p><img src="/../img/image-20221002171111001.png" srcset="/img/loading.gif" lazyload alt="image-20221002171111001"></p>
<p>再加一层神经元：上一层的输出是下一层的输入：</p>
<p><img src="/../img/image-20221002171809601.png" srcset="/img/loading.gif" lazyload alt="image-20221002171809601"></p>
<p><img src="/../img/image-20221002171623928.png" srcset="/img/loading.gif" lazyload alt="image-20221002171623928"></p>
<p>给第一层多加一个神经元：，每层的输出就从单值变成向量了：</p>
<p><img src="/../img/image-20221002171914583.png" srcset="/img/loading.gif" lazyload alt="image-20221002171914583"></p>
<p><img src="/../img/image-20221002172231769.png" srcset="/img/loading.gif" lazyload alt="image-20221002172231769"></p>
<p> 依此类推，神经元数量越多，模型越复杂：</p>
<p><img src="/../img/image-20221002173141748.png" srcset="/img/loading.gif" lazyload alt="image-20221002173141748"></p>
<p>只要增加的够多，什么形状都能编出来，这就叫 万能逼近定理。实现这个定理最关键的是一个非线性的激活函数，否则再怎么增加神经元得到的也只是一个多维的平面。</p>
<blockquote>
<p>总之，神经网络的复杂性来自于激活函数。</p>
</blockquote>
<p>不同的激活函数会拟合出不同的形状，它们可以是这些：</p>
<p><img src="/../img/image-20221002173432075.png" srcset="/img/loading.gif" lazyload alt="image-20221002173432075"></p>
<p>原理：举个例子，加入输入只是一个二维向量，经过矩阵运算后，相当于把它从二维升到了三维，原本不可分的数据，现在即使用更简单的模型也可以把它们分开。</p>
<p><img src="/../img/image-20221002174123794.png" srcset="/img/loading.gif" lazyload alt="image-20221002174123794"></p>
<p>所以我们可以把中间的层看做是对输入的一个升维操作，有多少神经元就会把数据升到多少维。只要维度够高，一定能找到一个超平面，完成对数据的划分。因此，最后一层神经元只需要考虑策略的问题就可以了，该层是否需要激活函数由策略决定。</p>
<p>但如果只从隐藏层的作用是升维这一角度去理解，通常会发现隐藏层的神经元要少于输入个数，比如：<img src="/../img/image-20221002175255234.png" srcset="/img/loading.gif" lazyload alt="image-20221002175255234"></p>
<p>举个例子，在人脸识别中，图片的每个像素都是一个输入，最后对这些输入进行抽象，提取特征，才是我们需要的东西，真实的输入像素反而不是。</p>
<p><img src="/../img/image-20221002175534642.png" srcset="/img/loading.gif" lazyload alt="image-20221002175534642"></p>
<p>再举个例子，在字符识别中，把字符分割成不同大小，分割的最小最具体的一层作为最外层神经元，可复用性最强，个数也最多，越往里分割的部分越抽象，神经元个数也越小，因此隐藏层越深，抽象程度越高。</p>
<p><img src="/../img/image-20221002175642701.png" srcset="/img/loading.gif" lazyload alt="image-20221002175642701"></p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>在预测的时候，用到的是正向传播$f_W(X)=\hat{Y}$，只不过这个f是一个神经网络，运算特别复杂。在训练的时候，这个关系就会反过来，用来训练的是已知的真实数据X和Y，作为变量需要我们求解的是系数。</p>
<p><img src="/../img/image-20221002181308051.png" srcset="/img/loading.gif" lazyload alt="image-20221002181308051"></p>
<p>这个曲面的最高点就是我们在选择策略后想要达到的目标。但是在初始化时，系数可能出现在任何一个点上，理想情况下，它到最优点的距离就是我们希望求解的目标，但是白想，没有用。</p>
<p>但是有一点，那就是在这个曲面上任何一点， 我们都可以求出一个向量，这个向量总是指向上升最快的方向，这个向量在参数平面上的投影就叫做梯度。</p>
<p><img src="/../img/image-20221002182458208.png" srcset="/img/loading.gif" lazyload alt="image-20221002182458208"></p>
<p> 梯度下降法：梯度总是指向上升最快的方向，但是有的策略是求最大，有的是求最小，为了统一他们，即便是求最大的问题。也会在函数前面加一个负号，转换成求最小。</p>
<p><img src="/../img/image-20221002182914297.png" srcset="/img/loading.gif" lazyload alt="image-20221002182914297"></p>
<p>这里曲面虽然反了，但是梯度仍然是指向上升最快的方向，所以会先对梯度求反，让他指向下降最快的方法。</p>
<p><img src="/../img/image-20221002183649863.png" srcset="/img/loading.gif" lazyload alt="image-20221002183649863"></p>
<p>通过这个梯度进行反向传播，更新W的参数：</p>
<p><img src="/../img/image-20221002183846639.png" srcset="/img/loading.gif" lazyload alt="image-20221002183846639"></p>
<p>反向传播表达式：</p>
<p><img src="/../img/image-20221002184116647.png" srcset="/img/loading.gif" lazyload alt="image-20221002184116647"></p>
<p><img src="/../img/image-20221002184128040.png" srcset="/img/loading.gif" lazyload alt="image-20221002184128040"></p>
<h2 id="支持向量机SVM"><a href="#支持向量机SVM" class="headerlink" title="支持向量机SVM"></a>支持向量机SVM</h2><p>软间隔：</p>
<p><img src="/../img/image-20221002191511910.png" srcset="/img/loading.gif" lazyload alt="image-20221002191511910"></p>
<p>延伸到多维平面：</p>
<p><img src="/../img/image-20221002191712109.png" srcset="/img/loading.gif" lazyload alt="image-20221002191712109"></p>
<p>求x到平面的间距，先求和平面垂直的法向量，也就是下降最快的向量——梯度W，把f(X)对X求导，把X约掉，得到梯度W。</p>
<p><img src="/../img/image-20221003011038872.png" srcset="/img/loading.gif" lazyload alt="image-20221003011038872"></p>
<p>求距离公式=法向量x该点到平面上一点的向量/法向量的二范数。原理：两向量相乘，二者内积就等于一个是一方对另一方的贡献 ，几何意义就是A向量在B向量上的投影再乘上B向量。</p>
<p>推导过程：</p>
<p><img src="/../img/image-20221003011103574.png" srcset="/img/loading.gif" lazyload alt="image-20221003011103574"></p>
<p>以上，是间隔最大的证明过程，但还需要保证分类也是正确的，即蓝色的点f(X)都要大于0，黄色的点f(X)都要小于0。</p>
<p><img src="/../img/image-20221003011547492.png" srcset="/img/loading.gif" lazyload alt="image-20221003011547492"></p>
<p>推导过程：</p>
<p><img src="/../img/image-20221003011613894.png" srcset="/img/loading.gif" lazyload alt="image-20221003011613894"></p>
<p>整理以上两个条件可得：</p>
<p><img src="/../img/image-20221003011646310.png" srcset="/img/loading.gif" lazyload alt="image-20221003011646310"></p>
<p>$s.t.$约束条件保证分类是正确的，边界计算还可以更简化。</p>
<p>线性函数中W和b同时增加两倍，直线位置不变，$margin(W,b)$大小也不变。</p>
<p><img src="/../img/image-20221004160750128.png" srcset="/img/loading.gif" lazyload alt="image-20221004160750128"></p>
<p>因此可以做出左边的变形，虽然结果不变，但是括号内的直线部分却发生了变化，为了更确定H的值，另绝对值里的部分强行等于1，于是就又多了一个$s.t.$约束条件，因为左边有个绝对值，所以右边要加上一个标签，可知在分类正确的前提下，加了标签该式就肯定大于0，就相当于加绝对值了。</p>
<p>合并起来后，可以得到：</p>
<p><img src="/../img/image-20221004161255962.png" srcset="/img/loading.gif" lazyload alt="image-20221004161255962" style="zoom:50%;"></p>
<p>还可以转化成求最小问题：$\min||W||_2$。</p>
<p>为了方便用<a target="_blank" rel="noopener" href="https://www.cnblogs.com/maybe2030/p/4946256.html">拉格朗日乘数法</a>，还可以把它展开成：$\min\frac{1}{2}W^TW$。基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题。</p>
<p>这里回看对偶问题那一章，晚会看。</p>
<p>只有在边界上的点，才对边界的大小起作用，才有贡献，因此我们只在乎边界上的点。</p>
<p><img src="/../img/image-20221004165715184.png" srcset="/img/loading.gif" lazyload alt="image-20221004165715184" style="zoom:50%;"></p>
<p>因此，最后决定参数W的只和支持向量有关，SVM就叫做支持向量机。</p>
<p>将拉格朗日函数转化为损失函数：</p>
<p><img src="/../img/image-20221004165937256.png" srcset="/img/loading.gif" lazyload alt="image-20221004165937256" style="zoom:50%;"></p>
<p>上面这些都是硬间隔，将蓝色和黄色绝对分开。而如果遇到不好分开的情况，我们允许它存在，就叫做软间隔，需要引入一个误差项并做一些改动。正则化是用来减少过拟合的，<u>这个暂时还不会，再看看</u>。</p>
<p><img src="/../img/image-20221004170227536.png" srcset="/img/loading.gif" lazyload alt="image-20221004170227536" style="zoom:67%;"></p>
<p>数学表达：</p>
<p><img src="/../img/image-20221004170325892.png" srcset="/img/loading.gif" lazyload alt="image-20221004170325892" style="zoom:50%;"></p>
<p>$\xi$代表被分错的点到边界的距离，如果分对了，那他就是零，因此还需要这个的总和尽量小，约束条件里也从小于0放宽成了小于$\xi$。两个约束条件写到一起就变成：</p>
<p><img src="/../img/image-20221004171736009.png" srcset="/img/loading.gif" lazyload alt="image-20221004171736009" style="zoom:50%;"></p>
<p>于是损失函数就变成：</p>
<p><img src="/../img/image-20221004171833056.png" srcset="/img/loading.gif" lazyload alt="image-20221004171833056" style="zoom:50%;"></p>
<p>看一眼损失函数里的经验损失项怎么来的：</p>
<p><img src="/../img/image-20221004172424868.png" srcset="/img/loading.gif" lazyload alt="image-20221004172424868"></p>
<p>先看紫线，黄点位于其上方，因此代入线性函数大于0，同时它又是黄点，标签应该为-1，二者相乘是个负数，代入不等式大于1，因此经验损失项也大于1。</p>
<p>看绿线，在黄点上方，离黄点距离很近，偏差小于1，代入不等式大于0，经验损失项大于0。</p>
<p>红线也在黄点上方，偏差很大，分类相当正确，代入不等式小于0，没有产生损失，经验损失项为0。</p>
<p>画出经验损失项的图：</p>
<p><img src="/../img/image-20221004172919851.png" srcset="/img/loading.gif" lazyload alt="image-20221004172919851" style="zoom:50%;"></p>
<p>W，b错的越离谱，损失函数越大，W，b正确的话则不会产生损失。</p>
<p>机器学习分类问题整理：</p>
<p><img src="/../img/image-20221004173511340.png" srcset="/img/loading.gif" lazyload alt="image-20221004173511340"></p>
<p>现实中我们能做出来的是分类函数T(X)，它能做到的顶多就是在训练集里能完全分类，但我们的目标其实是在所有情况下均可以正确分类，也就是G(X)。</p>
<p>我们在训练T(X)的过程就是把它拆分成两部分——模型函数和决策函数，用一个模型函数来分割或拟合这些样本点，这样样本点就被转化成了一个具体的数字，再用决策函数对这个数字进行标签化，大于0代表正类，小于0代表反类，因此，只要模型函数确定了，最后标签也就确定了。</p>
<p>可以说，模型函数就是给全空间的样本点都赋予了一个具体的实数值，f(X)=0既是分界线，也是度量的锚点。</p>
<p>我们比较T(X)和G(X)，想让他们尽可能一样，只需要比较模型函数即可，决策函数部分不用比较，因为只要他们对应的模型函数是一致的，决策结果一定是一致的。</p>
<p>因此，这里引入J(X)来比较T(X)和G(X)，这个J(X)就是我们常说的损失函数，学习算法就是用来找到最小损失函数的，这样就把机器学习拆成了两部分：</p>
<p><img src="/../img/image-20221004174609296.png" srcset="/img/loading.gif" lazyload alt="image-20221004174609296"></p>
<p>而损失函数就是为这个度量赋予意义，然后再做适当修正，让修正的结果具有某种现实意义，根据我们对现实意义的理解做出约束，最后在这个现实意义的基础上进行决策。</p>
<p>可以说，最开始得到的这个度量是没有现实意义的，我们无法根据它猜测这个模型函数和真实情况是否一致，只有在我们给它赋予一些在现实中已经证实的有经验的东西——我们对数据、经验、和世界的理解和归纳。（感觉开始玄学了）</p>
<p>举个最小二乘法的例子，它给损失函数赋予的意义是一个服从高斯分布的方差，因为根据经验，我们现实中的噪声多数是高斯噪声。</p>
<p><img src="/../img/image-20221004180202366.png" srcset="/img/loading.gif" lazyload alt="image-20221004180202366"></p>
<p>紫色是理想情况，最小二乘法默认方差是服从高斯分布的，因此我们只要找到这些值进行代入就可以了。但是理想情况的函数值只有因果律知道，不过没关系，我们知道训练集的标签，它是已知的，我们可以把训练集的标签当作理想情况的函数值来输入。但是这个标签很固定，只会是-1和1，怎么和我们的模型参数进行比较啊？用双曲正切函数修正它，把它约束到-1和1之间，那这个时候再去求它和标签值的方差，就很合理了。</p>
<p>再看看别的损失函数的意义：</p>
<p>最大似然估计法：通过sigmoid得到的值在0-1之间，就可以看作一个概率。在训练集里X和Y已知的情况下，对所有未知参数$\theta$求似然值，当这个似然值最大的时候，我们猜测这个函数和最终目标函数相同。（就相当于，如果在取这个$\theta$的时候，得到现实样本点的概率最大，那它是现实函数的参数的可能性就越大）</p>
<p>推导过程的最后一步，因为抽样X的概率各个X的概率是一样大的，所以$P(\overline{X})$可以看作是一个常数，在求max的时候直接忽略掉这个常数就可以了。然后就得到了最大似然值的表达式，要注意的是：$y$是真实标签的值，它的值只能是0或1，否则不成立。</p>
<p><img src="/../img/image-20221004183112819.png" srcset="/img/loading.gif" lazyload alt="image-20221004183112819"></p>
<p>并且，再假设：</p>
<p><img src="/../img/image-20221004183622147.png" srcset="/img/loading.gif" lazyload alt="image-20221004183622147" style="zoom:50%;"></p>
<p>这就是最大后验估计。</p>
<blockquote>
<p>从原因到结果的论证称为“先验的”，而从结果到原因的论证称为“后验的”。</p>
</blockquote>
<p>SVM：在函数模型前乘以一个项，得到它离软间隔的几何距离，有了这个度量后再进行三个约束，默认当约束全部满足时，我们猜的模型和真实模型是一致的。</p>
<p><img src="/../img/image-20221004184019866.png" srcset="/img/loading.gif" lazyload alt="image-20221004184019866" style="zoom:80%;"></p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>如果是两个同类型的model，比如都是高斯分布，那只用分别比较他们的期望和方差就可以了，但如果不是同一种模型，比如一个是高斯分布，一个是泊松分布，那是否存在一个公度，可以把他们放在一起比较呢？</p>
<p>答案是熵。</p>
<p>从信息论的角度可以理解为，从原来的不确定变成确定，它的难度有多大。</p>
<p>信息量其实是无法计算的，我们只能给出定义。</p>
<p><img src="/../img/image-20221004202949393.png" srcset="/img/loading.gif" lazyload alt="image-20221004202949393"></p>
<p>打个比方，阿根廷夺冠的信息量其实是等于进决赛和赢了决赛加在一起的时候的，因为当我们知道阿根廷进决赛并赢下决赛，就等于知道了阿根廷夺冠。但这在概率论上，相加明显就不成立了。为了保持自洽，信息量的计算公式必须有一个log，并且需要函数是递减的，所以要加个负号。而以2为底，最后得出的单位是bit。</p>
<p><img src="/../img/image-20221004211131305.png" srcset="/img/loading.gif" lazyload alt="image-20221004211131305" style="zoom:67%;"></p>
<p>这时候下面的函数值就等于3。</p>
<p><img src="/../img/image-20221004211509482.png" srcset="/img/loading.gif" lazyload alt="image-20221004211509482"></p>
<p>德国和比利时赢下足球的概率是一半一半，非常难确定谁会赢球，这个系统的熵就很大。但是法国和中国几乎稳稳地就是法国赢球，非常容易确定，因此这个系统的熵就很小。然而简单相加左边这个系统的熵就很小，所以要根据贡献度乘以一个比例才行。</p>
<p>因此，右边对系统贡献的信息量大概是0.058，</p>
<p><img src="/../img/image-20221004211951950.png" srcset="/img/loading.gif" lazyload alt="image-20221004211951950" style="zoom:50%;"></p>
<p>左边对系统贡献的信息量是1，</p>
<p><img src="/../img/image-20221004212121257.png" srcset="/img/loading.gif" lazyload alt="image-20221004212121257" style="zoom:50%;"></p>
<p>这么一看不就是求期望吗，因此可以把熵定义为：</p>
<p><img src="/../img/image-20221004212214048.png" srcset="/img/loading.gif" lazyload alt="image-20221004212214048"></p>
<p>最后算出来的是一个不确定（混乱）程度。</p>
<p>相对熵（KL散度）：</p>
<p><img src="/../img/image-20221004212640747.png" srcset="/img/loading.gif" lazyload alt="image-20221004212640747"></p>
<p>P在前，所以是以P系统为基准，熵是恒定的，因此最后得到的式子里，后半部分是恒定的，我们只用看前面部分。当KL散度等于0的时候，P和Q是最接近的。</p>
<p>根据吉布斯不等式，KL散度是绝对大于等于0的。</p>
<p>因此，如果我们想让这两个模型尽可能的接近，那就去找交叉熵的最小值就好了，也就是说，交叉熵本身就可以作为损失函数。</p>
<p>交叉熵的定义：</p>
<p><img src="/../img/image-20221004213244529.png" srcset="/img/loading.gif" lazyload alt="image-20221004213244529" style="zoom:50%;"></p>
<p>这个m是什么呢？如果P，Q的系统里，x也就是发生事件的数量不一样，那么这个x只要选数量大的那个就可以了。</p>
<p><img src="/../img/image-20221004213649587.png" srcset="/img/loading.gif" lazyload alt="image-20221004213649587"></p>
<p>当把图像识别和人脑做交叉熵的时候，p就是人脑，它只肯给出两个可能，要么是猫要么不是，而q就是机器学习要给的它像猫或者不像猫的概率到底是多少，最后展开成上图。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>当我们求梯度的时候，求的就是损失函数的梯度。损失函数就是我们设计出来的函数和我们真正想要的之间的误差，那么损失函数是如何设计出来的呢？</p>
<p>吴恩达老师给出的公式，可以写成两种形式：</p>
<p><img src="/../img/image-20221005085901958.png" srcset="/img/loading.gif" lazyload alt="image-20221005085901958" style="zoom:50%;"></p>
<p><img src="/../img/image-20221005085918649.png" srcset="/img/loading.gif" lazyload alt="image-20221005085918649" style="zoom:50%;"></p>
<h4 id="最小二乘法（最简单）"><a href="#最小二乘法（最简单）" class="headerlink" title="最小二乘法（最简单）"></a>最小二乘法（最简单）</h4><p>以下面这个神经网络为例：</p>
<p><img src="/../img/image-20221005090133768.png" srcset="/img/loading.gif" lazyload alt="image-20221005090133768" style="zoom:80%;"></p>
<p>输入是一个非0即1的标签，代表输入的图片是不是猫，经过神经网络计算后输出一个0-1之间的概率，表示是猫的概率有多少。当x和y相差无几的时候，我们就可以认定机器学习的模型和我们人脑的模型是近似的。</p>
<p>因此，为了求这个差距，我们可以用$\min|x-y|$来表示，但是绝对值通常不会是全程可导的，因此我们需要对$(x-y)$求平方，这样虽然值发生了变化，但是最小还是最小，不影响。</p>
<p><img src="/../img/image-20221005090735612.png" srcset="/img/loading.gif" lazyload alt="image-20221005090735612" style="zoom:50%;"></p>
<p>因此可以证得吴恩达老师的第一个公式：</p>
<script type="math/tex; mode=display">
L(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2</script><p>这里的$\hat{y}$就是上面神经网络里的$x_i$，为什么要加一个$\frac{1}{2}$的系数是因为求偏导后会下来一个2，正好可以约掉。</p>
<p>直接把两个模型结果做差求距离虽然很简单，但是在求梯度下降的时候会非常麻烦，因此不建议选用。</p>
<h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>是一种概率的反向运用，打个比方：抛硬币的时候，假设我们不知道其实是正反各0.5的概率，但是我们掷了1w次以后，正反差不多各5k次，我们就可以反推它们的概率分布是正反各0.5。</p>
<p>但如果投了10次硬币，7次正3次反，我们又规定了这样的概率模型，那么这种情况会出现的概率有多大呢？答案很明显是$0.1^7\cdot0.9^3$。</p>
<p><img src="/../img/image-20221005092107603.png" srcset="/img/loading.gif" lazyload alt="image-20221005092107603" style="zoom:50%;"></p>
<p>我们把这个概率值叫做似然值，它虽然也是一种可能性，但是是从真实世界反推回来的。它是在真实的情况已经发生后，我们假设它有很多模型，在某一个概率模型下发生这种真实情况的可能性。</p>
<p>在训练神经网络的时候，我们输入的图片就像是抛出的那些硬币一样。我们人脑已经知道这些图片是不是猫了，并且告诉了神经网络，现在就要找到一种似然值最大的情况，使神经网络输出的分类也和我们人脑一样，这时的概率模型就应该是最接近真实情况的。</p>
<p>举个例子：</p>
<p><img src="/../img/image-20221005093452699.png" srcset="/img/loading.gif" lazyload alt="image-20221005093452699"></p>
<p>这世上有各种各样的猫，但是存在一个边界，超出这个边界就不是猫了，这个边界是我们人脑划定的，比如他可能呈高斯分布，那么神经网络就要调整自己的分布，通过调期望和方差来逼近我们人脑的边界线。</p>
<p>但也可能不是，不过没关系，只要叠加的感知级够多，什么形状都可以逼近出来的，因此我们搭建一个NN神经网络，通过调整W和b来逼近这个人脑模型。</p>
<p>推导极大似然估计的公式：</p>
<p><img src="/../img/image-20221005094256500.png" srcset="/img/loading.gif" lazyload alt="image-20221005094256500" style="zoom:50%;"></p>
<p>$x_i$放到上面那个判断猫猫的例子里，就是判断的结果0或1，将所有概率连乘后得到第二行的式子，但是每一项的结果不是0就是1，用不了，因此要引入一个$y_i$，它的参数是W和b，输出的是猫猫的概率为多少。</p>
<p>因为$x$不是0就是1，所以其实是符合伯努利分布的，p代表是猫猫的概率，y就是猫猫的分布，最后展开成：</p>
<p><img src="/../img/image-20221005095408839.png" srcset="/img/loading.gif" lazyload alt="image-20221005095408839" style="zoom:50%;"></p>
<p>连乘不好算，换成连加，log不改变单调性，该极大值还是极大值：</p>
<p><img src="/../img/image-20221005095629064.png" srcset="/img/loading.gif" lazyload alt="image-20221005095629064" style="zoom:50%;"></p>
<p>然后就得到了吴恩达老师的第二个公式：</p>
<script type="math/tex; mode=display">
L(\hat{y},y)=-[y\log\hat{y}+(1-y)\log(1-\hat{y})]</script><p>这里的$\hat{y}$就是我们的$x_i$，因为我们更常算最小值，所以加个负号把max变成了min。</p>
<p><img src="/../img/image-20221005095753697.png" srcset="/img/loading.gif" lazyload alt="image-20221005095753697" style="zoom:50%;"></p>
<h2 id="最大熵"><a href="#最大熵" class="headerlink" title="最大熵"></a>最大熵</h2><p>训练神经网络用梯度下降法，优势是简单好用，劣势是计算量非常大。看看大佬们怎么优化和改进梯度下降法，其中有一种是在隐藏层，把隐藏层感知机的激活函数换掉，从原来的sigmoid换成ReLU函数。</p>
<p>原因很好理解，因为如果传入的数值恰好在绿框部分的话，这里的梯度是非常小的，如果这个梯度再反向传播，传到前面几层基本上就不剩啥了，这就叫做梯度消失。</p>
<p><img src="/../img/image-20221005101034990.png" srcset="/img/loading.gif" lazyload alt="image-20221005101034990"></p>
<p>如果换成ReLU函数，只要大于0就是一条直线，梯度是一个定值，不会出现梯度消失的问题。但是被ReLU激活过的函数输出是可以无穷大的，怎么转化成概率值需要的0-1之间呢？</p>
<p><img src="/../img/image-20221005101226575.png" srcset="/img/loading.gif" lazyload alt="image-20221005101226575" style="zoom:50%;"></p>
<p>可以在最后一层用sigmoid，把它重新归到0-1之间，这个时候用损失函数就没太大问题了。但是这时候就只能解决一种分类的情况了，因为sigmoid函数只能解决单个感知机归一的问题，比如最后可能会输出两个分类结果，是猫的概率是60%，是狗的概率也是60%，加起来都超过1了。明显不太适合用于这种分类造成的结果互斥情况（是猫就不会是狗），但如果是可以标签重合的情况，就比如这个小动物既可以是可爱的，也可以是吃肉的，等等等等，这些要一起分类的话，softmax反而不合适了，反而要回去用sigmoid。</p>
<p>输出层只有单个感知机的情况：</p>
<p><img src="/../img/image-20221005103127031.png" srcset="/img/loading.gif" lazyload alt="image-20221005103127031"></p>
<p>输出层有多个感知机的情况，z，y，a就变成了一个向量：</p>
<p><img src="/../img/image-20221005103222720.png" srcset="/img/loading.gif" lazyload alt="image-20221005103222720"></p>
<p>a作为最后输出的结果：</p>
<p><img src="/../img/image-20221005103326143.png" srcset="/img/loading.gif" lazyload alt="image-20221005103326143" style="zoom:50%;"></p>
<p>首先是一个向量，其次每一项都是一个概率，所以大于等于0肯定，然后任何一个图片或数据输入给神经网络，都应该有一个分类，把每一个分类的概率都加起来，那它就应该是百分百会发生的事，所以元素之和等于1，用e为底，将z作为e的指数，就可以做到值域绝对大于0，再除以所有分子的总和，做归一化处理，得到的就是softmax函数。</p>
<p><img src="/../img/image-20221005150321481.png" srcset="/img/loading.gif" lazyload alt="image-20221005150321481" style="zoom:50%;"></p>
<p>通过比较softmax和sigmoid函数发现，sigmoid函数其实就是只有一种分类情况下的softmax，$t_2$是不会变化的。</p>
<p>之前章节里，我们学到的都是无论这个神经网络有多么复杂，只要似然值最大并且交叉熵最小，就可以认为这个模型已经逼近了人脑，但其实还存在一个隐藏的前提假设，只要激活函数选择了sigmoid或者softmax，这个前提条件就默认被勾选了，它就是最大熵原理。</p>
<p>举个例子，比如说你对一个概率问题完全没有什么额外的信息，我们去猜概率的话只能挨个猜，相当于每个事件的概率都是相等的，当然也可以猜第一个是50%，第二个是25%，第三个是5%这样。这样计算出来第二种的熵明显比第一种要小，而第一种熵是所有猜测里最大的。</p>
<p>在已知一部分概率的条件下，未知的信息都要符合最大熵原理。这样得到的模型既能满足已知的信息，又能让未知的信息熵最大。因此，问题可以被拆成两部分，第一个部分是解决相同的问题，第二个部分是解决最大熵的问题。</p>
<p>前面提到过的交叉熵，KL散度，可以用来比较两个概率模型之间的差别，但前提是两个概率模型是已知的，而现在两个里我们只知道半个，首先目标概率模型是不到的，已知的只是一堆样本数据，还要从中归纳出背后的概率模型。</p>
<p>那咋办呢？数学家提出了一个概念，是概率论里的矩。</p>
<p>先看一眼正态分布：</p>
<p><img src="/../img/image-20221006092037858.png" srcset="/img/loading.gif" lazyload alt="image-20221006092037858"></p>
<p>不论什么样的正态分布，只要确定了期望和方差，都可以没有任何信息损失的把它的概率分布描绘出来，它的期望和方差表达式如上，期望就是平均分布，方差就是一个二次减一次的平方。但就凭这个还是无法归纳出背后的规律，现在对正态分布做一定量的偏移，得到：</p>
<p><img src="/../img/image-20221006092517835.png" srcset="/img/loading.gif" lazyload alt="image-20221006092517835"></p>
<p>多了一个三次项，而$\mu$和$\sigma$在前面都已经知道了，那是不是可以猜想，概率分布和这些多次项有关呢？然后数学家证明了，真的有，而且还起了名字。</p>
<p><img src="/../img/image-20221006092915969.png" srcset="/img/loading.gif" lazyload alt="image-20221006092915969"></p>
<p>得到的归纳结果是，如果是一个正态分布，只用一阶矩和二阶矩就能描述出来，更复杂一点，需要三阶矩，再复杂的话就通过四阶矩、五阶矩、六阶矩等等，也能给描述出来。</p>
<p>也就是说，任何一个概率分布，它的特征都可以用这样一个向量来表现：</p>
<p><img src="/../img/image-20221006093434038.png" srcset="/img/loading.gif" lazyload alt="image-20221006093434038" style="zoom:50%;"></p>
<p>数学家们定义了一个特征函数，$e^{itx}$是一个复数，求这个复数的期望，任何一个概率分布都可以用特征函数来表示出来，而且一个概率分布就对应一个特征函数，把这个特征函数在$x=0$处<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/404446570#:~:text=%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%98%AF%E5%A4%9A%E9%A1%B9%E5%BC%8F%E9%80%BC%E8%BF%91%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E4%BD%8E%E6%AC%A1%E5%88%B0%E9%AB%98%E6%AC%A1%E7%9A%84%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%B4%AF%E5%8A%A0%E6%9D%A5%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0%20f%20%28x%29%20%E5%9C%A8%E6%9F%90%E4%B8%AA%E7%82%B9%E9%82%BB%E5%9F%9F%E7%9A%84%E5%87%BD%E6%95%B0%E5%80%BC%E3%80%82%20%E6%AF%94%E5%A6%82%E5%9C%A8%E7%89%A9%E7%90%86%E4%B8%8A%EF%BC%8C%20text%20%7Bsin%7D%20%28x%29,x%20%E6%9D%A5%E9%80%BC%E8%BF%91%20text%20%7Bsin%7D%20%28x%29%20%E5%9C%A8%20x%3D0%20%E9%99%84%E8%BF%91%E7%9A%84%E5%87%BD%E6%95%B0%E5%80%BC%E3%80%82">泰勒展开</a>，得到上面的式子。</p>
<blockquote>
<p><strong>泰勒展开的实质是找一个各阶导数都和原函数在展开点处的导数相等的多项式，来达到拟合的效果。</strong></p>
</blockquote>
<p>这个式子其实就是上面那个向量的线性关系。</p>
<p>也就是说，如果有两组数据，想知道它们的概率分布是否一样，只需要去比较它们的一阶矩，二阶矩，三阶矩……是否相同，如果相同，不需要去计算他们的概率分布具体表达式，就可以确定是一样的。</p>
<p>因此，我们在做神经网络拟合现实模型的时候，只需要保证：</p>
<p><img src="/../img/image-20221006100355037.png" srcset="/img/loading.gif" lazyload alt="image-20221006100355037" style="zoom:50%;"></p>
<p>现在我们知道拿到两个概率模型之后，就算我们不知道它们具体是什么，也知道应该比什么了。具体到机器学习和训练集，这个情况是什么样的呢？</p>
<blockquote>
<p>$\tilde{P}$代表经验概率，即从数据中直接归纳出的概率。</p>
</blockquote>
<p><img src="/../img/image-20221006100815684.png" srcset="/img/loading.gif" lazyload alt="image-20221006100815684"></p>
<p>在图片识别应用中：</p>
<p><img src="/../img/image-20221006102106310.png" srcset="/img/loading.gif" lazyload alt="image-20221006102106310"></p>
<p>训练集内有N个图片，每个照片都可以用$(x,y)$来表达，其中x表示照片，y表示标签，它是一个向量，每个分量代表一个分类，因为神经网络的输出层有四个，所以是一个四维的向量，代表它属于哪一个分类。</p>
<p>但是这里给出的照片不可能有两个完全一样的照片，所以若按像素输入的话概率值只可能等于1/N，这样就没有意义了，无法区分开不同的照片。 </p>
<p>但不看输入层，看隐藏层，尤其是输出层前一层的隐藏层，感知机就没有那么多了，可以把这一层的感知机看作一个一个的特征，将特征作为x输入，这时候概率值就不是1/N了。</p>
<p><img src="/../img/image-20221006104908439.png" srcset="/img/loading.gif" lazyload alt="image-20221006104908439"></p>
<p>$\tilde{P}(x,y)$和$\tilde{P}(x)$都是已知的，我们的目标是求在输入x图片的情况下，y——即分类概率是多少，$P(y|x)$中包含了一部分已知的，又包含了一部分未知的，已知的部分要完全相等，未知的部分要用最大熵，这就是我们接下来求解问题的方针。</p>
<p>用贝叶斯公式把目标概率展开得：</p>
<p><img src="/../img/image-20221006105744859.png" srcset="/img/loading.gif" lazyload alt="image-20221006105744859" style="zoom:50%;"></p>
<p>$P(x,y)$按理说是不知道的，但是前面有经验概率，我们若想利用这个经验概率，需要保证$P(x)\cdot P(y|x)$和这个经验概率相等，$P(x)$仍然可以用经验概率，未知部分要用最大熵来确定，因此，要保证：</p>
<p><img src="/../img/image-20221006110648640.png" srcset="/img/loading.gif" lazyload alt="image-20221006110648640" style="zoom:50%;"></p>
<p>第三个式子虽然也可以用，但是求偏导不太方便，所以不建议，我们不用。</p>
<p>在前面通过数数的方式得到了$\tilde{P}$，但是只知道概率是无法求期望的，期望的公式是概率乘以随机变量的累加或积分，在这里只有概率，没有随机变量，所以我们还需要在设计一个x和y的随机变量。</p>
<p>举个例子：</p>
<p>当确定了事件A以后，只要不符合其中的一个特征，比如“尖脸，是鸭”，就不属于这个事件那么X就等于0。</p>
<p><img src="/../img/image-20221006164639901.png" srcset="/img/loading.gif" lazyload alt="image-20221006164639901" style="zoom:50%;"></p>
<p>这样满足A和不满足A这两个事件就构成了二项伯努利分布非0即1的条件，最后得到X的期望结果就是$\tilde{P}$，就是满足A的概率本身。</p>
<p>以上是只有一个条件，当一共有m个条件，把样本空间里面所有可能的事件都覆盖掉，那么对$X_1,X_2,…,X_3$求期望，得到的就是对应事件的概率。这部分的函数在很多论文里叫特征函数，其实英文直译应该是指示函数。</p>
<p><img src="/../img/image-20221006171520590.png" srcset="/img/loading.gif" lazyload alt="image-20221006171520590" style="zoom:50%;"></p>
<p>这时候再去完成让两个模型分布相等的目标，只需要把f设计成所有事件的一阶矩，因为只要一阶矩相等，这两个概率分布就一定是相等的。因为对于伯努利分布来说，决定它的形状的只有一阶矩。</p>
<p>因此，之前那个需要考虑很多阶矩的复杂问题就只用考虑一阶矩了，只要将样本空间里所有的可能事件都列举出来就可以了。</p>
<p>这时候，下面这个问题其实就已经解决了：</p>
<p><img src="/../img/image-20221006172021361.png" srcset="/img/loading.gif" lazyload alt="image-20221006172021361" style="zoom:50%;"></p>
<p>在设计了这些$X_m$事件后，就相当于在原来的样本空间$(x^(k),y^(k))$上，重新设计了一个样本空间，原样本空间的维度非常高，现在变成了只有一维的情况，这个一维就是事件。</p>
<p><img src="/../img/image-20221006172434773.png" srcset="/img/loading.gif" lazyload alt="image-20221006172434773"></p>
<p>也就是把原来很多维的输入，都投射到一维的事件上，在一维上进行比较。</p>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>我们之前的目标有两个，第一个已经达成了，还有第二个求条件概率的最大熵。要求$P(y|x)$的熵，需要把每一个事件下y的熵都求出来，并对它们求期望，所以前面再乘以一个概率，然后求和。</p>
<p><img src="/../img/image-20221006174155241.png" srcset="/img/loading.gif" lazyload alt="image-20221006174155241"></p>
<p>得到右边条件熵的公式$H(Y|X)$，代表X发生之后Y的概率，求和符号下的$x,y$代表x和y的任意一种组合情况，我们可以把它看作X取一个具体值时的熵，然后再对这个熵求期望。</p>
<p>我们要求它的最大值，还需要满足别的条件。</p>
<blockquote>
<p>sigmoid和softmax函数的本质其实就是最大熵。</p>
</blockquote>
<p>因为概率经验$\tilde{P}$是可以直接从训练样本里数出来的，所以这里把它看作常数$\Delta$。</p>
<p><img src="/../img/image-20221006181845123.png" srcset="/img/loading.gif" lazyload alt="image-20221006181845123"></p>
<p>在同时满足上面两个条件的同时，用拉格朗日乘数法来求极小值，推导过程如下：</p>
<p><img src="/../img/image-20221006182040984.png" srcset="/img/loading.gif" lazyload alt="image-20221006182040984"></p>
<p>拉格朗日乘数法求极值有一个基础，原本的问题是求当$P,\lambda$等于多少的时候，L的值为最小值，具体过程是，先把$\lambda$当作需要调整的参数，P当作一个参数，找到L的最大值，然后再在最大值的情况下，再去调整P，找到L的最小值。对偶问题则是先把P当作参数，再把$\lambda$当作参数。二者结果是一样的，具体求解如下：</p>
<p><img src="/../img/image-20221006183112036.png" srcset="/img/loading.gif" lazyload alt="image-20221006183112036"></p>
<p>因为$\lambda$项没有$\tilde{P}(x)$系数，为了把$\tilde{P}(x)$提出来，乘上一个它的加和，易知是等于1。这时候就需要偏导等于0来求极值，因为后半部分始终同号，所以求和所有项都是同号，要求和为0，所以要每项都为0。</p>
<p><img src="/../img/image-20221006183804353.png" srcset="/img/loading.gif" lazyload alt="image-20221006183804353"></p>
<p>把结果化简成感知机进行线性运算的形式，因为$\lambda_0$是归一化条件的系数，所以条件概率最后是归一的，并且样本空间所有样本概率求和结果为1，因此分子分母应该相等。</p>
<p><img src="https://gitee.com/labulavish/blog/raw/master/img/202210061841725.png" srcset="/img/loading.gif" lazyload alt="image-20221006184118190" style="zoom:50%;"></p>
<p>回到softmax函数，可以闭环了。</p>
<p><img src="/../img/image-20221006184557423.png" srcset="/img/loading.gif" lazyload alt="image-20221006184557423"></p>
<p>这里softmax以e为底，不只是为了保证结果大于0，而且是因为在求最大熵偏导的过程中引入了$\log$，反函数用到了e，因此，只要在输出层用到了sigmoid或者softmax函数，就保留了最大熵原理的应用。</p>
<p>前面只确定了最大熵P的形式，还没有确定参数。</p>
<p><img src="/../img/image-20221006185111870.png" srcset="/img/loading.gif" lazyload alt="image-20221006185111870" style="zoom:50%;"></p>
<p>我们在做机器学习的时候，我们已知的是训练集，但我们希望可以把模型应用到训练集的延长线上，我们扩大的是什么？</p>
<p><img src="/../img/image-20221006185301603.png" srcset="/img/loading.gif" lazyload alt="image-20221006185301603"></p>
<p>如果x和y都在训练集上，那么条件概率就等于经验概率，如果x和y有一个不在训练集上，那么条件概率就是未知的，未知的话就可以用到最大熵原理，因此，这就是机器学习的前提：</p>
<p><img src="/../img/image-20221006185450171.png" srcset="/img/loading.gif" lazyload alt="image-20221006185450171" style="zoom:50%;"></p>
<p>通过$\min L(P,\lambda)$确定出$P(y|x)$的形式，他逃不出指数族的形式。</p>
<p><img src="/../img/image-20221006191000121.png" srcset="/img/loading.gif" lazyload alt="image-20221006191000121"></p>
<p>确定形式后代回原式来求$\lambda$的具体值，这个$\lambda$是一个向量，维数非常多，仅凭计算是求不出来的，常用方法是构建一个损失函数，用梯度下降法训练得到。</p>
<p><img src="/../img/image-20221006191420978.png" srcset="/img/loading.gif" lazyload alt="image-20221006191420978"></p>
<p>只能说为了水论文看这个不合适，想要自己设计算法，知道谁是谁的爹谁和谁有血缘关系太重要了。</p>
<p>我觉得他们最终归一其实就是为了解决机器学习，但针对不同工程问题而设计出来的算法，极大似然和交叉熵这种计算简单，可以用在直观数据的机器学习，而softmax这种适合更加复杂的问题，比如图片。</p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/book/" class="category-chain-item">book</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/book/">#book</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/09/30/Machine-Learning/" title="Machine Learning">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Machine Learning</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/09/28/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="深度强化学习">
                        <span class="hidden-mobile">深度强化学习</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="vcomment" class="comment"></div> 
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
  <script>
    var notify = '' == true ? true : false;
    var verify = '' == true ? true : false;
      window.onload = function() {
          new Valine({
              el: '#vcomment',
              app_id: "AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz",
              app_key: "kMsGLN3hzkQJuLrmqQBgquFF",
              placeholder: "说点什么",
              avatar:"retro",
              visitor: true       

          });
      }
  </script>

 
  <noscript>Please enable JavaScript to view the comments</noscript>



  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  




  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  
    
  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  
      <script>
        MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>

  <script defer src="/js/leancloud.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/yinghua.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiantiao.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiaoxingxing.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
