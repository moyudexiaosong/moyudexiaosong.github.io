

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.jpg">
  <link rel="icon" href="/img/icon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="一、绪论这章介绍了专用名词和两个原理。 数据集里一个样本在属性空间里被称为一个特征向量，每个特征对应一个维度，维度的总数称为维数。 从数据中学得模型的过程叫做“学习”和“训练”，学到的潜在规律叫做“假设”，训练结果作为标记，所有标记的集合叫做“标记空间”或者“输出空间”。 预测值为离散变量时称为“分类”，预测值为连续变量时称为“回归”，将训练集中样本按照某种内在规律分成若干组，每组称为一个“簇”">
<meta property="og:type" content="article">
<meta property="og:title" content="西瓜书重刷">
<meta property="og:url" content="http://example.com/2022/10/10/%E5%91%A8%E5%BF%97%E5%8D%8E%E9%87%8D%E5%88%B7%E2%80%9C/index.html">
<meta property="og:site_name" content="摸鱼之家">
<meta property="og:description" content="一、绪论这章介绍了专用名词和两个原理。 数据集里一个样本在属性空间里被称为一个特征向量，每个特征对应一个维度，维度的总数称为维数。 从数据中学得模型的过程叫做“学习”和“训练”，学到的潜在规律叫做“假设”，训练结果作为标记，所有标记的集合叫做“标记空间”或者“输出空间”。 预测值为离散变量时称为“分类”，预测值为连续变量时称为“回归”，将训练集中样本按照某种内在规律分成若干组，每组称为一个“簇”">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/photo37.jpeg">
<meta property="article:published_time" content="2022-10-10T02:22:53.000Z">
<meta property="article:modified_time" content="2022-10-19T12:43:39.855Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="机器学习基础理论">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/photo37.jpeg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>西瓜书重刷 - 摸鱼之家</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/bynotes/texiao/source/css/shubiao.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.1","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":false},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz","app_key":"kMsGLN3hzkQJuLrmqQBgquFF","server_url":"https://awhbuajs.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>快乐老家</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                主页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                档案馆
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                目录
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/photo37.jpeg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="西瓜书重刷"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-10-10 10:22" pubdate>
          October 10, 2022 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          24k words
        
      </span>
    

    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> views
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">西瓜书重刷</h1>
            
              <p class="note note-info">
                
                  
                    Last updated on 14 days ago
                  
                
              </p>
            
            <div class="markdown-body">
              
              <hr>
<h3 id="一、绪论"><a href="#一、绪论" class="headerlink" title="一、绪论"></a>一、绪论</h3><p>这章介绍了专用名词和两个原理。</p>
<p><strong>数据集</strong>里一个<strong>样本</strong>在<strong>属性空间</strong>里被称为一个<strong>特征向量</strong>，每个<strong>特征</strong>对应一个<strong>维度</strong>，维度的总数称为<strong>维数</strong>。</p>
<p>从数据中学得模型的过程叫做“<strong>学习</strong>”和“<strong>训练</strong>”，学到的潜在规律叫做“<strong>假设</strong>”，训练结果作为<strong>标记</strong>，所有标记的集合叫做“<strong>标记空间</strong>”或者“<strong>输出空间</strong>”。</p>
<p>预测值为离散变量时称为“<strong>分类</strong>”，预测值为连续变量时称为“<strong>回归</strong>”，将训练集中样本按照某种内在规律分成若干组，每组称为一个“簇”，叫做“<strong>聚类</strong>”。</p>
<p>通过有标记的训练样本学习叫做“<strong>监督学习</strong>”，通过无标记的训练样本学习叫做“<strong>无监督学习</strong>”。分类和回归属于前者，聚类属于后者。</p>
<p>假设空间指一个<strong>机器学习算法可以生成的所有函数的集合</strong>。</p>
<p><u>奥卡姆剃刀原理：若有多个假设和观察一致，选最简单的那个。</u>在曲线中，平滑意味着简单。</p>
<p><u>没有免费的午餐NFL定理：无论学习算法的“聪明”或“笨拙”与否，它们的期望性能都相同。</u>但这是建立在所有问题机会均等的条件下的，现实中我们肯定只会关心我们需要关心的问题，因此肯定有算法会在某个问题上表现的更好，我们要做的就是找到它。</p>
<h3 id="二、模型评估与选择"><a href="#二、模型评估与选择" class="headerlink" title="二、模型评估与选择"></a>二、模型评估与选择</h3><p>“模型选择”问题最理想的解决方案是对候选模型的泛化性能进行评估，选择性能最好的那个。泛化性能的评估不仅需要<strong>实验估计方法</strong>，还需要有衡量泛化能力的<strong>评判标准</strong>。</p>
<h4 id="实验估计方法"><a href="#实验估计方法" class="headerlink" title="实验估计方法"></a>实验估计方法</h4><h5 id="留出法（hold-out）"><a href="#留出法（hold-out）" class="headerlink" title="留出法（hold-out）"></a>留出法（hold-out）</h5><p>将$D$划分为互斥的两部分$S和T$，在$S$上训练出模型后，用$T$来评估测试误差，划分时采用<strong>分层采样</strong>，保证训练集和测试集中正反类的比例一致。</p>
<p>为了平衡模型的准确性与测试结果的保真性，常见做法是将大约 2/3～ 4/5 的样本用于训练，剩余样本用于测试。</p>
<h5 id="交叉验证法（cross-validation）"><a href="#交叉验证法（cross-validation）" class="headerlink" title="交叉验证法（cross validation）"></a>交叉验证法（cross validation）</h5><p>将数据集 $D $划分为$k$个互斥子集，即$D=D_1\cup D_2\cup ……\cup D_k$，$D_i\cap D_j=\empty $，每次用$k-1 $个子集的并集作为训练集，余下的那个子集作为测试集；对获得的 $k$组训练/测试集进行 $k$ 次训练和测试，得到这 $k$ 个测试结果的均值。</p>
<p><img src="/../img/image-20221011194409037.png" srcset="/img/loading.gif" lazyload alt="image-20221011194409037" style="zoom:50%;"></p>
<p>交叉验证法的一 个特例：留一法(Leave-One-Out，简称 LOO) </p>
<p>假定数据集 $D$ 中包含 $m$ 个样本，令 $k=m$ ，即每个子集只包含一个样本。</p>
<h5 id="自助法（bootstrapping）"><a href="#自助法（bootstrapping）" class="headerlink" title="自助法（bootstrapping）"></a>自助法（bootstrapping）</h5><p>从$D$中随机复制一个样本到$D’$中，复制$m$次，会得到一个拥有$m$个数据样本的$D’$数据集。</p>
<p>根据平均分布概率计算，大约有36.8%的数据不会被复制到$D’$中，因此$D$覆盖的样本要多于$D’$，把$D’$拿来当训练集，$D$用来当测试集。这样的测试结果，亦称”包外估计” (out-of-bag estimate)。</p>
<blockquote>
<p>样本在 $m$ 次采样中始终不被采到的概率是 $(1-\frac{1}{m})^m$， 取极限得到$\lim_{m\to \infty}(1-\frac{1}{m})^m \to \frac{1}{e} \approx. 0.368$</p>
</blockquote>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>自助法在<strong>数据集较小、难以有效划分训练/测试集时</strong>很有用，另外两个都是采用分层采样的。</p>
<p>自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，<u>在初始数据量足够时，留出法和交叉验证法更常用一些。</u></p>
<p>留出法和交叉验证法由于都把数据集中的一部分用来测试，没有参与训练，因此必然会引入一些因训练样本规模不同而导致的估计偏差，留一法受影响较小，但计算复杂度又太高了。</p>
<h4 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h4><p>一般参数分两种：一类是算法的参数，亦称“超参数”；一类是模型的参数，如神经网络中每个节点的权重。前者多为人工选择一组候选，后者是让机器自己学习，常用的做法是对每个参数选定一个范围和变化步长，这种做法选出的参数往往不是最佳的，但却是在计算开销与性能估计之间折中的最好结果。</p>
<p>一般调参过程中，会将训练数据再次划分为训练集和验证集(validation set)，训练集用来训练出模型，验证集用来进行模型评估与选择，对比不同算法的泛化性能。</p>
<p><u>验证集和测试集有啥区别？</u></p>
<p>使用验证集是为了快速调参，也就是用验证集选择超参数（网络层数，网络节点数，迭代次数，学习率这些）。另外用验证集还可以监控模型是否异常（过拟合啦什么的），然后决定是不是要提前停止训练。</p>
<p>验证集的关键在于选择超参数，我们手动调参是为了让模型在验证集上的表现越来越好，如果把测试集作为验证集，调参去拟合测试集，就有点像作弊了。</p>
<p>而测试集既不参与参数的学习过程，也不参与参数的选择过程，仅仅用于模型评价。</p>
<p><u>验证集的正确打开方式：</u></p>
<p>验证集可以看做参与了 “人工调参” 的训练过程。一般训练几个 epoch 就跑一次验证看看效果（大部分网络自带这个功能）。</p>
<p>这样做的第一个好处是：可以及时发现模型或者参数的问题，比如模型在验证集上发散啦、出现很奇怪的结果啦（Inf）、mAP不增长或者增长很慢啦等等情况，这时可以及时终止训练，重新调参或者调整模型，而不需要等到训练结束。</p>
<p>另一个好处是验证模型的泛化能力，如果在验证集上的效果比训练集上差很多，就该考虑模型是否过拟合了。同时，还可以通过验证集对比不同的模型。</p>
<h4 id="评判标准"><a href="#评判标准" class="headerlink" title="评判标准"></a>评判标准</h4><p>模型好坏是相对的，任务不同，使用的性能度量也不同，以下是几种常用的：</p>
<p>在预测任务中，给定样例集 $D = {(x_1 , y_1) , (x_2 ， y_2)， . . . , (x_m, y_m)} $， 其中 $y_i$ 是示例 $x_i$ 的真实标记。要评估学习器 $f$ 的性能，就要把学习器预测结果 $f(x)$  与真实标记  $y$ 进行比较。</p>
<h5 id="1-均方误差：回归任务常用"><a href="#1-均方误差：回归任务常用" class="headerlink" title="1.均方误差：回归任务常用"></a>1.均方误差：回归任务常用</h5><p>离散变量：</p>
<script type="math/tex; mode=display">
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(xi)-yi)^2</script><p>连续变量：</p>
<script type="math/tex; mode=display">
E(f;Ð)=\int_{x～Ð}(f(x)-y)^2p(x)dx</script><h5 id="2-错误率与精度：分类任务常用"><a href="#2-错误率与精度：分类任务常用" class="headerlink" title="2.错误率与精度：分类任务常用"></a>2.错误率与精度：分类任务常用</h5><p>离散变量：</p>
<script type="math/tex; mode=display">
错误率：E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\amalg(f(xi)\ne yi)</script><script type="math/tex; mode=display">
精度：acc(f;D)=\frac{1}{m}\sum_{i=1}^{m}\amalg(f(xi)= yi)=1-E(f;D)</script><blockquote>
<p>$\amalg$是指示函数（indicator function），含义是：当输入为True的时候，输出为1，输入为False的时候，输出为0。</p>
</blockquote>
<p>连续变量：</p>
<script type="math/tex; mode=display">
错误率：E(f;Ð)=\int_{x～Ð}\amalg(f(x)\ne y)p(x)dx</script><script type="math/tex; mode=display">
精度：acc(f;Ð)=\int_{x～Ð}\amalg(f(x)= y)p(x)dx=1-E(f;Ð)</script><h5 id="3-查准率、查全率与-F1"><a href="#3-查准率、查全率与-F1" class="headerlink" title="3.查准率、查全率与$F1$"></a>3.查准率、查全率与$F1$</h5><p>查准率和查全率可以理解为”检索出的信息中有多少比例是用户感兴趣的” 和”用户感兴趣的信息中有多少被检索出来了”。</p>
<p>查准率 $P $ 与查全率 $R$ 分别定义为：</p>
<p>$P=\frac{TP}{TP+FP}$  =  真正例 / 预测出的正例</p>
<p>$R=\frac{TP}{TP+FN}$  =  真正例 / 实际上的正例</p>
<p>二者通常呈反比。</p>
<p><img src="/../img/image-20221011214808636.png" srcset="/img/loading.gif" lazyload alt="image-20221011214808636" style="zoom:33%;"></p>
<p>在进行比较时，若一个学习器的P-R曲线B把另一个学习器C的曲线完全“包住”，则可断言B的性能优于C。很好理解，查全率一致的情况下，查准率越好的学习器性能越好。因此，ROC曲线越接近左上角，分类效果越好。</p>
<p>当曲线有相交时，就很难断言两者的优劣。平衡点(Break-Event Point，简称 BEP)是” 查准率=查全率”时的取值，例如图中基于 BEP的比较，可认为学习器 A 优于 B。</p>
<p>但 BEP 还是过于简化了些，更常用的是 F1 度量，基于查准率与查全率的调和平均 (harinonic mean)定义：</p>
<script type="math/tex; mode=display">
\frac{1}{F1}=\frac{1}{2}.(\frac{1}{P}+\frac{1}{R})</script><p>为了表达出对查准率/查全率的不同偏好，定义了$F1 $度量的一般形式：</p>
<script type="math/tex; mode=display">
F_\beta=\frac{(1+\beta^2)\times P \times R}{(\beta^2 \times P)+R}</script><p>$ß&gt; 1$ 时查全率有更大影响 ; $ß &lt; 1$ 时查准率有更大影响。</p>
<blockquote>
<p>混淆矩阵就是TP、TN、FP、FN组成的2x2矩阵。</p>
</blockquote>
<p>有多个二分类混淆矩阵的情况，例如：</p>
<ul>
<li>进行多次训练/测试，每次得到一个混淆矩阵。</li>
<li>在多个数据集上进行训练/测试，希望估计算法的”全局” 性能。</li>
<li>多分类任务每两两类别的组合都对应一个混淆矩阵。</li>
</ul>
<p>有两种做法：</p>
<p>一、在各混淆矩阵上分别计算出查准率和查全率，记为 $(Pn ， Rn)$ ，再计算平均值，这样就得到”宏查准率” ($macro-P$) 、”宏查全率” ($macro-R$) ，以及相应的”宏 $F1$” ($macro-F1$):</p>
<script type="math/tex; mode=display">
macro-P=\frac{1}{n}\sum^n_{i=1}P_i</script><script type="math/tex; mode=display">
macro-R=\frac{1}{n}\sum^n_{i=1}R_i</script><script type="math/tex; mode=display">
macro-F1=\frac{2\times macro-P \times macro-R}{macro-P+macro-R}</script><p>二、求各混淆矩阵 $TP 、FP 、 TN 、FN$ 的平均值，分别记为 $\overline{TP}、 \overline{FP}、 \overline{TN}、 \overline{FN}$，再基于这些平均值计算出”微查准率 “($micro-P$) 、 “微查全率” ($micro-R$)和”微F1” ($micro-F1$):</p>
<script type="math/tex; mode=display">
micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}</script><script type="math/tex; mode=display">
micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}</script><script type="math/tex; mode=display">
micro-F1=\frac{2\times micro-P\times micro-R}{micro-P+micro-R}</script><p>以此综合考虑这个模型的查准率、查全率及二者的综合$F_1$。</p>
<h5 id="4-ROC-与-AUC"><a href="#4-ROC-与-AUC" class="headerlink" title="4.ROC 与 AUC"></a>4.ROC 与 AUC</h5><p>ROC：“受试者工作特征”（receiver operating characteristic）曲线，取用“真正率”（TPR ）作为纵轴，“假正率”（FPR）作为横轴 ，两者分别定义为：</p>
<script type="math/tex; mode=display">
TPR=\frac{TP}{TP+FN},FPR=\frac{FP}{TN+FP}</script><p>学习器得出的预测通常会设置一个阈值来做分类，我们对这些测试样本进行排序，最可能是正类的排在前面，最不可能是正类的排在后面，以某个截断点将这些样本分为两部分，重视准确率就采用靠前的截断点，重视“查全率”就采用靠后的截断点。</p>
<p>显然，假正率越小越好，真正率越大越好。</p>
<p>体现在ROC图中就是把样本排序，依次选择它们作为阈值（或称为“截断点”），画出全部的关键点以后，再连接关键点即可最终得到ROC曲线，如下图所示。</p>
<p><img src="/../img/webp" srcset="/img/loading.gif" lazyload alt="img" style="zoom: 33%;"></p>
<p>若要把两个学习器进行比较，简单方法是如果一个学习器A的 ROC 曲线把另一个学习器B的曲线完全”包住”， 则可断言A的性能优于B；较为合理的判据是比较 ROC 曲线下的面积，即 AUC (Area Under ROC Curve) 。</p>
<script type="math/tex; mode=display">
AUC=\frac{1}{2}\sum^{m-1}_{i=1}(x_{i+1}-x_i).(y_i+y_{i+1})</script><p>AUC考虑的是样本预测的排序质量，显然是越大越好。</p>
<h5 id="5-代价敏感错误率与代价曲线"><a href="#5-代价敏感错误率与代价曲线" class="headerlink" title="5.代价敏感错误率与代价曲线"></a>5.代价敏感错误率与代价曲线</h5><p>现实任务中，不同类型的错误所造成的后果不同，设定一个”代价矩阵” (cost matrix) ，其中 $cost_{ij} $表示将第$ i $类样本预测为第 $j $类样本的代价， $cost_{ii} = 0$​，我们的目标是最小化“总体代价”，以二分类任务为例，期望总体代价为：</p>
<script type="math/tex; mode=display">
E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in D^+}I(f(x_i)\ne y_i)\times cost_{01}+\sum_{x_i\in D^-}I(f(x_i)\ne y_i)\times cost_{10})</script><h5 id="6-注意事项"><a href="#6-注意事项" class="headerlink" title="6.注意事项"></a>6.注意事项</h5><p>（1）loss函数往往要求有更好的数学性质，比如连续、可导、微分，性能指标常常因为不可微分，无法作为优化的loss函数，因此多采用如cross-entropy, rmse等“距离”可微函数作为优化目标。</p>
<p>（2）实际中还需要考虑时间开销，存储开销和可解释性等问题。比如DDoS防御中实际上最困难的不是检测到DDoS攻击，而是准确的，高性能的检测DDoS流量，并进行清洗。</p>
<p>（3）医学，信息检索，web搜索引擎，网络安全等领域都有自己的专用术语，但都与传统的机器学习的名词概念有所重合，只是叫法不同。</p>
<p>以上指标在sklearn库里基本都有现成的模块可以直接使用，相关API可查阅doc，可参考<a target="_blank" rel="noopener" href="https://yq.aliyun.com/articles/623375">https://yq.aliyun.com/articles/623375</a></p>
<h4 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_45666566/article/details/106674542">比较检验</a></h4><p>在测得学习器的性能度量后，如何进行比较比我们想象中的复杂得多，存在着多个问题：</p>
<ol>
<li>我们要的是<strong>泛化性能</strong>，但是能用的只是测试性能；</li>
<li>测试性能和测试集的选择对测试结果有很大影响；</li>
<li>有的机器学习算法本身也有随机性，对同一测试集的多次测试结果也可能会不同。</li>
</ol>
<p>为了解决这些问题，统计假设检验（hypothesis test）为我们提供了重要的依据。<strong>基于假设检验结果，我们可以推断出，若在测试集上观察到模型A优于B，则A的泛化性能是否在统计意义上也优于B，以及做这个结论的把握有多大。</strong></p>
<p>在数理统计中，假设检验是根据一定条件由样本推断总体的一种方法。</p>
<p>几个基础概念：</p>
<ul>
<li><strong>置信度</strong>：表示有多大的把握认为假设是正确的。</li>
<li><strong>显著度</strong>：也称“显著性水平”，表示假设出错的概率。显著度越大，假设被拒绝的可能性越大。</li>
<li><strong>自由度</strong>：不被限制的样本数，也可以理解为能自由取值的样本数，记为 vvv 或 dfdfdf。</li>
</ul>
<p>看不懂，不看了。</p>
<p>参考阅读：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_63309778/article/details/124309616">https://blog.csdn.net/m0_63309778/article/details/124309616</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/57866029">https://zhuanlan.zhihu.com/p/57866029</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54178491">https://zhuanlan.zhihu.com/p/54178491</a></p>
<h4 id="偏差—方差分解"><a href="#偏差—方差分解" class="headerlink" title="偏差—方差分解"></a>偏差—方差分解</h4><p>偏差——方差分解是解释学习算法泛化性能的一种重要工具，同时也对如何在模型的拟合能力和复杂度之间取得一个较好的平衡问题提供了一个很好的分析和指导。</p>
<p>省略推导过程，偏差——方差分解是对学习算法的期望泛化错误率的分解：</p>
<p>$E(f;D)=bias^2(x)+var(x)+\varepsilon^2$</p>
<p>也就是说，泛化误差可分解为偏差、方差与噪声之和，三者共同决定泛化误差。</p>
<p>这三者的含义是这样的：</p>
<ul>
<li><p>偏差：度量学习算法的期望预测与真实结果的偏离程度，代表学习算法本身的拟合能力；</p>
</li>
<li><p>方差：度量了训练集变动所导致的学习性能的变化，代表数据扰动所造成的影响；</p>
</li>
<li><p>噪声：表达当前任务上任何学习算法所能达到的期望泛化误差的下界，代表学习问题本身的难度；</p>
</li>
</ul>
<p>在给定学习任务的条件下，为了取得好的泛化性能，偏差和方差越小越好。但这两者其实是有冲突的，这称为偏差-方差窘境（bias-variance dilemma）。</p>
<p>给定学习任务，训练不足时，拟合能力不够强，训练数据的扰动对学习器的影响不大，此时偏差主导了泛化错误率； 随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率； 在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身没什么用的特性也被学习器学到了，则将发生过拟合。</p>
<p>过拟合的意思就是模型对训练集中所有的样本点都有非常好的预测能力，但是对于非训练集中的新数据预测能力非常差。<strong>为了防止过拟合现象的出现，就要降低决策函数的复杂度，通常的做法是给经验风险后面加一个<u>正则化项</u>（惩罚项）</strong>，这个后面章节有。</p>
<p>以上是从回归任务中推导得出的公式和结论，告诉我们训练的太过容易发生过拟合。对于分类任务，由于0/1损失函数的跳变性，理论上推导出偏差和方差很困难，但是可以通过实验来估计。</p>
<h3 id="三、线性模型"><a href="#三、线性模型" class="headerlink" title="三、线性模型"></a>三、线性模型</h3><p>线性模型的本质就是通过训练数据学习出一个通过样本数据的属性的线性组合来进行预测的函数。</p>
<p>其中参数$w,b$分别代表特征的权重和偏执系数，如果训练样本共有 $d$ 个属性，将描述为：$x=(x_1;x_2;……;x_d)$ 。</p>
<p>线性模型试图学得$x$的预测函数，即$f(x)=w_1x_1+w_2x_2+……+w_dx_d+b$ ，一般用向量形式写成$f(x)=w^Tx+b$ ，其中$w=(w_1;w_2;…;w_d;b)$ 。</p>
<p>本章介绍几种经典的线性模型，先从回归任务开始，然后讨论二分类和多分类任务。</p>
<h4 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h4><p>线性回归预测实值不止可以用于连续属性，也可以用于离散属性。</p>
<p>对离散属性，若属性值间存在”<strong>序</strong>“ (order)关系，可通过连续化将其转化为连续值，例如二值属性”身高”的取值——“高” “矮”可转化为 {1,0}，三值属性”高” “中” “低”可转化为 {1, 0.5, 0}。</p>
<p>若属性值间不存在序关系，则通常转化为 $k$ 维向量，例如属性”瓜类”的取值”西瓜”、”南瓜”、”黄瓜”可转化为(0,0,1)、(0,1,0)、(1,0,0)。</p>
<h5 id="最小二乘"><a href="#最小二乘" class="headerlink" title="最小二乘"></a>最小二乘</h5><p>确定W和b的方法通常是用它模型分布的某个参数来构造损失函数，均方误差是回归任务中最常用的衡量参数，也就是我们常说的”欧式距离“和“平方损失”。</p>
<script type="math/tex; mode=display">
(w^*,b^*)=arg\min_{(w,b)}\sum^m_{i=1}(f(x_i)-y_i)^2=arg\min_{(w,b)}\sum^m_{i=1}(y_i-wx_i-b)^2</script><p>这种基于均方误差最小化来求解模型的方法称为“最小二乘法”，可将上式对 $ω $和 $b$ 分别求导，得到</p>
<script type="math/tex; mode=display">
s.t. \frac{\partial{E_{w,b}}}{\partial w}=2(w\sum^m_{i=1}x_i^2-\sum^m_{i=1}x_i(y_i-b))=0</script><script type="math/tex; mode=display">
s.t.\frac{\partial{E_{w,b}}}{\partial b}=2(mb-\sum^m_{i=1}(y_i-wx_i))=0</script><p>得到$w$和$b$最优解的闭式(解析)解：</p>
<script type="math/tex; mode=display">
w=\frac{\sum^m_{i=1}y_i(x_i-\overline{x})}{\sum^m_{i=1}x_i^2-\frac{1}{m}(\sum^m_{i=1}x_i)^2}</script><script type="math/tex; mode=display">
b=\frac{1}{m}\sum^m_{i=1}(y_i-wx_i)</script><blockquote>
<p><strong>数值解是在特定条件下通过近似计算得出来的一个数值，而解析解就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值。</strong>eg: $x^2=2$   $\rightarrow x=sqrt(2)$ — <strong>（解析解）</strong></p>
<p>$\rightarrow x=1.414$ — <strong>（数值解）</strong></p>
</blockquote>
<p>上述过程对应的是一个样本只有一个特征的问题，当问题中存在的不止一个特征，我们称之为多元线性回归。</p>
<p>形式与上述过程类似。把$w,b$吸入向量模式，表示为$\hat{w}=(w;b)$，然后把容量为$m$的数据集$D$表示为一个$m*(d+1)$的大小矩阵，每个样本就是其中的一行$x$，$d$对应属性个数，最后一列元素置为$1$，用来乘偏置系数，即</p>
<script type="math/tex; mode=display">
X=\left|
    \begin{matrix}
    x_{11} & x_{12} & \cdots & x_{1d} & 1\\
    x_{21} & x_{22} & \cdots & x_{2d} & 1\\
    \vdots & \vdots & \ddots & \vdots & \vdots\\
    x_{m1} & x_{m2} & \cdots & x_{md} & 1 \\
    \end{matrix}
    \right|=\left|
    \begin{matrix}
    x_1^T &1\\
    x_2^T &1\\
    \vdots & \vdots\\
    x_m^T &1\\
    \end{matrix}
    \right|</script><p>再把标记也写成向量形式$y=(y_1;y_2;…;y_m)$，则类似于单变量线性回归模型，有</p>
<script type="math/tex; mode=display">
\hat{w}^*=argmin_{\hat{w}}(y-X\hat{w})^T{}(y-X\hat{w})</script><p>令$E_\hat{w}=(y-X\hat{w})^T{}(y-X\hat{w})$，对$\hat{w}$求导得到：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{\hat{w}}}{ \partial\hat w}=2X^T(X\hat w-y)</script><p>假设$E_\hat{w}$为满秩矩阵或正定矩阵时<script type="math/tex">\hat{w^*}=(X^TX)^{-1}X^Ty</script>，回归模型为<script type="math/tex">f(\hat{x_i})=\hat{x_i}(X^TX)^{-1}X^Ty</script>。 <strong>【注】现实任务中，会遇到大量的变量，甚至会超过样本数，导致列数多于行数，<script type="math/tex">X^TX</script>不满秩，所以会有多个$\hat{w}$，此时结果由算法的学习偏好决定。</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导法则</a></p>
<p>几个公式：</p>
<p><img src="/../img/image-20221014152313021.png" srcset="/img/loading.gif" lazyload alt="image-20221014152313021" style="zoom:50%;"></p>
<p>其中， a 为常数向量， a=(a1,a2,⋯,an)T 。</p>
<p><img src="/../img/image-20221014152338461.png" srcset="/img/loading.gif" lazyload alt="image-20221014152338461" style="zoom:50%;"></p>
<p><img src="/../img/image-20221014153030829.png" srcset="/img/loading.gif" lazyload alt="image-20221014153030829" style="zoom:50%;"></p>
<p><img src="/../img/image-20221014153049266.png" srcset="/img/loading.gif" lazyload alt="image-20221014153049266" style="zoom:50%;"></p>
<h5 id="联系函数"><a href="#联系函数" class="headerlink" title="联系函数"></a>联系函数</h5><p>有时候$y$和$x$之间不是线性变化，这就导致$y=w<em>x+b$​不能再表示他们之间的关系，就需要通过一个“<em>*联系函数</em></em>”让他们可以线性表示，考虑单调可微函数$ g(.) $， 令</p>
<script type="math/tex; mode=display">
y=g^{-1}(w^Tx+b)</script><p>$g(.) $连续且充分光滑，这样得到的模型称为”广义线性模型” (generalized linear model) ，其中函数$g(.)$ 就称为”联系函数” (link function)。</p>
<p>这实际上是一种空间映射，起到了<u>将线性回归模型的预测值与真实标记联系起来的作用</u>。</p>
<h5 id="对数几率函数sigmoid"><a href="#对数几率函数sigmoid" class="headerlink" title="对数几率函数sigmoid"></a>对数几率函数sigmoid</h5><p>之前讲的是回归学习，训练模型来预测出一个实值，如果要做分类任务的话，就要加一个联系函数把预测值和真实输出标签[0,1]联系起来，最理想的是”单位阶跃函数”。</p>
<script type="math/tex; mode=display">
y=
\left\{
\begin{array}{c}
    0,z<0\\
    0.5,z=0\\
    1,z>0
\end{array}
\right.</script><p>但是它并不连续，需要用对数几率函数这种近似单位阶跃函数且单调可微的函数来作为其替代函数。</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-z}}</script><p>将线性函数代入对数几率函数并写成如下形式：</p>
<script type="math/tex; mode=display">
ln\frac{y}{1-y}=-z=w^Tx+b</script><p>y是正例的概率，1-y就是反例的概率，两者的比值就是“<strong>几率</strong>”，反映了x作为正例的<strong>相对可能性</strong>，对这个几率取的对数就叫做对数几率。</p>
<p>我们所要做的就是让预测输出，也就是右边部分，尽可能的去逼近真实标签y，由此就用极大似然法得到了目标函数，一个任意阶可导的凸函数，拥有非常好的数学性能。</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-(w^Tx+b)}}</script><p>求解<script type="math/tex">w,b</script>用了<strong>极大似然法</strong>，最后得到一个关于β的高阶可导连续凸函数</p>
<script type="math/tex; mode=display">
l(β)=\sum_{i=1}^m(-y_iβ^T\hat{x_i}+\ln(1+e^{β^T\hat{x_i}}))</script><p>通过梯度下降法、牛顿法等都可求得这个函数的最优解<script type="math/tex">β^*=argmin_βl(β)</script>，逐步迭代找到最优下降路径。</p>
<h5 id="线性判别分析（LDA）"><a href="#线性判别分析（LDA）" class="headerlink" title="线性判别分析（LDA）"></a>线性判别分析（LDA）</h5><p>讲正例尽量投在一起，反例投一起，正反例尽可能远离，拿到新样例投上去看更接近哪。</p>
<p><img src="/../img/image-20221016130515153.png" srcset="/img/loading.gif" lazyload alt="image-20221016130515153" style="zoom: 33%;"></p>
<p>椭圆表示数据簇的外轮廓，虚线表示投影， 红色实心园和实心三角形分别表示两类样本投影后的中心点。</p>
<p>LDA常常用于二分类问题以及多分类的降维处理，举例如图本来一个二维数据投到直线上就变成一个一维数据了，LDA的基本思想就是找到一条这样可以将两类数据分开到两边的直线，并确定一个分界点。</p>
<p><img src="/../img/image-20221016130906359.png" srcset="/img/loading.gif" lazyload alt="image-20221016130906359"></p>
<p>如图，右边天蓝色的分界点就做的很好，左图就做不到。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41876213/article/details/107695663">PCA降维</a></p>
<p>这里引入协方差矩阵的概念，若要做数据降维，直接简单粗暴的删掉一维数据必然不合理，因此有两种方案：</p>
<ol>
<li><p>先看每一维数据的方差，对于分散程度较小的因素而言(极端情况为一个不变的数值)说明分析的价值不大，所以可以舍弃。</p>
</li>
<li><p>但是由于各个特征之间可能是充满联系的，比如说空气质量可能会影响上班时间，一些方差小的原因可能不是该因素本身的原因，而是其它因素影响造成的，需要将这些因素之间的影响去除掉，在除掉之前先引进协方差的概念，将各维数据去中心化后得到：</p>
<p><img src="/../img/image-20221016131555712.png" srcset="/img/loading.gif" lazyload alt="image-20221016131555712" style="zoom:50%;"></p>
<p>大于0正相关，等于0不相关，小于0负相关。协方差矩阵对角线位置上的数值表示方差，非对角线位置上的数值表示各因素之间的相关性。</p>
<p><img src="/../img/image-20221016131717029.png" srcset="/img/loading.gif" lazyload alt="image-20221016131717029"></p>
</li>
</ol>
<p>为使各个因素之间不相关，需要一种方法对各个因素进行<strong>坐标转换</strong>使协方差矩阵转换为对角矩阵，<strong>这就是我们的求解目标。</strong></p>
<p>在m维空间中，令$P=[e_1 e_2 … e_m]$，求$x$在$P$组成的m维标准正交基中的坐标公式如下：</p>
<script type="math/tex; mode=display">
[e_1 e_2 ... e_m]^T x=P^Tx</script><p>保留特征值较大的特征向量，不知道降低到多少维合适时一般按照经验判断保留到特征值所在比例为95%左右，有时候为了可视化比例可能远低于这个数值，最终降维之后的坐标为$y_j =P’^Tx_j$。</p>
<p><strong>二分类线性判别分析</strong></p>
<p>通过尽量增大两类样本中心点的距离，并尽量减小正类或反类样本各自内部的协方差，得到一个“广义瑞利商”，这就是LDA想要最大化的目标。</p>
<p>LDA可以从贝叶斯决策论理论的角度来阐述，并可证明，当两类数据同先验、满足高斯分布并且协方差相等时，LDA可达到最优分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA</span>(<span class="hljs-params">data, target, n_dim</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    :param data: (n_samples, n_features)</span><br><span class="hljs-string">    :param target: data class</span><br><span class="hljs-string">    :param n_dim: target dimension</span><br><span class="hljs-string">    :return: (n_samples, n_dims)</span><br><span class="hljs-string"> 	&#x27;&#x27;&#x27;</span>   <br>    clusters = np.unique(target)<br><br>    <span class="hljs-keyword">if</span> n_dim &gt; <span class="hljs-built_in">len</span>(clusters)-<span class="hljs-number">1</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;K is too much&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;please input again&quot;</span>)<br>        exit(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment">#within_class scatter matrix</span><br>    Sw = np.zeros((data.shape[<span class="hljs-number">1</span>],data.shape[<span class="hljs-number">1</span>]))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> clusters:<br>        datai = data[target == i]<br>        datai = datai-datai.mean(<span class="hljs-number">0</span>)<br>        Swi = np.mat(datai).T*np.mat(datai)<br>        Sw += Swi<br><br>    <span class="hljs-comment">#between_class scatter matrix</span><br>    SB = np.zeros((data.shape[<span class="hljs-number">1</span>],data.shape[<span class="hljs-number">1</span>]))<br>    u = data.mean(<span class="hljs-number">0</span>)  <span class="hljs-comment">#所有样本的平均值</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> clusters:<br>        Ni = data[target == i].shape[<span class="hljs-number">0</span>]<br>        ui = data[target == i].mean(<span class="hljs-number">0</span>)  <span class="hljs-comment">#某个类别的平均值</span><br>        SBi = Ni*np.mat(ui - u).T*np.mat(ui - u)<br>        SB += SBi<br>    S = np.linalg.inv(Sw)*SB<br>    eigVals,eigVects = np.linalg.eig(S)  <span class="hljs-comment">#求特征值，特征向量</span><br>    eigValInd = np.argsort(eigVals)<br>    eigValInd = eigValInd[:(-n_dim-<span class="hljs-number">1</span>):-<span class="hljs-number">1</span>]<br>    w = eigVects[:,eigValInd]<br>    data_ndim = np.dot(data, w)<br>    <span class="hljs-keyword">return</span> data_ndim<br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    iris = load_iris()<br>    X = iris.data<br>    Y = iris.target<br>    target_names = iris.target_names<br>    X_r2 = LDA(X, Y, <span class="hljs-number">2</span>)<br>    colors = [<span class="hljs-string">&#x27;navy&#x27;</span>, <span class="hljs-string">&#x27;turquoise&#x27;</span>, <span class="hljs-string">&#x27;darkorange&#x27;</span>]<br>    <span class="hljs-keyword">for</span> color, i, target_name <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(colors, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], target_names):<br>        plt.scatter(X_r2[Y == i, <span class="hljs-number">0</span>], X_r2[Y == i, <span class="hljs-number">1</span>], alpha=<span class="hljs-number">.8</span>, color=color,<br>                label=target_name)<br>    plt.legend(loc=<span class="hljs-string">&#x27;best&#x27;</span>, shadow=<span class="hljs-literal">False</span>, scatterpoints=<span class="hljs-number">1</span>)<br>    plt.title(<span class="hljs-string">&#x27;LDA of IRIS dataset by Python&#x27;</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure>
<p>多分类LDA的均值向量仍然是所有样本的均值向量，类内散度矩阵则重定义为每个类别的散度矩阵之和，最后常见的采用的优化目标是最大化</p>
<p><img src="/../img/image-20221016144004468.png" srcset="/img/loading.gif" lazyload alt="image-20221016144004468" style="zoom: 50%;"></p>
<p>看下图，全局均值到类内均值的距离就是$S_b$，类内样本间的距离就是$S_w$，也就是说，尽量增加每一类中心点到全局中心点的距离，尽量缩小每一类内部样本的协方差。</p>
<p><img src="/../img/image-20221016142851974.png" srcset="/img/loading.gif" lazyload alt="image-20221016142851974" style="zoom:33%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#coding=utf-8</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-comment"># 这是sklearn中实现的LDA，待会我们会比较自己实现的LDA和它的区别</span><br><span class="hljs-keyword">from</span> sklearn.discriminant_analysis <span class="hljs-keyword">import</span> LinearDiscriminantAnalysis<br><br><span class="hljs-comment"># k为目标</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">LDA</span>(<span class="hljs-params">X, y, k</span>):<br>    label_ = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(y))<br>    X_classify = &#123;&#125;<br>    <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> label_:<br>        X1 = np.array([X[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X)) <span class="hljs-keyword">if</span> y[i] == label])<br>        X_classify[label] = X1<br><br>    miu = np.mean(X, axis=<span class="hljs-number">0</span>)<br>    miu_classify = &#123;&#125;<br>    <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> label_:<br>        miu1 = np.mean(X_classify[label], axis=<span class="hljs-number">0</span>)<br>        miu_classify[label] = miu1<br><br>    <span class="hljs-comment"># St = np.dot((X - mju).T, X - mju)</span><br>    <span class="hljs-comment"># 计算类内散度矩阵Sw</span><br>    Sw = np.zeros((<span class="hljs-built_in">len</span>(miu), <span class="hljs-built_in">len</span>(miu)))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> label_:<br>        Sw += np.dot((X_classify[i] - miu_classify[i]).T, X_classify[i] - miu_classify[i])<br><br>    <span class="hljs-comment">#Sb = St-Sw</span><br>    <span class="hljs-comment"># 计算类内散度矩阵Sb</span><br>    Sb = np.zeros((<span class="hljs-built_in">len</span>(miu), <span class="hljs-built_in">len</span>(miu)))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> label_:<br>        Sb += <span class="hljs-built_in">len</span>(X_classify[i]) * np.dot((miu_classify[i] - miu).reshape(<br>            (<span class="hljs-built_in">len</span>(miu), <span class="hljs-number">1</span>)), (miu_classify[i] - miu).reshape((<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(miu))))<br><br>    <span class="hljs-comment"># 计算S_w^&#123;-1&#125;S_b的特征值和特征矩阵</span><br>    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))<br>    sorted_indices = np.argsort(eig_vals)<br>    <span class="hljs-comment"># 提取前k个特征向量</span><br>    topk_eig_vecs = eig_vecs[:, sorted_indices[:-k - <span class="hljs-number">1</span>:-<span class="hljs-number">1</span>]]<br>    <span class="hljs-keyword">return</span> topk_eig_vecs<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    iris = load_iris()<br>    X = iris.data<br>    y = iris.target<br><br>    W = LDA(X, y, <span class="hljs-number">2</span>)<br>    X_new = np.dot(X, W)<br>    plt.scatter(X_new[:, <span class="hljs-number">0</span>], X_new[:, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&#x27;o&#x27;</span>, c=y)<br>    plt.show()<br><br>    <span class="hljs-comment"># 和sklearn的函数对比</span><br>    lda = LinearDiscriminantAnalysis(n_components=<span class="hljs-number">2</span>)<br>    lda.fit(X, y)<br>    X_new = lda.transform(X)<br>    plt.scatter(X_new[:, <span class="hljs-number">0</span>], X_new[:, <span class="hljs-number">1</span>], marker=<span class="hljs-string">&#x27;o&#x27;</span>, c=y)<br>    plt.show()<br>main()<br><br></code></pre></td></tr></table></figure>
<h4 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h4><p>多分类学习有两个思路。一种是将二分类算法变形之后推广到多分类，比如上一节讲到的LDA。另一种则是利用二分类的学习器来解决多分类问题。下面讨论第二种，即用拆解法将多分类任务拆为若干个二分类任务求解。</p>
<p><strong>拆解法步骤：</strong></p>
<ol>
<li><p>通过拆分策略对问题进行【<strong>拆分】</strong>；</p>
</li>
<li><p>为拆分出的每个二分类任务【<strong>训练】</strong>一个分类器；</p>
</li>
<li><p>对各个分类器的结果进行【<strong>集成】</strong>，以获得多分类结果。</p>
</li>
</ol>
<p><strong>［1］“一对一”（One vs One，简称OvO）</strong></p>
<p>OvO将所有类别两两配对，一个作为正类，一个作为反类，从而产生$N(N-1)/2$个二分类任务。把最后得到的每个分类结果进行统计，被预测次数最多的类别作为最终结果。</p>
<p><strong>［2］“一对其余”（One vs Rest，简称OvR）</strong></p>
<p>将N个分类中的一个类作为正例，其余均设置为反例，从而【拆分】成N个分类任务；【训练】得到N个分类结果；【集成】的方法是考虑各被判为正例的分类器的置信度，选择置信度大的类别标记作为分类的结果。</p>
<p><img src="/../img/image-20221016145205366.png" srcset="/img/loading.gif" lazyload alt="image-20221016145205366" style="zoom:50%;"></p>
<p>二者对比：OVO的分类器数量很大，存储开销和测试时间开销通常比OvR更大，但是OVO用到的训练样本很少，因此，在类别很多时，OvO的训练时间开销通常比OvR更小。至于预测性能，则取决于具体的数据分布，在多数情形下两者差不多。</p>
<p><strong>［3］“多对多”（Many vs Many，简称MvM）</strong></p>
<p>每次将若干个类别作为正类，若干个其它类作为反类。分类方法采用ECOC编码，不想看了。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lafengxiaoyu/article/details/107886065">ECOC编码详解</a></p>
<h4 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h4><p>目前类别不平衡性学习的一个基本策略是“再缩放”（rescaling），现有技术大体上有以下三种做法：</p>
<p><strong>［1］欠采样（undersampling）</strong></p>
<p>哪类样本多，去掉哪一类，让两类样本数目接近。</p>
<p>代表性算法EasyEnsemble：在反例多的情况下，把反例分给多个学习器，这样每个学习器学到的正反类样本数目就差不多了，而且还不会丢失重要信息。</p>
<p><strong>［2］过采样（oversampling）</strong></p>
<p>哪类样本多，增加哪一类，让两类样本数目接近。</p>
<p>但也不能简单地对初始正例样本进行重复采样，否则会招致严重的过拟合。代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例。</p>
<p><strong>［3］阈值移动（threshold-moving）</strong></p>
<p>一般我们认为正反类发生的概率都是0.5，但是在两类样本不平衡的条件下，分类器决策规则为</p>
<script type="math/tex; mode=display">
\frac{ y'} { (1 - y') }= \frac{y} {(1 - y)} \times \frac{m^-}{m^+} > 1</script><p>时，预测为正例——这就是“再缩放”。</p>
<p>当正类概率大于反类概率时，我们判定结果为正类，也就是：</p>
<p><img src="/../img/image-20221016154116050.png" srcset="/img/loading.gif" lazyload alt="image-20221016154116050" style="zoom:50%;"></p>
<h3 id="四、决策树"><a href="#四、决策树" class="headerlink" title="四、决策树"></a>四、决策树</h3><p>决策树需要分类结果已知的监管学习，决策树在逻辑上以树的形式存在，包含<strong>根节点、内部结点和叶节点</strong>。</p>
<ul>
<li>根节点：包含数据集中的所有数据</li>
<li>内部节点：一个包含符合该判断条件的所有数据的集合。</li>
<li>叶节点：可以得出最终分类结果的节点。</li>
</ul>
<p>一般的原则是，希望通过不断划分节点，使得一个分支节点包含的数据尽可能的属于同一个类别，即“纯度“越来越高，举个例子：</p>
<p><img src="/../img/image-20221016174520050.png" srcset="/img/loading.gif" lazyload alt="image-20221016174520050" style="zoom:50%;"></p>
<p><img src="/../img/image-20221016174536199.png" srcset="/img/loading.gif" lazyload alt="image-20221016174536199" style="zoom:50%;"></p>
<p><img src="/../img/image-20221016174557843.png" srcset="/img/loading.gif" lazyload alt="image-20221016174557843" style="zoom:50%;"></p>
<p><img src="/../img/image-20221016174620982.png" srcset="/img/loading.gif" lazyload alt="image-20221016174620982" style="zoom:50%;"></p>
<p><img src="/../img/image-20221016174637143.png" srcset="/img/loading.gif" lazyload alt="image-20221016174637143" style="zoom:50%;"></p>
<p>可知决策树的生成只需要两步：</p>
<ol>
<li>节点的分裂：当该节点无法给出最终分类结果，会继续生成判断条件。</li>
<li>阈值的确定：选择适当的阈值使得分类错误率最小 （Training Error）。</li>
</ol>
<p>停止分裂的三种情况：</p>
<ul>
<li>没必要分裂：所有样本在当前节点有相同的标签，比如图中分到最后每一类里只有一种类别了。</li>
<li>没办法分裂：当前分布集合是空的或者所有样本在所有的分布都相同</li>
<li>没数据分裂：当前节点是空的或者由父节点直接分配了主标签</li>
</ul>
<p>决策树学习的关键在于如何选择最好的划分属性，即确定这个阈值，使决策树随着划分过程的进行，使得一个分支节点包含的数据尽可能的属于同一个类别，也就是节点的“纯度”能越来越高。</p>
<p>常用的最优属性选取方法有三种：</p>
<ul>
<li>信息增益：在 ID3 决策树中使用</li>
<li>信息增益率：在 C4.5 决策树中使用</li>
<li>基尼系数：在 CART 决策树中使用</li>
</ul>
<h4 id="常用划分准则"><a href="#常用划分准则" class="headerlink" title="常用划分准则"></a>常用划分准则</h4><h5 id="信息增益准则"><a href="#信息增益准则" class="headerlink" title="信息增益准则"></a>信息增益准则</h5><p>我们先对一个节点的纯度进行定义，我们将其称之为<strong>信息熵</strong>，代表着一种信息的混乱程度，当它的确定性越大，它的信息熵就越小。</p>
<p><img src="/../img/image-20221016175531401.png" srcset="/img/loading.gif" lazyload alt="image-20221016175531401" style="zoom:50%;"></p>
<p>其中$p_k$代表当前节点D的数据中第k类样本所占的比例，</p>
<p><img src="/../img/image-20221016181555097.png" srcset="/img/loading.gif" lazyload alt="image-20221016181555097" style="zoom:50%;"></p>
<p>蓝线是基尼系数的分布，黑线是熵的分布，这是一个二分类的布局，最小值为0，最大值为各类概率相等时，混乱程度最大。</p>
<p>定义一个信息增益，$D$代表整个数据集，$D^v$代表第$v$个节点下的数据集。</p>
<p><img src="/../img/image-20221016220629446.png" srcset="/img/loading.gif" lazyload alt="image-20221016220629446" style="zoom:50%;"></p>
<p>光看这个式子可能难以理解，看书上的例子：</p>
<p><img src="/../img/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzU5NTI0ODk3,size_16,color_FFFFFF,t_70.jpeg" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>举例以“a=色泽”的情况下，可以分成三个子集合v={绿，白，黑}，分别有{6,6,5}个样本，没个集合中正例和反例的数量分别代入信息熵可得：</p>
<p><img src="/../img/image-20221017163045713.png" srcset="/img/loading.gif" lazyload alt="image-20221017163045713" style="zoom: 33%;"></p>
<p>最后计算总的<strong>色泽</strong>的信息增益：</p>
<p><img src="/../img/20201123212936414.png" srcset="/img/loading.gif" lazyload alt="img" style="zoom:67%;"></p>
<p>然后继续分裂。</p>
<p>也就是说，对每一个节点信息增益的计算包括：</p>
<ul>
<li>对该层正反例进行计数，算出该层节点的信息熵。</li>
<li>选择下一层的划分属性，对每一个划分属性计算信息熵。</li>
<li>乘以该属性的样本数量占上一层总样本数量的权重，代入信息增益公式。</li>
<li>比较每一种划分属性的信息增益大小，选择最大的为该层划分属性。</li>
</ul>
<p>观察该过程，可以得到：</p>
<ol>
<li>第一项$Ent(D)$是确定的，是整个节点的信息熵，可以当成定值。</li>
<li>$D^v/D$表示分支节点所占比例的大小，数据集越大的分支节点权重越高。</li>
<li>分支节点整体纯度越大，分类越好，信息熵越小，后一项就越小，信息增益Gain就变得越大，所以我们的目标是如何<strong>最大化信息增益</strong>。</li>
</ol>
<p>由此，我们依据这个，<strong>计算以每个属性进行划分子节点得到的信息增益，选择其中最大的作为选择的属性。</strong></p>
<h5 id="信息增益率准则"><a href="#信息增益率准则" class="headerlink" title="信息增益率准则"></a>信息增益率准则</h5><p>当分支节点很多时，每个节点的数据量就会变得很小，得到的Gain值就越大，这样会导致<strong>信息熵准则偏爱那些分类数目较多的属性</strong>。</p>
<p>为了解决该问题，这里引入了信息增益率，定义如下：</p>
<p><img src="/../img/image-20221016224649861.png" srcset="/img/loading.gif" lazyload alt="image-20221016224649861" style="zoom:50%;"></p>
<p>引入修正项IV(a)，它是是分布a的固有属性。属性a的可能取值越多（即V越大），则IV(a)的值通常会越大（以此调整也越大的信息增益），</p>
<p><img src="/../img/image-20221016224832567.png" srcset="/img/loading.gif" lazyload alt="image-20221016224832567" style="zoom:50%;"></p>
<p>需注意，信息增益率对可取值数目较少的属性有所偏好，因此C4.5算法并不是直接根据增益率选择最优划分属性，而是使用了一个启发式：</p>
<ol>
<li>先从候选划分属性中选择出信息增益高于平均水平的几个属性</li>
<li>再从选择出的属性中选择增益率最高的属性作为划分属性</li>
</ol>
<p>即C4.5选择的最优划分属性是<strong>信息增益高于平均水平且信息增益率最大</strong>的属性。</p>
<h5 id="基尼指数-Gini-index"><a href="#基尼指数-Gini-index" class="headerlink" title="基尼指数(Gini index)"></a>基尼指数(Gini index)</h5><p>数据集<em>D</em>的纯度可用基尼值来度量：</p>
<p><img src="/../img/image-20221017165621389.png" srcset="/img/loading.gif" lazyload alt="image-20221017165621389" style="zoom: 50%;"></p>
<p>直观来说，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不同的概率。因此，Gini(D)越小，抽到的样例类别相同的概率越大，数据集D的纯度越高。属性<em>a</em>的基尼指数定义为：</p>
<p><img src="/../img/image-20221017165923354.png" srcset="/img/loading.gif" lazyload alt="image-20221017165923354" style="zoom:50%;"></p>
<p>以西瓜集中“a=触感”举例，基尼值为：</p>
<p><img src="/../img/image-20221017171024738.png" srcset="/img/loading.gif" lazyload alt="image-20221017171024738" style="zoom:50%;"></p>
<p>我们在候选属性中选择使得划分后基尼指数最小的属性作为最优划分属性。</p>
<h4 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h4><p>过拟合</p>
<ul>
<li><p>太多的分支会让数据集在训练集上做的过好，而泛化能力不咋地。</p>
</li>
<li><p>过拟合的风险可以通过剪枝来解决。</p>
</li>
</ul>
<p>剪枝策略</p>
<ul>
<li>前剪枝(Pre-pruning)，在生成过程就对每个节点进行估计，如果不能提高泛化能力，就提前停止分支的增长。</li>
<li>后剪枝(Post-pruning)，在生成决策树之后，再自下而上地对子节点进行考察，如果替换成叶节点可以提高泛化能力，那就替换。</li>
<li>将分支剪除就相当于把它替换成叶节点。</li>
</ul>
<p>如何决定要不要剪枝呢？也就是说，如何判断泛化性能是否能提升呢？</p>
<p>把训练集预留一部分做验证集，这就叫做留出法(hold-out)。具体做法为：</p>
<ol>
<li>把数据集分成两部分训练集和测试集</li>
<li>分层采样(Stratified sampling)，让训练集和测试集有相似的分布</li>
<li>多次随机采样，用平均分数来评价训练集的泛化指标</li>
</ol>
<h5 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h5><p><img src="/../img/5bc728ec80d34.png" srcset="/img/loading.gif" lazyload alt="8.png"></p>
<p>这是原决策树，现在用预剪枝看脐部这个属性节点，把下属节点全标记为叶节点，按正反比率分个类，再把验证集带进去。</p>
<p>对1节点，随便设它为一个分类为正类的叶节点，发现划分正确率仅为42.9%，远没有划分一下正确率高，因此我们还是决定对脐部进行分类。</p>
<p>对2,3,4节点，看看是之前分的非常细的决策树准确率高还是直接分的准确率高，如果是直接分更高或者差不了多少的话，相当于它之前的活就白干了，直接在这分类就行了。</p>
<p><img src="/../img/5bc728ec9e330.png" srcset="/img/loading.gif" lazyload alt="9.png"></p>
<p>得出的结果是1节点要分，2节点别分了，直接确定为一个“好瓜”的叶节点，3节点也别分，直接确定为一个“好瓜”的叶节点，4节点本来就分到头了，不用再看了。</p>
<p>最后得到一棵仅有一层划分的决策树，也叫做“决策树桩”。</p>
<h5 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h5><p><img src="/../img/5bc728ec9d497.png" srcset="/img/loading.gif" lazyload alt="10.png"></p>
<p>预剪枝的顺序是1-2-3-4-5，后剪枝则是5-2-3-4-1。</p>
<p>最终得到的决策树如上图所示，可以看出其保留分支明显多于预剪枝。并且是在生成决策树后再进行剪枝的，因此欠拟合风险很小，泛化性能往往优于预剪枝，但由于其自下而上的进行剪枝，相当于遍历了所有的节点，训练开支要远大于预剪枝。</p>
<h4 id="连续值与缺失值处理"><a href="#连续值与缺失值处理" class="headerlink" title="连续值与缺失值处理"></a>连续值与缺失值处理</h4><h5 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h5><p>前面用的都是“好瓜”“坏瓜”这样的离散值来生成决策树，但现实任务中常会遇到连续属性。</p>
<p>最简单的策略是采用二分法，相当于该连续属性是每次都只有两个取值（小于等于 t 和大于 t）的离散属性，以此来划分样本。</p>
<p>对样本的所有连续值进行排序，依次对连续的两个样本值取中位数，然后对这些候选中位数t选取信息增益最大的一个。</p>
<p><img src="/../img/20190522235120722.png" srcset="/img/loading.gif" lazyload alt="在这里插入图片描述"></p>
<p>需注意，和离散属性只分一次不同，连续属性可多次划分，例如在父节点判断”密度&gt;0.33”之后，子节点还可以判断”密度&gt;0.66”。</p>
<h5 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h5><p>如何在属性值缺失的情况下进行划分属性选择？</p>
<p>为了不浪费数据，将总样本分布视为无缺失分布，用无缺失值的样本所占比例 p 来修正计算出的无缺失样本集的信息增益$Gain(\tilde D, a )$ ，以此来近似为全部样本的信息增益$Gain( D , a )$：</p>
<p><img src="/../img/image-20221017204027501.png" srcset="/img/loading.gif" lazyload alt="image-20221017204027501" style="zoom:50%;"></p>
<p>得到该属性的信息增益后就可以进行与其他属性的比较，选择划分属性。</p>
<p>样本在划分属性上缺失值，如何对样本进行划分？</p>
<p>将缺值样本划入所有子节点，且以不同的概率划分到子节点中，权重大小由$r_v$决定。</p>
<h4 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h4><p>将每个属性视为坐标空间中的一个维度，也就是一个坐标轴，对样本分类就意味着在这个坐标空间中寻找不同类样本的分类边界。</p>
<p>决策树形成的分类边界由若干个与坐标轴平行的分段组成，这些分类边界使得学习结果有较好的可解释性，因为每一段划分都对应了一个属性的取值的划分，比如：</p>
<p><img src="/../img/image-20221017214852110.png" srcset="/img/loading.gif" lazyload alt="image-20221017214852110" style="zoom:50%;"></p>
<p>但是，当学习任务的真实分类边界比较复杂时，就需要有很多的分段才能有较好的近似，此时的决策树会相当复杂，需进行大量属性测试，时间开销很大。</p>
<p><img src="/../img/image-20221017221146620.png" srcset="/img/loading.gif" lazyload alt="image-20221017221146620" style="zoom:50%;"></p>
<p>多变量决策树正是适合能实现这样的“斜划分”甚至更复杂划分的决策树，从简单的判断属性变成一个$\sum w_ia_i =t$这样的线性分类器。</p>
<p><img src="/../img/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWNoZW5zdXl1,size_16,color_FFFFFF,t_70.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<h4 id="阅读材料"><a href="#阅读材料" class="headerlink" title="阅读材料"></a>阅读材料</h4><p>除了信息增益，增益率，基尼指数之外，还有许多其他准则用于决策树划分选择，要注意的是，虽然这些准则对决策树的尺寸有较大影响，但<strong>对泛化性能影响很有限</strong>。</p>
<p>本质上，各种特征选择方法均可用于决策树的划分属性选择。</p>
<p>信息增益和基尼指数仅在2%的情况下会有所不同。</p>
<p><strong>剪枝方法和程度对决策树的泛化性能的影响相当显著</strong>，在数据带有噪声时通过剪枝甚至可将决策树的泛化性能提高25%。</p>
<p>OC1先贪心地寻找每个属性的最优权值，在局部优化的基础上再对分类边界进行随机扰动以试图找到更好的边界。</p>
<p>一些多变量决策树算法在决策树的叶结点上嵌入神经网络，以结合这两种学习机制的优势（如感知机树）。</p>
<h3 id="五、神经网络"><a href="#五、神经网络" class="headerlink" title="五、神经网络"></a>五、神经网络</h3><p>神经网络原理：当神经网络中的某个神经元的电位超过了一个阈值(threshold)，那么它就会被激活，向其他神经元发送化学物质。把许多这样的神经元按一定的层次结构连接起来，就得到了神经网络。</p>
<p>M-P神经元模型就是从生物神经网络中抽象出来的。</p>
<h4 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h4><p>感知机(Perception)由两层神经元组成，输入层接受外界输入信号后传递给输出层。学习过程：通过逐个样本输入来更新权重，先初始化权重(一般为随机)，再逐个地输入样本数据，若输出值与真实标记一致则继续输入下一个样本，否则更新权重，再开始重新逐个检验，直到所有的样本的输出值与真实标记一致。</p>
<p>感知机只拥有一层功能神经元，其学习能力非常有限，只能处理线性可分的问题。</p>
<blockquote>
<p>如果用一个线性函数可以将两类样本完全分开，就称这些样本是“线性可分”的。</p>
</blockquote>
<ul>
<li>对于线性可分的问题，感知机的学习过程总能收敛而找到合适的模型参数，即感知机模型总能将训练数据(不是测试数据)的每一个样本都预测正确，但也容易产生过拟合问题。</li>
<li>对于线性不可分的问题(如异或问题，需要两条平行的分割线)，则需要用多层功能神经元，把原本不可分的数据投射到高维上，可能就变得可分了。</li>
</ul>
<p>考虑到现实任务的复杂性，常见的神经网络是层级结构，这样的神经网络结构通常称为“多层前馈神经网络”(multi-layer feedforward neural networks)。</p>
<p><img src="/../img/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWNoZW5zdXl1,size_16,color_FFFFFF,t_70-20221018215403315.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>神经网络的学习过程就是：根据训练数据来调整神经元之间的连接权和每个功能神经元的阈值，直到每个数据都能够分类正确，或误差在标准之内。</p>
<h4 id="BP算法"><a href="#BP算法" class="headerlink" title="BP算法"></a>BP算法</h4><p>多层网络比单层的感知机复杂多了，因此需要更强大的学习算法，误差逆传播(error BackPropagation，简称BP)算法是迄今为止最成功的神经网络学习算法。</p>
<p><img src="/../img/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoaWNoZW5zdXl1,size_16,color_FFFFFF,t_70-20221018215735459.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<p>随机初始化模型参数，根据输入预测出实值向量$\hat y_j$后，得到它和实际$y_j$的均方误差，通过最小化这个均方误差来获得最优的模型参数。</p>
<p>迭代过程选择梯度下降法，找到均方误差函数每一个参数的最速下降方向，用一个0-1之间的学习率$\mu$来乘这个梯度，每次沿着这个梯度下降$\mu$，直到达到停止条件。为达到精细调节的目的，每个参数的$\mu$不一定要相等。</p>
<p>以下是BP算法的工作流程：</p>
<p><img src="/../img/image-20221018220608802.png" srcset="/img/loading.gif" lazyload alt="image-20221018220608802" style="zoom:50%;"></p>
<p>从repeat循环和for循环可以看出，一旦某个样例$(x_k,y_k)$的预测输出与实际$y$值不一样，使得各参数做出更新，就要开始新一轮迭代，重新从$y_1$开始检验，直到每一个样例的预测输出都符合我们的预期为止。</p>
<p>具体的数据流过程为：输入示例传给输入层，正向传递到输出层产生一个输出结果，然后计算与实际$y$值的误差(第4-5行)，再将误差反向传播到隐层神经元算出这一步神经元的梯度(第6行)，根据梯度和学习率对模型参数进行调整后(第7行)，再回到第3行。</p>
<p>这是基于for循环下的单个训练数据的$E_k$而进行的（虽然最终效果还是最小化累积误差），若类似地推导出基于累积误差最小化的更新规则（即在读取整个训练集一遍后才对参数进行更新），就得到了累积误差逆传播算法。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/mynameisgt/article/details/115644410?spm=1001.2101.3001.6650.5&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-115644410-blog-89704075.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-5-115644410-blog-89704075.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=8">标准BP和累积BP的区别代码</a></p>
<p>两种方法的缺点？<strong>标准BP</strong>：震荡；<strong>累积BP</strong>：大样本时下降速度慢。</p>
<p>多层前馈神经网络有强大的表示能力，已证明只要隐层包含的神经元数量足够多，它就能以任意精度逼近任意复杂度的连续函数。</p>
<p>但是，如何设置隐层神经元的个数仍是个未决问题，实际应用中通常靠“试错法”(trail-by-error)调整。</p>
<blockquote>
<p>试错法：为了追求达到理想目标而通过不断试验和消除误差，探索具有黑箱性质的系统的方法。统称为试错法</p>
</blockquote>
<p>BP神经网络过拟合的缓解方法：</p>
<p>由于多层前馈网络的强大表示能力，经常遇到这样的情况：训练误差持续降低，但测试误差却在上升，这就是BP神经网络经常遭遇的过拟合问题。通常，有以下两种方法缓解BP网络的过拟合：</p>
<ol>
<li>“早停(early stopping)”：将数据分成训练集和验证集，训练集用来训练神经网络，验证集用来估计误差。若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。</li>
<li>“正则化(regularization)”：其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和。</li>
</ol>
<p>仍令$E_k$表示第k个训练样例上的误差，则误差目标函数改变为：</p>
<p><img src="/../img/20190708143448576.png" srcset="/img/loading.gif" lazyload alt="img" style="zoom:50%;"></p>
<p>其中$λ ∈ ( 0 , 1 )$用于对经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。</p>
<blockquote>
<p><strong>将数据划分为n份，依次使用其中一份作为测试集，其他n-1份作为训练集，多次计算模型的精确性来评估模型的平均准确程度</strong>。</p>
</blockquote>
<h4 id="全局最小与局部最小"><a href="#全局最小与局部最小" class="headerlink" title="全局最小与局部最小"></a>全局最小与局部最小</h4><p>看这个图就已经非常形象了，梯度下降是最常用的参数寻优方法，在此类方法中，我们从某些初始解出发，迭代寻找最优参数值，每次迭代中，我们先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。</p>
<p><img src="/../img/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoMTgzMjgwNzE1ODA=,size_16,color_FFFFFF,t_70.png" srcset="/img/loading.gif" lazyload alt="在这里插入图片描述" style="zoom:67%;"></p>
<p>如果误差函数具有多个局部极小，则不能保证找到的解是全局最小，这显然不是我们所希望的。</p>
<p>在现实任务中，通常采用以下策略来试图“跳出”局部极小，从而进一步接近全局最小：</p>
<ol>
<li>采用多个不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数。</li>
<li>使用模拟退火技术。模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部极小。在每步迭代过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。</li>
<li>使用随机梯度下降，与标准梯度下降法精确计算梯度不同，随机梯度下降法在计算梯度时加入了随机因素（随机选取训练数据中的特例），相当于是梯度下降的平滑版本，减少了非常多的计算量，并且即便陷入局部极小点，它计算出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索。</li>
<li>此外，遗传算法也常用来训练神经网络以更好地逼近全局最小。</li>
</ol>
<p>模拟退火具体步骤：设置一个初始高温，当温度越大的时候，分子运动越剧烈，也就是说，x移动的幅度也就越大，在移动一次后，当得到的新的$f(x)$值更好的话，我们要么直接用新的x取代当前位置，要么以一个概率接受原来不怎么好的旧值，打个比方，如果基于贪心的思想，让x永远只接受更好的新位置，那么在下图中，随着迭代次数增多，温度下降，分子运动越微弱，x能移动的距离越小，$x_0$就很容易会陷入左边那个局部最大值，但是如果它没有接受新值，而是在旧的位置上，那么它下一次运动时就有更大的概率可以摇摆到左边那座山上去，找到真正的全局最优值。</p>
<p><img src="/../img/image-20221019000546114.png" srcset="/img/loading.gif" lazyload alt="image-20221019000546114" style="zoom:50%;"></p>
<h4 id="BP算法的改进"><a href="#BP算法的改进" class="headerlink" title="BP算法的改进"></a>BP算法的改进</h4><h5 id="引入动量法"><a href="#引入动量法" class="headerlink" title="引入动量法"></a>引入动量法</h5><p>标准BP算法实质上是一种简单的最速下降静态寻优算法，没有考虑积累的经验，即以前的梯度方向，从而使学习过程振荡，收敛缓慢。</p>
<p>我们的目的是，假如梯度下降得到的路径一直是沿着一个向右的方向前进着，只不过会在上下方向上来回做没用的震荡，我们会基于前面这些步的经验，考虑在算法上减少上下分量的影响，增加左右方向上的影响，这就是动量法的基本思想。</p>
<p><img src="/../img/2019070814472366.png" srcset="/img/loading.gif" lazyload alt="img" style="zoom:67%;"></p>
<p>$D(k)$为k时刻的梯度，$\alpha$为学习率，$\mu$为动量项因子，通常介于0-1之间。</p>
<p>所加入的动量项实质上相当于阻尼项，它减小了学习过程的振荡趋势，改善了收敛性，是一种应用比较广泛的改进算法。</p>
<h5 id="尺度变换法"><a href="#尺度变换法" class="headerlink" title="尺度变换法"></a>尺度变换法</h5><p>标准BP学习算法采用的是一阶梯度法，因而收敛较慢。若采用二阶梯度法，则可以大大提高收敛性。</p>
<p><img src="/../img/20190708144956306.png" srcset="/img/loading.gif" lazyload alt="img" style="zoom:80%;"></p>
<p>虽然二阶梯度法具有较好的收敛性，但是需要计算E对w的 二阶导数，这个计算量很大。所以一般不直接采用二阶梯度法，而常常采用变尺度法或共轭梯度法，它们具有如二阶梯度法收敛较快的优点，又无需直接计算二阶梯度。<br><img src="/../img/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoMTgzMjgwNzE1ODA=,size_16,color_FFFFFF,t_70-20221019003334577.png" srcset="/img/loading.gif" lazyload alt="img" style="zoom:80%;"></p>
<h5 id="自适应学习率调整法"><a href="#自适应学习率调整法" class="headerlink" title="自适应学习率调整法"></a>自适应学习率调整法</h5><p>在BP算法中，网络权值的调整取决于学习速率和梯度。</p>
<p><strong>自适应学习率调整准则是</strong>：检查权值的修正是否真正降低了误差函数，当连续两次迭代其梯度方向相同时，表明下降太慢，这时可使步长加倍；当连续两次迭代其梯度方向相反时，表明下降过头，这时可使步长减半。</p>
<p><img src="/../img/20190708145150247.png" srcset="/img/loading.gif" lazyload alt="img"></p>
<h4 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h4><p>理论上，参数越多的模型复杂度越高，容量越大，能完成更复杂的任务。但一般情况下，复杂模型的训练效率低，易陷入过拟合。训练效率低可由算力的提升解决，过拟合可由数据规模的增大有所缓解。因此，以深度学习为代表的复杂模型开始受到关注。</p>
<p>对于神经网络模型，提高容量的一个简单方法就是增加隐层的数量。从增加模型复杂度的角度看，增加隐层的数目显然比增加隐层神经元的数目更有效。因为增加隐层数不仅增加了拥有激活函数的神经元数目，还增加了激活函数嵌套的层数。</p>
<p>多隐层神经网络难以直接用经典算法（如标准BP算法）进行训练，因为误差在多隐层内逆传播往往会发散而不能收敛。</p>
<p>无监督逐层训练（unsupervised layer-wise training）是多隐层神经网的训练的有效手段，其基本思想是：</p>
<p>每次训练一层隐结点，训练时将上一层隐结点的输出作为输入，而本层隐结点的输出又作为下一层隐结点的输入，这称为“预训练”(pre-training)。</p>
<p>预训练全部完成后，再对整个网络进行微调（fine-tuning）训练，预训练+微调的做法可视为：对每组先找到局部看起来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优。<br>这样就在利用模型大量参数所提供的自由度的同时，有效地节省了训练开销。</p>
<p>还有一种节省训练开销的策略是权共享(weight sharing)，即让一组神经元使用相同的连接权。这种策略在卷积神经网络（CNN）中发挥了重要作用。</p>
<h3 id="六、支持向量机"><a href="#六、支持向量机" class="headerlink" title="六、支持向量机"></a>六、支持向量机</h3><p>支持向量机（Support Vector Machine），简称SVM，是一种经典的二分类模型，属于监督学习算法。因为很多损失函数都不是凸优化问题，容易陷入局部最优，SVM把优化目标定为间隔最大化，就转化为了一个凸二次规划求解的问题。</p>
<h4 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h4><p>对于二分类问题，其基本思想就是基于训练集$D$在样本空间中找到一个用来划分的超平面，将不同类别的样本分开。</p>
<p>但仅仅是将样本分开的话，可以找到许多个划分超平面。从几何意义易知，粗线对训练样本局部的<strong>扰动的容忍性</strong>最好（考虑到噪声等因素，其他的超平面训练样本很容易越界，分类结果很不稳定），也是最<strong>鲁棒</strong>的（即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小）。于是<strong>同时离两类样本都尽量远</strong>的粗线就是我们想要的最优划分超平面。</p>
<p><img src="/../img/image-20221019132945679.png" srcset="/img/loading.gif" lazyload alt="image-20221019132945679" style="zoom:50%;"></p>
<p><strong>超平面方程</strong>：样本空间中，用来描述超平面的线性方程为：</p>
<script type="math/tex; mode=display">
W^TX+b=0</script><p>其中$W=(w_1;w_2;…;w_d)$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。</p>
<p><strong>约束条件</strong>：假设超平面$(W，b)$能将训练样本正确分类，即对于$(x_i,y_i)\in D$，若$y_i=+1$，则有$w^Tx_i+b&gt;0$；若$y_i=-1$，则有$w^Tx_i+b&lt;0$​，令</p>
<script type="math/tex; mode=display">
\begin{cases}
w^Tx_i+b\geq+1, & y_i=+1\\
w^Tx_i+b\leq-1, &  y_i=-1
\end{cases}</script><p>为超平面将训练样本正确分类的约束条件，如下图所示，距离超平面最近的三个训练样本使得约束条件中的等号成立，它们就是支持向量，支持向量到超平面的距离就是软间隔。</p>
<p><img src="/../img/image-20221019134926382.png" srcset="/img/loading.gif" lazyload alt="image-20221019134926382" style="zoom:50%;"></p>
<p>我们的目标是最大化这个软间隔，因此只用关注支持向量就可以了，别的点可以都忽略，因为只要支持向量满足约束条件，别的点一定都能满足。</p>
<p>SVM公式推导</p>
<p>目标函数：$\max margin(W,b)$，其中$margin$就是所有点中离平面最近的点到平面的距离：</p>
<script type="math/tex; mode=display">
margin(W,b)=\min_ {i=1,2,...,N} \frac{1}{||W||_2}\cdot|W^TX^{(i)}+b|</script><p>点到平面距离公式的推导：</p>
<p><img src="/../img/image-20221019141221799.png" srcset="/img/loading.gif" lazyload alt="image-20221019141221799" style="zoom:50%;"></p>
<p>求$x$到平面$f(x)$的间距，先求$x$到$f(x)$的梯度，单位化之后得到一个单位法向量，再乘$x$到$f(x)$上任意一点的向量，得到在法向量方向上的投影，就是点到平面的距离。</p>
<p>原理：两向量相乘，二者内积就等于一个是一方对另一方的贡献 。几何意义就是向量a在向量b方向上的投影与向量b的模的乘积。</p>
<p>以上是软间隔的推导过程，但还需要保证分类也是正确的，即保证Y=+1类f(x)都大于0，Y=-1类f(x)都小于0，即$Y^i(W^TX^i+b)&gt;0$，$(X^i,Y^i)$是样本数据。有了这个约束条件，保证了分类的正确后，还可以把距离$H$更简化，</p>
<p><img src="/../img/image-20221004160750128-6161181.png" srcset="/img/loading.gif" lazyload alt="image-20221004160750128" style="zoom: 67%;"></p>
<p>做出左边最后一行的变形，虽然$H$的结果不变，但是括号内的直线部分却发生了变化，为了缩小我们想要的$H$的范围，我们直接把样本点限定在位于边界的支持向量上，令绝对值里的部分强行等于1，于是就又多了一个约束条件。</p>
<p>因为有绝对值的存在，所以要加上标签$Y$，这样在分类正确的前提下，若$(W^TX^i+b)&gt;0$属于正类，标签就是+1，若$(W^TX^i+b)&lt;0$属于反类，标签就是-1，标签$Y$就起到了绝对值的作用，保证了函数的连续可微性。</p>
<p>因此，最后就得到了SVM的目标函数：</p>
<script type="math/tex; mode=display">
min \frac{1}{2}||w||^2\\
s.t.\ Y^i(W^TX^i+b)\geq1,\ i=1,2,...,N</script><p>这也是支持向量机的基本形式。</p>
<h4 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h4><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/90zeng/p/Lagrange_duality.html">具体思想</a></p>
<p>​     在得到SVM的基本形式之后，因为其带有一个不等式约束，为了方便计算，我们利用拉格朗日乘子法将其转化为无约束问题，再求解$L(w,b,\alpha)$对$w,b$的偏导令其等于0，代回上式，得到SVM的对偶问题：</p>
<p><img src="/../img/image-20221019171826241.png" srcset="/img/loading.gif" lazyload alt="image-20221019171826241" style="zoom: 50%;"></p>
<p>对于上述的转换对偶问题求解过程，需满足KKT(Karush-Kuhn-Tucker)条件，即要求：</p>
<p><img src="/../img/20190530120814493.png" srcset="/img/loading.gif" lazyload alt="img" style="zoom:80%;"></p>
<p>这样才能保证对偶问题可以得到和原问题一样的最优解。</p>
<p>最后得出支持向量机重要结论：<strong>训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。</strong></p>
<p>如何求解对偶问题？</p>
<p>我们采用二次规划算法SMO进行求解。我们现在的目标是求解使得对偶问题结果最小的拉格朗日乘子$\alpha$，SMO的基本思想：先固定$α_i$之外的所有参数，然后求$α_i$上的极值，它的思想与坐标上升算法的思想类似。</p>
<p>坐标上升算法每次通过更新多元函数中的一维，经过多次迭代直到收敛来达到优化函数的目的。简单的讲就是不断地选中一个变量做一维最优化直到函数达到局部最优点。</p>
<p>例子：</p>
<p>若我们的优化目标为一个二元函数：$arg\min f(x_1,x_2)=−x_1^2−3x_2^2+2x_1x_2+6$</p>
<p>我们先给一个$ (x_1,x_2)$ 的初值然后开始迭代。</p>
<ol>
<li>先固定 $x_1$ ，把$ f$ 看做$ x_2$ 的一元函数求最优值，可以简单的进行求导获取解析解:<br>$∂f/∂x_1=−2x_1+2x_2=0→x1=x2$</li>
<li>再固定$x_2$, 把$f$看成$x_1$的一元函数求最优值，得到$x_1$的解析解:<br>$∂f/∂x_2=−6x_2+2x_1→x_2=\frac13x1$</li>
</ol>
<p>按照上面两个过程不断交替的优化$ x_1 $和$ x_2 $，直到函数收敛。</p>
<p>通过下面的图就可以看出，优化的过程，因为每次只优化一个变量，每次迭代的方向都是沿着坐标轴方向的。</p>
<p><img src="/../img/v2-6a95f54ca97a7f0a87924172794af2d7_r.jpg" srcset="/img/loading.gif" lazyload alt="img" style="zoom: 33%;"></p>
<p>SMO的思想类似坐标上升算法，我们需要优化一系列的$α$的值，我们每次选择尽量少的$ α $来优化，不断迭代直到函数收敛到最优值。</p>
<p>来到SVM的对偶问题上，其中我们需要对$ (α_1,α_2,…,α_N) $进行求解，SMO算法可以高效的求解这个对偶问题，只需要把原始问题的求解 N 个参数二次规划问题分解成多个二次规划问题求解，每个子问题只需要求解两个参数，节省了时间成本和内存需求。</p>
<blockquote>
<p>二次规划(QP, Quadratic Programming)定义：目标函数为二次函数，约束条件为线性约束，属于最简单的一种非线性规划。</p>
</blockquote>
<p>与坐标上升算法不同的是，我们在SMO算法中每次需要选择<strong>一对</strong>变量$ (α_i,α_j) $，因为在SVM中，$ α $c乘子并不是完全独立的，而是具有约束的：$∑_{i=1}^Nα_iy_i=0$</p>
<p>因此一个$ α $改变，另一个也要随之变化以满足条件，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29212107">具体推导过程</a>。</p>
<h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><p>前两节是在样本在其原始样本空间线性可分的假设下进行讨论的。</p>
<p>针对原始样本空间线性不可分的问题，基于<strong>有限维原始样本空间一定存在一个高维特征空间使样本线性可分</strong>这一定理，引出了原始空间和特征空间的桥梁——核函数。</p>
<h3 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ch18328071580/article/details/95048420">BP神经网络</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29212107">SMO算法实现</a></p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="category-chain-item">人工智能</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/">#机器学习基础理论</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/09/30/Machine-Learning/" title="Machine Learning">
                        <span class="hidden-mobile">Machine Learning</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="vcomment" class="comment"></div> 
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
  <script>
    var notify = '' == true ? true : false;
    var verify = '' == true ? true : false;
      window.onload = function() {
          new Valine({
              el: '#vcomment',
              app_id: "AwHBUAjSP1GvDVpiBgxfS2Pg-gzGzoHsz",
              app_key: "kMsGLN3hzkQJuLrmqQBgquFF",
              placeholder: "说点什么",
              avatar:"retro",
              visitor: true       

          });
      }
  </script>

 
  <noscript>Please enable JavaScript to view the comments</noscript>



  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  




  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  

  

  

  

  

  

  
    
  




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  
      <script>
        MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.1/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>

  <script defer src="/js/leancloud.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/yinghua.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiantiao.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/xiaoxingxing.js"></script>
<script src="//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/caidai.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
